var documenterSearchIndex = {"docs":
[{"location":"datasets/vision/#Vision-Datasets","page":"Vision","title":"Vision Datasets","text":"","category":"section"},{"location":"datasets/vision/","page":"Vision","title":"Vision","text":"A collection of datasets for 2d computer vision. ","category":"page"},{"location":"datasets/vision/","page":"Vision","title":"Vision","text":"Numerical arrays can be converted to color images using  convert2image, and displayed in the terminal with the package ImageInTerminal.jl","category":"page"},{"location":"datasets/vision/#Index","page":"Vision","title":"Index","text":"","category":"section"},{"location":"datasets/vision/","page":"Vision","title":"Vision","text":"Pages = [\"vision.md\"]","category":"page"},{"location":"datasets/vision/#Documentation","page":"Vision","title":"Documentation","text":"","category":"section"},{"location":"datasets/vision/","page":"Vision","title":"Vision","text":"convert2image","category":"page"},{"location":"datasets/vision/#MLDatasets.convert2image","page":"Vision","title":"MLDatasets.convert2image","text":"convert2image(d, i)\nconvert2image(d, x)\nconvert2image(DType, x)\n\nConvert the observation(s) i from dataset d to image(s). It can also convert a numerical array x.\n\nExamples\n\njulia> using MLDatasets: MNIST\n        \njulia> using ImageInTerminal\n\njulia> d = MNIST()\n\njulia> i = 1:2;\n\njulia> convert2image(d, i)\n\njulia> x = d[1].features;\n\njulia> convert2image(MNIST, x) # or convert2image(d, x)\n\n\n\n\n\n","category":"function"},{"location":"datasets/vision/","page":"Vision","title":"Vision","text":"CIFAR10\nCIFAR100\nEMNIST\nFashionMNIST\nMNIST\nSVHN2","category":"page"},{"location":"datasets/vision/#MLDatasets.CIFAR10","page":"Vision","title":"MLDatasets.CIFAR10","text":"CIFAR10(; Tx=Float32, split=:train, dir=nothing)\nCIFAR10([Tx, split])\n\nThe CIFAR10 dataset is a labeled subsets of the 80 million tiny images dataset. It consists of 60000 32x32 colour images in 10 classes, with 6000 images per class.\n\nArguments\n\nYou can pass a specific dir where to load or download the dataset, otherwise uses\n\nthe default one.\n\nsplit: selects the data partition. Can take the values :train: or :test. \n\nFields\n\nmetadata: A dictionary containing additional information on the dataset.\nfeatures: An array storing the data features. \ntargets: An array storing the targets for supervised learning.\nsplit.\n\nMethods\n\ndataset[i]: Return observation(s) i as a named tuple of features and targets . \ndataset[]: Return all observations as a named tuple of features and targets.\nlength(dataset): Number of observations.\nconvert2image converts features to RGB images.\n\nExamples\n\njulia> using MLDatasets: CIFAR10\n\njulia> dataset = CIFAR10()\nCIFAR10:\n  metadata    =>    Dict{String, Any} with 2 entries\n  split       =>    :train\n  features    =>    32×32×3×50000 Array{Float32, 4}\n  targets     =>    50000-element Vector{Int64}\n\njulia> dataset[1:5].targets\n5-element Vector{Int64}:\n 6\n 9\n 9\n 4\n 1\n\njulia> X, y = dataset[];\n\njulia> dataset = CIFAR10(Tx=Float64, split=:test)\nCIFAR10:\n  metadata    =>    Dict{String, Any} with 2 entries\n  split       =>    :test\n  features    =>    32×32×3×10000 Array{Float64, 4}\n  targets     =>    10000-element Vector{Int64}\n\njulia> dataset.metadata\nDict{String, Any} with 2 entries:\n  \"n_observations\" => 10000\n  \"class_names\"    => [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n\n\n\n\n\n","category":"type"},{"location":"datasets/vision/#MLDatasets.CIFAR100","page":"Vision","title":"MLDatasets.CIFAR100","text":"CIFAR100(; Tx=Float32, split=:train, dir=nothing)\nCIFAR100([Tx, split])\n\nThe CIFAR100 dataset is a labeled subsets of the 80 million tiny images dataset. It consists of 60000 32x32 colour images in 10 classes, with 6000 images per class.\n\nReturn the CIFAR-100 trainset labels (coarse and fine) corresponding to the given indices as a tuple of two Int or two Vector{Int}. The variables returned are the coarse label(s) (Yc) and the fine label(s) (Yf) respectively.\n\nArguments\n\nYou can pass a specific dir where to load or download the dataset, otherwise uses\n\nthe default one.\n\nsplit: selects the data partition. Can take the values :train: or :test. \n\nFields\n\nmetadata: A dictionary containing additional information on the dataset.\nfeatures: An array storing the data features. \ntargets: An array storing the targets for supervised learning.\nsplit.\n\nMethods\n\ndataset[i]: Return observation(s) i as a named tuple of features and targets . \ndataset[]: Return all observations as a named tuple of features and targets.\nlength(dataset): Number of observations.\nconvert2image converts features to RGB images.\n\nExamples\n\njulia> dataset = CIFAR100()\nCIFAR100:\n  metadata    =>    Dict{String, Any} with 3 entries\n  split       =>    :train\n  features    =>    32×32×3×50000 Array{Float32, 4}\n  targets     =>    (coarse = \"50000-element Vector{Int64}\", fine = \"50000-element Vector{Int64}\")\n\njulia> dataset[1:5].targets\n(coarse = [11, 15, 4, 14, 1], fine = [19, 29, 0, 11, 1])\n\njulia> X, y = dataset[];\n\njulia> dataset.metadata\nDict{String, Any} with 3 entries:\n  \"n_observations\"     => 50000\n  \"class_names_coarse\" => [\"aquatic_mammals\", \"fish\", \"flowers\", \"food_containers\", \"fruit_and_vegetables\", \"household_electrical_devices\", \"household_furniture\", \"insects\", \"large_carnivores\", \"large_man-made_…\n  \"class_names_fine\"   => [\"apple\", \"aquarium_fish\", \"baby\", \"bear\", \"beaver\", \"bed\", \"bee\", \"beetle\", \"bicycle\", \"bottle\"  …  \"train\", \"trout\", \"tulip\", \"turtle\", \"wardrobe\", \"whale\", \"willow_tree\", \"wolf\", \"w…\n\n\n\n\n\n","category":"type"},{"location":"datasets/vision/#MLDatasets.EMNIST","page":"Vision","title":"MLDatasets.EMNIST","text":"EMNIST(name; Tx=Float32, split=:train, dir=nothing)\nEMNIST(name, [Tx, split])\n\nThe EMNIST dataset is a set of handwritten character digits derived from the NIST Special Database 19 (https://www.nist.gov/srd/nist-special-database-19) and converted to a 28x28 pixel image format and dataset structure that directly matches the MNIST dataset (http://yann.lecun.com/exdb/mnist/). Further information on the dataset contents and conversion process can be found in the paper available at https://arxiv.org/abs/1702.05373v1.\n\nArguments\n\nname: name of the EMNIST dataset. Possible values are: :balanced, :byclass, :bymerge, :digits, :letters, :mnist.\nsplit: selects the data partition. Can take the values :train: or :test. \nYou can pass a specific dir where to load or download the dataset, otherwise uses\n\nthe default one.\n\nFields\n\nname.\nsplit.\nmetadata: A dictionary containing additional information on the dataset.\nfeatures: An array storing the data features. \ntargets: An array storing the targets for supervised learning.\n\nMethods\n\ndataset[i]: Return observation(s) i as a named tuple of features and targets . \ndataset[]: Return all observations as a named tuple of features and targets.\nlength(dataset): Number of observations.\nconvert2image converts features to Gray images.\n\nExamples\n\nThe images are loaded as a multi-dimensional array of eltype Tx. If Tx <: Integer, then all values will be within 0 and 255,  otherwise the values are scaled to be between 0 and 1. EMNIST().features is a 3D array (i.e. a Array{Tx,3}), in WHN format (width, height, num_images). Labels are stored as a vector of integers in EMNIST().targets. \n\njulia> using MLDatasets: EMNIST\n\njulia> dataset = EMNIST(:letters, split=:train)\nEMNIST:\n  metadata    =>    Dict{String, Any} with 3 entries\n  split       =>    :train\n  features    =>    28×28×60000 Array{Float32, 3}\n  targets     =>    60000-element Vector{Int64}\n\njulia> dataset[1:5].targets\n5-element Vector{Int64}:\n7\n2\n1\n0\n4\n\njulia> X, y = dataset[];\n\njulia> dataset = EMNIST(:balanced, Tx=UInt8, split=:test)\nEMNIST:\n  metadata    =>    Dict{String, Any} with 3 entries\n  split       =>    :test\n  features    =>    28×28×10000 Array{UInt8, 3}\n  targets     =>    10000-element Vector{Int64}\n\n\n\n\n\n","category":"type"},{"location":"datasets/vision/#MLDatasets.FashionMNIST","page":"Vision","title":"MLDatasets.FashionMNIST","text":"FashionMNIST(; Tx=Float32, split=:train, dir=nothing)\nFashionMNIST([Tx, split])\n\nFashionMNIST is a dataset of Zalando's article images consisting of a training set of 60000 examples and a test set of 10000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. It can serve as a drop-in replacement for MNIST.\n\nAuthors: Han Xiao, Kashif Rasul, Roland Vollgraf\nWebsite: https://github.com/zalandoresearch/fashion-mnist\n\nSee MNIST for details of the interface.\n\n\n\n\n\n","category":"type"},{"location":"datasets/vision/#MLDatasets.MNIST","page":"Vision","title":"MLDatasets.MNIST","text":"MNIST(; Tx=Float32, split=:train, dir=nothing)\nMNIST([Tx, split])\n\nThe MNIST database of handwritten digits.\n\nAuthors: Yann LeCun, Corinna Cortes, Christopher J.C. Burges\nWebsite: http://yann.lecun.com/exdb/mnist/\n\nMNIST is a classic image-classification dataset that is often used in small-scale machine learning experiments. It contains 70,000 images of handwritten digits. Each observation is a 28x28 pixel gray-scale image that depicts a handwritten version of 1 of the 10 possible digits (0-9).\n\nArguments\n\nYou can pass a specific dir where to load or download the dataset, otherwise uses\n\nthe default one.\n\nsplit: selects the data partition. Can take the values :train: or :test. \n\nFields\n\nmetadata: A dictionary containing additional information on the dataset.\nfeatures: An array storing the data features. \ntargets: An array storing the targets for supervised learning.\nsplit.\n\nMethods\n\ndataset[i]: Return observation(s) i as a named tuple of features and targets . \ndataset[]: Return all observations as a named tuple of features and targets.\nlength(dataset): Number of observations.\nconvert2image converts features to Gray images.\n\nExamples\n\nThe images are loaded as a multi-dimensional array of eltype Tx. If Tx <: Integer, then all values will be within 0 and 255,  otherwise the values are scaled to be between 0 and 1. MNIST().features is a 3D array (i.e. a Array{Tx,3}), in WHN format (width, height, num_images). Labels are stored as a vector of integers in MNIST().targets. \n\njulia> using MLDatasets: MNIST\n\njulia> dataset = MNIST()\nMNIST:\n  metadata    =>    Dict{String, Any} with 3 entries\n  split       =>    :train\n  features    =>    28×28×60000 Array{Float32, 3}\n  targets     =>    60000-element Vector{Int64}\n\njulia> dataset[1:5].targets\n5-element Vector{Int64}:\n7\n2\n1\n0\n4\n\njulia> X, y = dataset[];\n\njulia> dataset = MNIST(UInt8, :test)\nMNIST:\n  metadata    =>    Dict{String, Any} with 3 entries\n  split       =>    :test\n  features    =>    28×28×10000 Array{UInt8, 3}\n  targets     =>    10000-element Vector{Int64}\n\n\n\n\n\n","category":"type"},{"location":"datasets/vision/#MLDatasets.SVHN2","page":"Vision","title":"MLDatasets.SVHN2","text":"SVHN2(; Tx=Float32, split=:train, dir=nothing)\nSVHN2([Tx, split])\n\nThe Street View House Numbers (SVHN) Dataset.\n\nAuthors: Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, Andrew Y. Ng\nWebsite: http://ufldl.stanford.edu/housenumbers\n\nSVHN was obtained from house numbers in Google Street View images. As such they are quite diverse in terms of orientation and image background. Similar to MNIST, SVHN has 10 classes (the digits 0-9), but unlike MNIST there is more data and the images are a little bigger (32x32 instead of 28x28) with an additional RGB color channel. The dataset is split up into three subsets: 73257 digits for training, 26032 digits for testing, and 531131 additional to use as extra training data.\n\nArguments\n\nYou can pass a specific dir where to load or download the dataset, otherwise uses\n\nthe default one.\n\nsplit: selects the data partition. Can take the values :train:, :test or :extra. \n\nFields\n\nmetadata: A dictionary containing additional information on the dataset.\nfeatures: An array storing the data features. \ntargets: An array storing the targets for supervised learning.\nsplit.\n\nMethods\n\ndataset[i]: Return observation(s) i as a named tuple of features and targets . \ndataset[]: Return all observations as a named tuple of features and targets.\nlength(dataset): Number of observations.\nconvert2image converts features to RGB images.\n\nExamples\n\njulia> using MLDatasets: SVHN2\n\njulia> using MLDatasets: SVHN2\n\njulia> dataset = SVHN2()\nSVHN2:\n  metadata    =>    Dict{String, Any} with 2 entries\n  split       =>    :train\n  features    =>    32×32×3×73257 Array{Float32, 4}\n  targets     =>    73257-element Vector{Int64}\n\njulia> dataset[1:5].targets\n5-element Vector{Int64}:\n 1\n 9\n 2\n 3\n 2\n\njulia> dataset.metadata\nDict{String, Any} with 2 entries:\n  \"n_observations\" => 73257\n  \"class_names\"    => [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"0\"]\n\n\n\n\n\n","category":"type"},{"location":"datasets/misc/#Miscellaneuous-Datasets","page":"Miscellaneous","title":"Miscellaneuous Datasets","text":"","category":"section"},{"location":"datasets/misc/#Index","page":"Miscellaneous","title":"Index","text":"","category":"section"},{"location":"datasets/misc/","page":"Miscellaneous","title":"Miscellaneous","text":"Pages = [\"misc.md\"]","category":"page"},{"location":"datasets/misc/#Documentation","page":"Miscellaneous","title":"Documentation","text":"","category":"section"},{"location":"datasets/misc/","page":"Miscellaneous","title":"Miscellaneous","text":"BostonHousing\nIris\nMutagenesis\nTitanic","category":"page"},{"location":"datasets/misc/#MLDatasets.BostonHousing","page":"Miscellaneous","title":"MLDatasets.BostonHousing","text":"BostonHousing(; as_df = true, dir = nothing)\n\nThe classical Boston Housing tabular dataset.\n\nSources:    (a) Origin:  This dataset was taken from the StatLib library which is                 maintained at Carnegie Mellon University.    (b) Creator:  Harrison, D. and Rubinfeld, D.L. 'Hedonic prices and the                   demand for clean air', J. Environ. Economics & Management,                  vol.5, 81-102, 1978.    (c) Date: July 7, 1993\n\nNumber of Instances: 506\n\nNumber of Attributes: 13 continuous attributes (including target                             attribute \"MEDV\"), 1 binary-valued attribute.\n\nArguments\n\nIf as_df = true, load the data as dataframes instead of plain arrays.\nYou can pass a specific dir where to load or download the dataset, otherwise uses\n\nthe default one.\n\nFields\n\nmetadata: A dictionary containing additional information on the dataset.\nfeatures: The data features. An array if as_df=true, otherwise a dataframe. \ntargets: The targets for supervised learning. An array if as_df=true, otherwise a dataframe.\ndataframe: A dataframe containing both features and targets. It is nothing if as_df=false.\n\nMethods\n\ndataset[i]: Return observation(s) i as a named tuple of features and targets . \ndataset[]: Return all observations as a named tuple of features and targets.\nlength(dataset): Number of observations.\n\nExamples\n\njulia> using MLDatasets: BostonHousing\n\njulia> dataset = BostonHousing()\nBostonHousing:\n  metadata => Dict{String, Any} with 5 entries\n  features => 506×13 DataFrame\n  targets => 506×1 DataFrame\n  dataframe => 506×14 DataFrame\n\n\njulia> dataset[1:5][1]\n5×13 DataFrame\n Row │ CRIM     ZN       INDUS    CHAS   NOX      RM       AGE      DIS      RAD    TAX    PTRATIO  B        LSTAT   \n     │ Float64  Float64  Float64  Int64  Float64  Float64  Float64  Float64  Int64  Int64  Float64  Float64  Float64 \n─────┼───────────────────────────────────────────────────────────────────────────────────────────────────────────────\n   1 │ 0.00632     18.0     2.31      0    0.538    6.575     65.2   4.09        1    296     15.3   396.9      4.98\n   2 │ 0.02731      0.0     7.07      0    0.469    6.421     78.9   4.9671      2    242     17.8   396.9      9.14\n   3 │ 0.02729      0.0     7.07      0    0.469    7.185     61.1   4.9671      2    242     17.8   392.83     4.03\n   4 │ 0.03237      0.0     2.18      0    0.458    6.998     45.8   6.0622      3    222     18.7   394.63     2.94\n   5 │ 0.06905      0.0     2.18      0    0.458    7.147     54.2   6.0622      3    222     18.7   396.9      5.33\n\njulia> dataset[1:5][2]\n5×1 DataFrame\nRow │ MEDV    \n    │ Float64 \n────┼─────────\n  1 │    24.0\n  2 │    21.6\n  3 │    34.7\n  4 │    33.4\n  5 │    36.2  \n\njulia> X, y = BostonHousing(as_df=false)[]\n([0.00632 0.02731 … 0.10959 0.04741; 18.0 0.0 … 0.0 0.0; … ; 396.9 396.9 … 393.45 396.9; 4.98 9.14 … 6.48 7.88], [24.0 21.6 … 22.0 11.9])\n\n\n\n\n\n","category":"type"},{"location":"datasets/misc/#MLDatasets.Iris","page":"Miscellaneous","title":"MLDatasets.Iris","text":"Iris(; as_df = true, dir = nothing)\n\nFisher's classic iris dataset. \n\nMeasurements from 3 different species of iris: setosa, versicolor and virginica. There are 50 examples of each species.\n\nThere are 4 measurements for each example: sepal length, sepal width, petal length and petal width.  The measurements are in centimeters.\n\nThe module retrieves the data from the UCI Machine Learning Repository.\n\nNOTE: no pre-defined train-test split for this dataset. \n\nArguments\n\nIf as_df = true, load the data as dataframes instead of plain arrays.\nYou can pass a specific dir where to load or download the dataset, otherwise uses\n\nthe default one.\n\nFields\n\nmetadata: A dictionary containing additional information on the dataset.\nfeatures: The data features. An array if as_df=true, otherwise a dataframe. \ntargets: The targets for supervised learning. An array if as_df=true, otherwise a dataframe.\ndataframe: A dataframe containing both features and targets. It is nothing if as_df=false.\n\nMethods\n\ndataset[i]: Return observation(s) i as a named tuple of features and targets . \ndataset[]: Return all observations as a named tuple of features and targets.\nlength(dataset): Number of observations.\n\nExamples\n\njulia> dataset = Iris()\nIris:\n  metadata => Dict{String, Any} with 4 entries\n  features => 150×4 DataFrame\n  targets => 150×1 DataFrame\n  dataframe => 150×5 DataFrame\n\n\njulia> dataset[1:2]\n(2×4 DataFrame\n Row │ sepallength  sepalwidth  petallength  petalwidth \n     │ Float64      Float64     Float64      Float64    \n─────┼──────────────────────────────────────────────────\n   1 │         5.1         3.5          1.4         0.2\n   2 │         4.9         3.0          1.4         0.2, 2×1 DataFrame\n Row │ class       \n     │ String15    \n─────┼─────────────\n   1 │ Iris-setosa\n   2 │ Iris-setosa)\n\njulia> X, y = Iris(as_df=false)[]\n([5.1 4.9 … 6.2 5.9; 3.5 3.0 … 3.4 3.0; 1.4 1.4 … 5.4 5.1; 0.2 0.2 … 2.3 1.8], InlineStrings.String15[\"Iris-setosa\" \"Iris-setosa\" … \"Iris-virginica\" \"Iris-virginica\"])\n\n\n\n\n\n","category":"type"},{"location":"datasets/misc/#MLDatasets.Mutagenesis","page":"Miscellaneous","title":"MLDatasets.Mutagenesis","text":"Mutagenesis(; split, dir=nothing)\n\nThe Mutagenesis dataset comprises 188 molecules trialed for mutagenicity on Salmonella typhimurium, available from  relational.fit.cvut.cz and  CTUAvastLab/datasets.\n\nSet split to :train, :val, :test, or :all, to select the training,  validation, test partition respectively or the whole dataset. The indexes field in the result contains the indexes of the partition in the full dataset.\n\nWebsite: https://relational.fit.cvut.cz/dataset/Mutagenesis License: CC0\n\njulia> using MLDatasets: Mutagenesis\n\njulia> dataset = Mutagenesis(split=:train)\nMutagenesis dataset:\n  split : train\n  indexes : 100-element Vector{Int64}\n  features : 100-element Vector{Dict{Symbol, Any}}\n  targets : 100-element Vector{Int64}\n\njulia> dataset[1].features\nDict{Symbol, Any} with 5 entries:\n  :lumo  => -1.246\n  :inda  => 0\n  :logp  => 4.23\n  :ind1  => 1\n  :atoms => Dict{Symbol, Any}[Dict(:element=>\"c\", :bonds=>Dict{Symbol, Any}[Dict(:element=>\"c\", :bond_type=>7, :charge=>-0.117, :atom_type=>22), Dict(:element=>\"h\", :bond_type=>1, :charge=>0.142, :atom_type=>3)…\n\njulia> dataset[1].targets\n1\n\njulia> dataset = Mutagenesis(split=:all)\nMutagenesis dataset:\n  split : all\n  indexes : 188-element Vector{Int64}\n  features : 188-element Vector{Dict{Symbol, Any}}\n  targets : 188-element Vector{Int64}\n\n\n\n\n\n","category":"type"},{"location":"datasets/misc/#MLDatasets.Titanic","page":"Miscellaneous","title":"MLDatasets.Titanic","text":"Titanic(; as_df = true, dir = nothing)\n\nThe Titanic dataset, describing the survival of passengers on the Titanic ship.\n\nArguments\n\nIf as_df = true, load the data as dataframes instead of plain arrays.\nYou can pass a specific dir where to load or download the dataset, otherwise uses\n\nthe default one.\n\nFields\n\nmetadata: A dictionary containing additional information on the dataset.\nfeatures: The data features. An array if as_df=true, otherwise a dataframe. \ntargets: The targets for supervised learning. An array if as_df=true, otherwise a dataframe.\ndataframe: A dataframe containing both features and targets. It is nothing if as_df=false.\n\nMethods\n\ndataset[i]: Return observation(s) i as a named tuple of features and targets . \ndataset[]: Return all observations as a named tuple of features and targets.\nlength(dataset): Number of observations.\n\nExamples\n\njulia> using MLDatasets: Titanic\n\njulia> using DataFrames\n\njulia> dataset = Titanic()\nTitanic:\n  metadata => Dict{String, Any} with 5 entries\n  features => 891×11 DataFrame\n  targets => 891×1 DataFrame\n  dataframe => 891×12 DataFrame\n\n\njulia> describe(dataset.dataframe)\n12×7 DataFrame\n Row │ variable     mean      min                  median   max                          nmissing  eltype                   \n     │ Symbol       Union…    Any                  Union…   Any                          Int64     Type                     \n─────┼──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n   1 │ PassengerId  446.0     1                    446.0    891                                 0  Int64\n   2 │ Survived     0.383838  0                    0.0      1                                   0  Int64\n   3 │ Pclass       2.30864   1                    3.0      3                                   0  Int64\n   4 │ Name                   Abbing, Mr. Anthony           van Melkebeke, Mr. Philemon         0  String\n   5 │ Sex                    female                        male                                0  String7\n   6 │ Age          29.6991   0.42                 28.0     80.0                              177  Union{Missing, Float64}\n   7 │ SibSp        0.523008  0                    0.0      8                                   0  Int64\n   8 │ Parch        0.381594  0                    0.0      6                                   0  Int64\n   9 │ Ticket                 110152                        WE/P 5735                           0  String31\n  10 │ Fare         32.2042   0.0                  14.4542  512.329                             0  Float64\n  11 │ Cabin                  A10                           T                                 687  Union{Missing, String15}\n  12 │ Embarked               C                             S                                   2  Union{Missing, String1}\n\n\n\n\n\n","category":"type"},{"location":"datasets/text/#Text-Datasets","page":"Text","title":"Text Datasets","text":"","category":"section"},{"location":"datasets/text/#Index","page":"Text","title":"Index","text":"","category":"section"},{"location":"datasets/text/","page":"Text","title":"Text","text":"Pages = [\"text.md\"]","category":"page"},{"location":"datasets/text/#Documentation","page":"Text","title":"Documentation","text":"","category":"section"},{"location":"datasets/text/","page":"Text","title":"Text","text":"PTBLM\nSMSSpamCollection\nUD_English","category":"page"},{"location":"datasets/text/#MLDatasets.PTBLM","page":"Text","title":"MLDatasets.PTBLM","text":"PTBLM\n\nThe PTBLM dataset consists of Penn Treebank sentences for language modeling, available from tomsercu/lstm. The unknown words are replaced with <unk> so that the total vocaburary size becomes 10000.\n\n\n\n\n\n","category":"module"},{"location":"datasets/text/#MLDatasets.SMSSpamCollection","page":"Text","title":"MLDatasets.SMSSpamCollection","text":"SMSSpamCollection\n\nDESCRIPTION\n\n\n\nThe SMS Spam Collection v.1 (hereafter the corpus) is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged acording being ham (legitimate) or spam.\n\n1.1. Compilation\n\nThis corpus has been collected from free or free for research sources at the Web:\n\nA collection of between 425 SMS spam messages extracted manually from the Grumbletext Web site. This is a UK forum in which cell phone users make public claims about SMS spam messages, most of them without reporting the very spam message received. The identification of the text of spam messages in the claims is a very hard and time-consuming task, and it involved carefully scanning hundreds of web pages. The Grumbletext Web site is: http://www.grumbletext.co.uk/\nA list of 450 SMS ham messages collected from Caroline Tag's PhD Theses available at http://etheses.bham.ac.uk/253/1/Tagg09PhD.pdf\nA subset of 3,375 SMS ham messages of the NUS SMS Corpus (NSC), which is a corpus of about 10,000 legitimate messages collected for research at the Department of Computer Science at the National University of Singapore. The messages largely originate from Singaporeans and mostly from students attending the University. These messages were collected from volunteers who were made aware that their contributions were going to be made publicly available. The NUS SMS Corpus is avalaible at: http://www.comp.nus.edu.sg/~rpnlpir/downloads/corpora/smsCorpus/\nThe amount of 1,002 SMS ham messages and 322 spam messages extracted from the SMS Spam Corpus v.0.1 Big created by José María Gómez Hidalgo and public available at: http://www.esp.uem.es/jmgomez/smsspamcorpus/\n\n1.2. Statistics\n\nThere is one collection:\n\nThe SMS Spam Collection v.1 (text file: smsspamcollection) has a total of 4,827 SMS legitimate messages (86.6%) and a total of 747 (13.4%) spam messages.\n\n1.3. Format\n\nThe files contain one message per line. Each line is composed by two columns: one with label (ham or spam) and other with the raw text. Here are some examples:\n\nham   What you doing?how are you? ham   Ok lar... Joking wif u oni... ham   dun say so early hor... U c already then say... ham   MY NO. IN LUTON 0125698789 RING ME IF UR AROUND! H* ham   Siva is in hostel aha:-. ham   Cos i was out shopping wif darren jus now n i called him 2 ask wat present he wan lor. Then he started guessing who i was wif n he finally guessed darren lor. spam   FreeMsg: Txt: CALL to No: 86888 & claim your reward of 3 hours talk time to use from your phone now! ubscribe6GBP/ mnth inc 3hrs 16 stop?txtStop spam   Sunshine Quiz! Win a super Sony DVD recorder if you canname the capital of Australia? Text MQUIZ to 82277. B spam   URGENT! Your Mobile No 07808726822 was awarded a L2,000 Bonus Caller Prize on 02/09/03! This is our 2nd attempt to contact YOU! Call 0871-872-9758 BOX95QU\n\nNote: messages are not chronologically sorted.\n\nUSAGE\n\n\n\nWe offer a comprehensive study of this corpus in the following paper that is under review. This work presents a number of statistics, studies and baseline results for several machine learning methods.\n\n[1] Almeida, T.A., Gómez Hidalgo, J.M., Yamakami, A. Contributions to the study of SMS Spam Filtering: New Collection and Results. Proceedings of the 2011 ACM Symposium on Document Engineering (ACM DOCENG'11), Mountain View, CA, USA, 2011. (Under review)\n\nABOUT\n\n\n\nThe corpus has been collected by Tiago Agostinho de Almeida (http://www.dt.fee.unicamp.br/~tiago) and José María Gómez Hidalgo (http://www.esp.uem.es/jmgomez).\n\nWe would like to thank Dr. Min-Yen Kan (http://www.comp.nus.edu.sg/~kanmy/) and his team for making the NUS SMS Corpus available. See: http://www.comp.nus.edu.sg/~rpnlpir/downloads/corpora/smsCorpus/. He is currently collecting a bigger SMS corpus at: http://wing.comp.nus.edu.sg:8080/SMSCorpus/\n\nLICENSE/DISCLAIMER\n\n\n\nWe would appreciate if:\n\nIn case you find this corpus useful, please make a reference to previous paper and the web page: http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/ in your papers, research, etc.\nSend us a message to tiago@dt.fee.unicamp.br in case you make use of the corpus.\n\nThe SMS Spam Collection v.1 is provided for free and with no limitations excepting:\n\nTiago Agostinho de Almeida and José María Gómez Hidalgo hold the copyrigth (c) for the SMS Spam Collection v.1.\nNo Warranty/Use At Your Risk. THE CORPUS IS MADE AT NO CHARGE. ACCORDINGLY, THE CORPUS IS PROVIDED AS IS,' WITHOUT WARRANTY OF ANY KIND, INCLUDING WITHOUT LIMITATION THE WARRANTIES THAT THEY ARE MERCHANTABLE, FIT FOR A PARTICULAR PURPOSE OR NON-INFRINGING. YOU ARE SOLELY RESPONSIBLE FOR YOUR USE, DISTRIBUTION, MODIFICATION, REPRODUCTION AND PUBLICATION OF THE CORPUS AND ANY DERIVATIVE WORKS THEREOF BY YOU AND ANY OF YOUR SUBLICENSEES (COLLECTIVELY,YOUR CORPUS USE'). THE ENTIRE RISK AS TO YOUR CORPUS USE IS BORNE BY YOU. YOU AGREE TO INDEMNIFY AND HOLD THE COPYRIGHT HOLDERS, AND THEIR AFFILIATES HARMLESS FROM ANY CLAIMS ARISING FROM OR RELATING TO YOUR CORPUS USE.\nLimitation of Liability. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR THEIR AFFILIATES, OR THE CORPUS CONTRIBUTING EDITORS, BE LIABLE FOR ANY INDIRECT, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES, INCLUDING, WITHOUT LIMITATION, DAMAGES FOR LOSS OF GOODWILL OR ANY AND ALL OTHER COMMERCIAL DAMAGES OR LOSSES, EVEN IF ADVISED OF THE POSSIBILITY THEREOF, AND REGARDLESS OF WHETHER ANY CLAIM IS BASED UPON ANY CONTRACT, TORT OR OTHER LEGAL OR EQUITABLE THEORY, RELATING OR ARISING FROM THE CORPUS, YOUR CORPUS USE OR THIS LICENSE AGREEMENT.\n\nInterface\n\nSMSSpamCollection.features`\nSMSSpamCollection.targets\n\n```julia-repl julia> using MLDatasets: SMSSpamCollection\n\njulia> targets = SMSSpamCollection.targets();\n\njulia> summary(targets) \"5574-element Vector{Any}\"\n\njulia> targets[1] \"ham\"\n\njulia> summary(features) \"5574-element Vector{Any}\"\n\n\n\n\n\n","category":"module"},{"location":"datasets/text/#MLDatasets.UD_English","page":"Text","title":"MLDatasets.UD_English","text":"UD_English\n\nDataset: Universal Dependencies - English Dependency Treebank Universal Dependencies English Web Treebank Authors: Natalia Silveira and Timothy Dozat and             Marie-Catherine de Marneffe and Samuel             Bowman and Miriam Connor and John Bauer and             Christopher D. Manning Website: https://github.com/UniversalDependencies/UD_English-EWT\n\nA Gold Standard Universal Dependencies Corpus for English, built over the source material of the English Web Treebank LDC2012T13 (https://catalog.ldc.upenn.edu/LDC2012T13).\n\n\n\n\n\n","category":"module"},{"location":"containers/overview/#Dataset-Containers","page":"Data Containers","title":"Dataset Containers","text":"","category":"section"},{"location":"containers/overview/","page":"Data Containers","title":"Data Containers","text":"MLDatasets.jl contains several reusable data containers for accessing datasets in common storage formats. This feature is a work-in-progress and subject to change.","category":"page"},{"location":"containers/overview/","page":"Data Containers","title":"Data Containers","text":"FileDataset\nTableDataset\nHDF5Dataset\nBase.close(::HDF5Dataset)\nJLD2Dataset\nBase.close(::JLD2Dataset)\nCachedDataset\nMLDatasets.make_cache","category":"page"},{"location":"containers/overview/#MLDatasets.FileDataset","page":"Data Containers","title":"MLDatasets.FileDataset","text":"FileDataset([loadfn = FileIO.load,] paths)\nFileDataset([loadfn = FileIO.load,] dir, pattern = \"*\", depth = 4)\n\nWrap a set of file paths as a dataset (traversed in the same order as paths). Alternatively, specify a dir and collect all paths that match a glob pattern (recursively globbing by depth). The glob order determines the traversal order.\n\n\n\n\n\n","category":"type"},{"location":"containers/overview/#MLDatasets.TableDataset","page":"Data Containers","title":"MLDatasets.TableDataset","text":"TableDataset(table)\nTableDataset(path::AbstractString)\n\nWrap a Tables.jl-compatible table as a dataset container. Alternatively, specify the path to a CSV file directly to load it with CSV.jl + DataFrames.jl.\n\n\n\n\n\n","category":"type"},{"location":"containers/overview/#MLDatasets.HDF5Dataset","page":"Data Containers","title":"MLDatasets.HDF5Dataset","text":"HDF5Dataset(file::AbstractString, paths)\nHDF5Dataset(fid::HDF5.File, paths::Union{HDF5.Dataset, Vector{HDF5.Dataset}})\nHDF5Dataset(fid::HDF5.File, paths::Union{AbstractString, Vector{<:AbstractString}})\nHDF5Dataset(fid::HDF5.File, paths::Union{HDF5.Dataset, Vector{HDF5.Dataset}}, shapes)\n\nWrap several HDF5 datasets (paths) as a single dataset container. Each dataset p in paths should be accessible as fid[p]. Calling getobs on a HDF5Dataset returns a tuple with each element corresponding to the observation from each dataset in paths. See close(::HDF5Dataset) for closing the underlying HDF5 file pointer.\n\nFor array datasets, the last dimension is assumed to be the observation dimension. For scalar datasets, the stored value is returned by getobs for any index.\n\n\n\n\n\n","category":"type"},{"location":"containers/overview/#Base.close-Tuple{HDF5Dataset}","page":"Data Containers","title":"Base.close","text":"close(dataset::HDF5Dataset)\n\nClose the underlying HDF5 file pointer for dataset.\n\n\n\n\n\n","category":"method"},{"location":"containers/overview/#MLDatasets.JLD2Dataset","page":"Data Containers","title":"MLDatasets.JLD2Dataset","text":"JLD2Dataset(file::AbstractString, paths)\nJLD2Dataset(fid::JLD2.JLDFile, paths::Union{String, Vector{String}})\n\nWrap several JLD2 datasets (paths) as a single dataset container. Each dataset p in paths should be accessible as fid[p]. Calling getobs on a JLD2Dataset is equivalent to mapping getobs on each dataset in paths. See close(::JLD2Dataset) for closing the underlying JLD2 file pointer.\n\n\n\n\n\n","category":"type"},{"location":"containers/overview/#Base.close-Tuple{JLD2Dataset}","page":"Data Containers","title":"Base.close","text":"close(dataset::JLD2Dataset)\n\nClose the underlying JLD2 file pointer for dataset.\n\n\n\n\n\n","category":"method"},{"location":"containers/overview/#MLDatasets.CachedDataset","page":"Data Containers","title":"MLDatasets.CachedDataset","text":"CachedDataset(source, cachesize = numbobs(source))\nCachedDataset(source, cacheidx = 1:numbobs(source))\nCachedDataset(source, cacheidx, cache)\n\nWrap a source data container and cache cachesize samples in memory. This can be useful for improving read speeds when source is a lazy data container, but your system memory is large enough to store a sizeable chunk of it.\n\nBy default the observation indices 1:cachesize are cached. You can manually pass in a set of cacheidx as well.\n\nSee also make_cache for customizing the default cache creation for source.\n\n\n\n\n\n","category":"type"},{"location":"containers/overview/#MLDatasets.make_cache","page":"Data Containers","title":"MLDatasets.make_cache","text":"make_cache(source, cacheidx)\n\nReturn a in-memory copy of source at observation indices cacheidx. Defaults to getobs(source, cacheidx).\n\n\n\n\n\n","category":"function"},{"location":"LICENSE/#LICENSE","page":"LICENSE","title":"LICENSE","text":"","category":"section"},{"location":"LICENSE/","page":"LICENSE","title":"LICENSE","text":"using Markdown\nMarkdown.parse_file(joinpath(@__DIR__, \"..\", \"..\", \"LICENSE\"))","category":"page"},{"location":"datasets/graphs/#Graph-Datasets","page":"Graphs","title":"Graph Datasets","text":"","category":"section"},{"location":"datasets/graphs/#Index","page":"Graphs","title":"Index","text":"","category":"section"},{"location":"datasets/graphs/","page":"Graphs","title":"Graphs","text":"Pages = [\"graphs.md\"]","category":"page"},{"location":"datasets/graphs/#Documentation","page":"Graphs","title":"Documentation","text":"","category":"section"},{"location":"datasets/graphs/","page":"Graphs","title":"Graphs","text":"CiteSeer\nCiteSeer.dataset\nCora\nCora.dataset\nOGBDataset\nPolBlogs\nPolBlogs.edge_index\nPolBlogs.labels\nPubMed\nPubMed.dataset\nTUDataset\nKarateClub\nKarateClub.edge_index\nKarateClub.labels","category":"page"},{"location":"datasets/graphs/#MLDatasets.CiteSeer","page":"Graphs","title":"MLDatasets.CiteSeer","text":"CiteSeer\n\nThe CiteSeer citation network dataset from Ref. [1]. Nodes represent documents and edges represent citation links. The dataset is designed for the node classification task.  The task is to predict the category of certain paper. The dataset is retrieved from Ref. [2].\n\nInterface\n\nCiteSeer.dataset\n\nReferences\n\n[1]: Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking [2]: Planetoid\n\n\n\n\n\n","category":"module"},{"location":"datasets/graphs/#MLDatasets.CiteSeer.dataset","page":"Graphs","title":"MLDatasets.CiteSeer.dataset","text":"dataset(; dir=nothing, reverse_edges=true)\n\nRetrieve the CiteSeer dataset. The output is a named tuple with fields\n\njulia> keys(CiteSeer.dataset())\n(:node_features, :node_labels, :adjacency_list, :train_indices, :val_indices, :test_indices, :num_classes, :num_nodes, :num_edges, :directed)\n\nIn particular, adjacency_list is a vector of vector,  where adjacency_list[i] will contain the neighbors of node i through outgoing edges.\n\nIf reverse_edges=true, the graph will contain the reverse of each edge and the graph will be undirected.\n\nSee also CiteSeer.\n\nUsage Examples\n\nusing MLDatasets: CiteSeer\ndata = CiteSeer.dataset()\ntrain_labels = data.node_labels[data.train_indices]\n\n\n\n\n\n","category":"function"},{"location":"datasets/graphs/#MLDatasets.Cora","page":"Graphs","title":"MLDatasets.Cora","text":"Cora\n\nThe Cora citation network dataset from Ref. [1]. Nodes represent documents and edges represent citation links. Each node has a predefined feature with 1433 dimensions.  The dataset is designed for the node classification task.  The task is to predict the category of certain paper. The dataset is retrieved from Ref. [2].\n\nStatistics\n\nNodes: 2708\nEdges: 10556\nNumber of Classes: 7\nLabel split:\nTrain:  140\nVal:    500\nTest:  1000\n\nThe split is the one used in the original paper [1] and  doesn't consider all nodes.\n\nInterface\n\nCora.dataset\n\nReferences\n\n[1]: Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking [2]: [Planetoid](https://github.com/kimiyoung/planetoid\n\n\n\n\n\n","category":"module"},{"location":"datasets/graphs/#MLDatasets.Cora.dataset","page":"Graphs","title":"MLDatasets.Cora.dataset","text":"dataset(; dir=nothing, reverse_edges=true)\n\nRetrieve the Cora dataset. The output is a named tuple with fields\n\njulia> keys(Cora.dataset())\n(:node_features, :node_labels, :adjacency_list, :train_indices, :val_indices, :test_indices, :num_classes, :num_nodes, :num_edges, :directed)\n\nIn particular, adjacency_list is a vector of vector,  where adjacency_list[i] will contain the neighbors of node i through outgoing edges.\n\nIf reverse_edges=true, the graph will contain the reverse of each edge and the graph will be undirected.\n\nSee also Cora.\n\nUsage Examples\n\nusing MLDatasets: Cora\n\ndata = Cora.dataset()\ntrain_labels = data.node_labels[data.train_indices]\n\n\n\n\n\n","category":"function"},{"location":"datasets/graphs/#MLDatasets.OGBDataset","page":"Graphs","title":"MLDatasets.OGBDataset","text":"OGBDataset(name; dir=nothing)\n\nThe collection of datasets from the Open Graph Benchmark: Datasets for Machine Learning on Graphs paper. \n\nname is the name  of one of the dasets (listed here) available for node prediction, edge prediction, or graph prediction tasks.\n\nThe OGBDataset type stores the graphs internally as dictionary objects.  The key \"edgeindex\" contains `2 x numedges`, where the first and second column contain the source and target nodes of each edge respectively.\n\nExamples\n\nNode prediction tasks\n\njulia> data = OGBDataset(\"ogbn-arxiv\")\nOGBDataset{Vector{Any}}:\n  name => ogbn-arxiv\n  path => /home/carlo/.julia/datadeps/OGBDataset/arxiv\n  metadata => Dict{String, Any} with 15 entries\n  graphs => 1-element Vector{Dict}\n  labels => 1-element Vector{Any}\n  split => Dict{String, Any} with 3 entries\n\n\njulia> data.metadata\nDict{String, Any} with 15 entries:\n  \"num classes\"           => 40\n  \"binary\"                => false\n  \"is hetero\"             => false\n  \"eval metric\"           => \"acc\"\n  \"task type\"             => \"multiclass classification\"\n  \"version\"               => 1\n  \"split\"                 => \"time\"\n  \"download_name\"         => \"arxiv\"\n  \"num tasks\"             => 1\n  \"url\"                   => \"http://snap.stanford.edu/ogb/data/nodeproppred/arxiv.zip\"\n  \"additional node files\" => \"node_year\"\n  \"add_inverse_edge\"      => false\n  \"has_node_attr\"         => true\n  \"additional edge files\" => nothing\n  \"has_edge_attr\"         => false\n\njulia> data.split\nDict{String, Any} with 3 entries:\n  \"test_idx\"  => [347, 399, 452, 481, 489, 491, 527, 538, 541, 603  …  169334, 169335, 169336, 169337, 169338, 169339, 169340, 169341, 169342, 169343]\n  \"train_idx\" => [1, 2, 3, 4, 5, 6, 7, 8, 9, 10  …  169110, 169112, 169113, 169114, 169115, 169116, 169118, 169146, 169149, 169252]\n  \"val_idx\"   => [350, 358, 367, 383, 394, 422, 430, 436, 468, 470  …  169089, 169096, 169108, 169111, 169128, 169156, 169177, 169186, 169262, 169297]\n\njulia> length(data)\n1\n\njulia> graph, labels = data[1];\n\njulia> graph\nDict{String, Any} with 6 entries:\n  \"edge_index\" => [104448 13092; 15859 47284; … ; 45119 162538; 45119 72718]\n  \"edge_feat\"  => nothing\n  \"node_feat\"  => Float32[-0.057943 -0.1245 … -0.138236 -0.029875; -0.05253 -0.070665 … 0.040885 0.268417; … ; -0.172796 -0.372111 … -0.041253 0.077647; -0.140059 -0.301036 … -0.376132 -0.091018]\n  \"num_nodes\"  => 169343\n  \"node_year\"  => [2013 2015 … 2020 2020]\n  \"num_edges\"  => 1166243\n\njulia> source, target = graph[\"edge_index][:,1], graph[\"edge_index][:,2];\n\nEdge prediction task\n\njulia> data = OGBDataset(\"ogbl-collab\")\nOGBDataset{Nothing}:\n  name => ogbl-collab\n  path => /home/carlo/.julia/datadeps/OGBDataset/collab\n  metadata => Dict{String, Any} with 13 entries\n  graphs => 1-element Vector{Dict}\n  labels => nothing\n  split => Dict{String, Any} with 3 entries\n\njulia> graph = data[1]  # no labels for this dataset\nDict{String, Any} with 7 entries:\n  \"edge_index\"  => [150990 224882; 150990 224882; … ; 221742 135759; 207233 140615]\n  \"edge_feat\"   => nothing\n  \"node_feat\"   => Float32[-0.177486 -0.237488 … 0.004236 -0.035025; -0.10298 0.022193 … 0.031942 -0.118059; … ; 0.003879 0.062124 … 0.05208 -0.176961; -0.276317 -0.081464 … -0.201557 -0.258715]\n  \"num_nodes\"   => 235868\n  \"edge_year\"   => [2004 2002 … 2006 1984; 2004 2002 … 2006 1984]\n  \"edge_weight\" => [2 1 … 1 1; 2 1 … 1 1]\n  \"num_edges\"   => 2358104\n\nGraph prediction task\n\njulia> data = OGBDataset(\"ogbg-molhiv\")\nOGBDataset{Matrix{Int64}}:\n  name => ogbg-molhiv\n  path => /home/carlo/.julia/datadeps/OGBDataset/molhiv\n  metadata => Dict{String, Any} with 15 entries\n  graphs => 41127-element Vector{Dict}\n  labels => 1×41127 Matrix{Int64}\n  split => Dict{String, Any} with 3 entries\n\njulia> length(data)\n41127\n\njulia> graph, labels = data[10]\n(Dict{String, Any}(\"edge_index\" => [-202 -201; -201 -200; … ; -198 -184; -201 -202], \"node_feat\" => Float32[7.0 6.0 … 7.0 7.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], \"edge_feat\" => Float32[0.0 0.0 … 0.0 1.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 1.0], \"num_nodes\" => 20, \"num_edges\" => 42), [0])\n\njulia> graph, labels = data[10];\n\njulia> graph\nDict{String, Any} with 5 entries:\n  \"edge_index\" => [1 2; 2 3; … ; 5 19; 2 1]\n  \"edge_feat\"  => Float32[0.0 0.0 … 0.0 1.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 1.0]\n  \"node_feat\"  => Float32[7.0 6.0 … 7.0 7.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]\n  \"num_nodes\"  => 20\n  \"num_edges\"  => 42\n\njulia> labels\n1-element Vector{Int64}:\n 0\n\n\n\n\n\n","category":"type"},{"location":"datasets/graphs/#MLDatasets.PolBlogs","page":"Graphs","title":"MLDatasets.PolBlogs","text":"PolBlogs\n\nThe Political Blogs dataset from the The Political Blogosphere and the 2004 US Election: Divided they Blog paper.\n\nPolBlogs is a graph with 1,490 vertices (representing political blogs) and 19,025 edges (links between blogs).\n\nThe links are automatically extracted from a crawl of the front page of the blog. \n\nEach vertex receives a label indicating the political leaning of the blog: liberal or conservative.\n\nInterface\n\nPolBlogs.edge_index\nPolBlogs.labels\n\n\n\n\n\n","category":"module"},{"location":"datasets/graphs/#MLDatasets.PolBlogs.edge_index","page":"Graphs","title":"MLDatasets.PolBlogs.edge_index","text":"edge_index(; dir = nothing)\n\nReturns a 19025 x 2 matrix containing edge indices where first column as source node and second column as target node together they represent an edge\n\nusing MLDatasets: PolBlogs\nadj = PolBlogs.edge_index()\n\n\n\n\n\n","category":"function"},{"location":"datasets/graphs/#MLDatasets.PolBlogs.labels","page":"Graphs","title":"MLDatasets.PolBlogs.labels","text":"labels(; dir = nothing)\n\nReturns a vector containing the 1490 labels.\n\nusing MLDatasets: PolBlogs\nlabels = PolBlogs.labels()\n\n\n\n\n\n","category":"function"},{"location":"datasets/graphs/#MLDatasets.PubMed","page":"Graphs","title":"MLDatasets.PubMed","text":"PubMed\n\nThe PubMed citation network dataset from Ref. [1]. Nodes represent documents and edges represent citation links. The dataset is designed for the node classification task.  The task is to predict the category of certain paper. The dataset is retrieved from Ref. [2].\n\nInterface\n\nPubMed.dataset\n\nReferences\n\n[1]: Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking [2]: Planetoid\n\n\n\n\n\n","category":"module"},{"location":"datasets/graphs/#MLDatasets.PubMed.dataset","page":"Graphs","title":"MLDatasets.PubMed.dataset","text":"dataset(; dir=nothing)\n\nRetrieve the PubMed dataset. The output is a named tuple with fields\n\njulia> keys(PubMed.dataset())\n(:node_features, :node_labels, :adjacency_list, :train_indices, :val_indices, :test_indices, :num_classes, :num_nodes, :num_edges, :directed)\n\nIn particular, adjacency_list is a vector of vector,  where adjacency_list[i] will contain the neighbors of node i through outgoing edges.\n\nIf reverse_edges=true, the graph will contain the reverse of each edge and the graph will be undirected.\n\nSee also PubMed.\n\nUsage Examples\n\nusing MLDatasets: PubMed\ndata = PubMed.dataset()\ntrain_labels = data.node_labels[data.train_indices]\n\n\n\n\n\n","category":"function"},{"location":"datasets/graphs/#MLDatasets.TUDataset","page":"Graphs","title":"MLDatasets.TUDataset","text":"TUDataset(name; dir=nothing)\n\nA variety of graph benchmark datasets, .e.g. \"QM9\", \"IMDB-BINARY\", \"REDDIT-BINARY\" or \"PROTEINS\", collected from the TU Dortmund University. Retrieve from TUDataset collection the dataset name, where name is any of the datasets available here. \n\nA TUDataset object can be indexed to retrieve a specific graph or a subset of graphs.\n\nInternal fields\n\nnum_nodes           # total number of nodes (considering all graphs)\nnum_edges           # total number of edges (considering all graphs)       \nnum_graphs          # total number of graphs\nsource              # vector of edges' source vectors      \ntarget              # vector of edges' target vectors\ngraph_indicator     # graph to which a node belongs too\nnode_labels\nedge_labels\ngraph_labels\nnode_attributes\nedge_attributes\ngraph_attributes\n\nSee here for an in-depth  description of the format. \n\nUsage Example\n\nusing MLDatasets: TUDataset\nusing LightGraphs: SimpleGraph, add_edge!\n\ndata = TUDataset(\"PROTEINS\")\n\n# Access first graph\nd1 = data[1] \n\n# Create a LightGraphs' graph\ng = SimpleGraph(d1.num_nodes)\nfor (s, t) in zip(d1.source, d1.target)\n    add_edge!(g, s, t)\nend\n\n# Node features\nX = d1.node_attributes # (nfeatures x nnodes) matrix\n\n\n\n\n\n","category":"type"},{"location":"datasets/graphs/#MLDatasets.KarateClub","page":"Graphs","title":"MLDatasets.KarateClub","text":"Zachary's Karate Club\n\nThe Karate Club dataset originally appeared in Ref [1].\n\nThe network contains 34 nodes (members of the karate club). The nodes are connected by 78 undirected and unweighted edges. The edges indicate if the two members interacted outside the club.\n\nThe node labels indicate which community or the karate club the member belongs to. The club based labels are as per the original dataset in Ref [1]. The community labels are obtained by modularity-based clustering following Ref [2]. The data is retrieved from Ref [3] and [4]. One node per unique label is used as training data.\n\nInterface\n\nKarateClub.edge_index\nKarateClub.labels\n\nReferences\n\n[1]: An Information Flow Model for Conflict and Fission in Small Groups [2]: Semi-supervised Classification with Graph Convolutional Networks [3]: PyTorch Geometric Karate Club Dataset [4]: NetworkX Zachary's Karate Club Dataset\n\n\n\n\n\n","category":"module"},{"location":"datasets/graphs/#MLDatasets.KarateClub.edge_index","page":"Graphs","title":"MLDatasets.KarateClub.edge_index","text":"edge_index()\n\nReturns an adjacency list where the first and second vector contain the source and target nodes of each edge respectively.\n\nusing MLDatasets: KarateClub\nadj = KarateClub.edge_index()\n\n\n\n\n\n","category":"function"},{"location":"datasets/graphs/#MLDatasets.KarateClub.labels","page":"Graphs","title":"MLDatasets.KarateClub.labels","text":"labels(mode = :club)\n\nReturns a vector containing the node labels indicating which club or community the node belongs to.\n\nThe type of labels can be specifed using the mode variable which takes two values: :club and :community.\n\nusing MLDatasets: KarateClub\n\n## Club labels\nclub_labels = KarateClub.labels(:club)\n\n## Community labels\ncom_labels = KarateClub.labels(:community) \n\n\n\n\n\n","category":"function"},{"location":"#MLDatasets.jl's-Documentation","page":"Home","title":"MLDatasets.jl's Documentation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package represents a community effort to provide a common interface for accessing common Machine Learning (ML) datasets. In contrast to other data-related Julia packages, the focus of MLDatasets.jl is specifically on downloading, unpacking, and accessing benchmark dataset. Functionality for the purpose of data processing or visualization is only provided to a degree that is special to some dataset.","category":"page"},{"location":"","page":"Home","title":"Home","text":"This package is a part of the JuliaML ecosystem. Its functionality is build on top of the package DataDeps.jl.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To install MLDatasets.jl, start up Julia and type the following code snippet into the REPL. It makes use of the native Julia package manger.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Pkg.add(\"MLDatasets\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"Additionally, for example if you encounter any sudden issues, or in the case you would like to contribute to the package, you can manually choose to be on the latest (untagged) version.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Pkg.checkout(\"MLDatasets\")","category":"page"},{"location":"#Basic-Usage","page":"Home","title":"Basic Usage","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The way MLDatasets.jl is organized is that each dataset has its own dedicated sub-module. Where possible, those sub-module share a common interface for interacting with the datasets. For example you can load the training set and the test set of the MNIST database of handwritten digits using the following commands:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using MLDatasets\n\ntrain_x, train_y = MNIST.traindata()\ntest_x,  test_y  = MNIST.testdata()","category":"page"},{"location":"","page":"Home","title":"Home","text":"To load the data the package looks for the necessary files in various locations (see DataDeps.jl for more information on how to configure such defaults). If the data can't be found in any of those locations, then the package will trigger a download dialog to ~/.julia/datadeps/MNIST. To overwrite this on a case by case basis, it is possible to specify a data directory directly in traindata(dir = <directory>) and testdata(dir = <directory>).","category":"page"},{"location":"utils/#Utils","page":"Utils","title":"Utils","text":"","category":"section"},{"location":"utils/","page":"Utils","title":"Utils","text":"MLDatasets.read_planetoid_data","category":"page"},{"location":"utils/#MLDatasets.read_planetoid_data","page":"Utils","title":"MLDatasets.read_planetoid_data","text":"Read any of the citation network datasets “Cora”, “CiteSeer” and “PubMed”  from the “Revisiting Semi-Supervised Learning with Graph Embeddings” paper.  Nodes represent documents and edges represent citation links. \n\nData collected from  https://github.com/kimiyoung/planetoid/raw/master/data\n\n\n\n\n\n","category":"function"}]
}

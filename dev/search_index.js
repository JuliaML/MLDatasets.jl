var documenterSearchIndex = {"docs":
[{"location":"datasets/vision/#Vision-Datasets","page":"Vision","title":"Vision Datasets","text":"","category":"section"},{"location":"datasets/vision/","page":"Vision","title":"Vision","text":"A collection of datasets for 2d computer vision. ","category":"page"},{"location":"datasets/vision/","page":"Vision","title":"Vision","text":"Numerical arrays can be converted to color images using  convert2image, and displayed in the terminal with the package ImageInTerminal.jl","category":"page"},{"location":"datasets/vision/#Index","page":"Vision","title":"Index","text":"","category":"section"},{"location":"datasets/vision/","page":"Vision","title":"Vision","text":"Pages = [\"vision.md\"]","category":"page"},{"location":"datasets/vision/#Documentation","page":"Vision","title":"Documentation","text":"","category":"section"},{"location":"datasets/vision/","page":"Vision","title":"Vision","text":"convert2image","category":"page"},{"location":"datasets/vision/#MLDatasets.convert2image","page":"Vision","title":"MLDatasets.convert2image","text":"convert2image(d, i)\nconvert2image(d, x)\nconvert2image(DType, x)\n\nConvert the observation(s) i from dataset d to image(s). It can also convert a numerical array x.\n\nIn order to support a new dataset, e.g. MyDataset,  implement convert2image(::Type{MyDataset}, x::AbstractArray).\n\nExamples\n\njulia> using MLDatasets, ImageInTerminal\n\njulia> d = MNIST()\n\njulia> convert2image(d, 1:2) \n# You should see 2 images in the terminal\n\njulia> x = d[1].features;\n\njulia> convert2image(MNIST, x) # or convert2image(d, x)\n\n\n\n\n\n","category":"function"},{"location":"datasets/vision/","page":"Vision","title":"Vision","text":"CIFAR10\nCIFAR100\nEMNIST\nFashionMNIST\nMNIST\nOmniglot\nSVHN2","category":"page"},{"location":"datasets/vision/#MLDatasets.CIFAR10","page":"Vision","title":"MLDatasets.CIFAR10","text":"CIFAR10(; Tx=Float32, split=:train, dir=nothing)\nCIFAR10([Tx, split])\n\nThe CIFAR10 dataset is a labeled subsets of the 80 million tiny images dataset. It consists of 60000 32x32 colour images in 10 classes, with 6000 images per class.\n\nArguments\n\nYou can pass a specific dir where to load or download the dataset, otherwise uses the default one.\nsplit: selects the data partition. Can take the values :train or :test. \n\nFields\n\nmetadata: A dictionary containing additional information on the dataset.\nfeatures: An array storing the data features.\ntargets: An array storing the targets for supervised learning.\nsplit.\n\nMethods\n\ndataset[i]: Return observation(s) i as a named tuple of features and targets.\ndataset[:]: Return all observations as a named tuple of features and targets.\nlength(dataset): Number of observations.\nconvert2image converts features to RGB images.\n\nExamples\n\njulia> using MLDatasets: CIFAR10\n\njulia> dataset = CIFAR10()\nCIFAR10:\n  metadata    =>    Dict{String, Any} with 2 entries\n  split       =>    :train\n  features    =>    32×32×3×50000 Array{Float32, 4}\n  targets     =>    50000-element Vector{Int64}\n\njulia> dataset[1:5].targets\n5-element Vector{Int64}:\n 6\n 9\n 9\n 4\n 1\n\njulia> X, y = dataset[:];\n\njulia> dataset = CIFAR10(Tx=Float64, split=:test)\nCIFAR10:\n  metadata    =>    Dict{String, Any} with 2 entries\n  split       =>    :test\n  features    =>    32×32×3×10000 Array{Float64, 4}\n  targets     =>    10000-element Vector{Int64}\n\njulia> dataset.metadata\nDict{String, Any} with 2 entries:\n  \"n_observations\" => 10000\n  \"class_names\"    => [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n\n\n\n\n\n","category":"type"},{"location":"datasets/vision/#MLDatasets.CIFAR100","page":"Vision","title":"MLDatasets.CIFAR100","text":"CIFAR100(; Tx=Float32, split=:train, dir=nothing)\nCIFAR100([Tx, split])\n\nThe CIFAR100 dataset is a labeled subsets of the 80 million tiny images dataset. It consists of 60000 32x32 colour images in 100 classes and 20 superclasses, with 600 images per class.\n\nReturn the CIFAR-100 trainset labels (coarse and fine) corresponding to the given indices as a tuple of two Int or two Vector{Int}. The variables returned are the coarse label(s) (Yc) and the fine label(s) (Yf) respectively.\n\nArguments\n\nYou can pass a specific dir where to load or download the dataset, otherwise uses the default one.\nsplit: selects the data partition. Can take the values :train or :test. \n\nFields\n\nmetadata: A dictionary containing additional information on the dataset.\nfeatures: An array storing the data features.\ntargets: An array storing the targets for supervised learning.\nsplit.\n\nMethods\n\ndataset[i]: Return observation(s) i as a named tuple of features and targets.\ndataset[:]: Return all observations as a named tuple of features and targets.\nlength(dataset): Number of observations.\nconvert2image converts features to RGB images.\n\nExamples\n\njulia> dataset = CIFAR100()\nCIFAR100:\n  metadata    =>    Dict{String, Any} with 3 entries\n  split       =>    :train\n  features    =>    32×32×3×50000 Array{Float32, 4}\n  targets     =>    (coarse = \"50000-element Vector{Int64}\", fine = \"50000-element Vector{Int64}\")\n\njulia> dataset[1:5].targets\n(coarse = [11, 15, 4, 14, 1], fine = [19, 29, 0, 11, 1])\n\njulia> X, y = dataset[:];\n\njulia> dataset.metadata\nDict{String, Any} with 3 entries:\n  \"n_observations\"     => 50000\n  \"class_names_coarse\" => [\"aquatic_mammals\", \"fish\", \"flowers\", \"food_containers\", \"fruit_and_vegetables\", \"household_electrical_devices\", \"household_furniture\", \"insects\", \"large_carnivores\", \"large_man-made_…\n  \"class_names_fine\"   => [\"apple\", \"aquarium_fish\", \"baby\", \"bear\", \"beaver\", \"bed\", \"bee\", \"beetle\", \"bicycle\", \"bottle\"  …  \"train\", \"trout\", \"tulip\", \"turtle\", \"wardrobe\", \"whale\", \"willow_tree\", \"wolf\", \"w…\n\n\n\n\n\n","category":"type"},{"location":"datasets/vision/#MLDatasets.EMNIST","page":"Vision","title":"MLDatasets.EMNIST","text":"EMNIST(name; Tx=Float32, split=:train, dir=nothing)\nEMNIST(name, [Tx, split])\n\nThe EMNIST dataset is a set of handwritten character digits derived from the NIST Special Database 19 (https://www.nist.gov/srd/nist-special-database-19) and converted to a 28x28 pixel image format and dataset structure that directly matches the MNIST dataset (http://yann.lecun.com/exdb/mnist/). Further information on the dataset contents and conversion process can be found in the paper available at https://arxiv.org/abs/1702.05373v1.\n\nArguments\n\nname: name of the EMNIST dataset. Possible values are: :balanced, :byclass, :bymerge, :digits, :letters, :mnist.\nsplit: selects the data partition. Can take the values :train or :test. \nYou can pass a specific dir where to load or download the dataset, otherwise uses the default one.\n\nFields\n\nname.\nsplit.\nmetadata: A dictionary containing additional information on the dataset.\nfeatures: An array storing the data features.\ntargets: An array storing the targets for supervised learning.\n\nMethods\n\ndataset[i]: Return observation(s) i as a named tuple of features and targets.\ndataset[:]: Return all observations as a named tuple of features and targets.\nlength(dataset): Number of observations.\nconvert2image converts features to Gray images.\n\nExamples\n\nThe images are loaded as a multi-dimensional array of eltype Tx. If Tx <: Integer, then all values will be within 0 and 255,  otherwise the values are scaled to be between 0 and 1. EMNIST().features is a 3D array (i.e. a Array{Tx,3}), in WHN format (width, height, num_images). Labels are stored as a vector of integers in EMNIST().targets. \n\njulia> using MLDatasets: EMNIST\n\njulia> dataset = EMNIST(:letters, split=:train)\nEMNIST:\n  metadata    =>    Dict{String, Any} with 3 entries\n  split       =>    :train\n  features    =>    28×28×60000 Array{Float32, 3}\n  targets     =>    60000-element Vector{Int64}\n\njulia> dataset[1:5].targets\n5-element Vector{Int64}:\n7\n2\n1\n0\n4\n\njulia> X, y = dataset[:];\n\njulia> dataset = EMNIST(:balanced, Tx=UInt8, split=:test)\nEMNIST:\n  metadata    =>    Dict{String, Any} with 3 entries\n  split       =>    :test\n  features    =>    28×28×10000 Array{UInt8, 3}\n  targets     =>    10000-element Vector{Int64}\n\n\n\n\n\n","category":"type"},{"location":"datasets/vision/#MLDatasets.FashionMNIST","page":"Vision","title":"MLDatasets.FashionMNIST","text":"FashionMNIST(; Tx=Float32, split=:train, dir=nothing)\nFashionMNIST([Tx, split])\n\nFashionMNIST is a dataset of Zalando's article images consisting of a training set of 60000 examples and a test set of 10000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. It can serve as a drop-in replacement for MNIST.\n\nAuthors: Han Xiao, Kashif Rasul, Roland Vollgraf\nWebsite: https://github.com/zalandoresearch/fashion-mnist\n\nSee MNIST for details of the interface.\n\n\n\n\n\n","category":"type"},{"location":"datasets/vision/#MLDatasets.MNIST","page":"Vision","title":"MLDatasets.MNIST","text":"MNIST(; Tx=Float32, split=:train, dir=nothing)\nMNIST([Tx, split])\n\nThe MNIST database of handwritten digits.\n\nAuthors: Yann LeCun, Corinna Cortes, Christopher J.C. Burges\nWebsite: http://yann.lecun.com/exdb/mnist/\n\nMNIST is a classic image-classification dataset that is often used in small-scale machine learning experiments. It contains 70,000 images of handwritten digits. Each observation is a 28x28 pixel gray-scale image that depicts a handwritten version of 1 of the 10 possible digits (0-9).\n\nArguments\n\nYou can pass a specific dir where to load or download the dataset, otherwise uses the default one.\nsplit: selects the data partition. Can take the values :train or :test. \n\nFields\n\nmetadata: A dictionary containing additional information on the dataset.\nfeatures: An array storing the data features.\ntargets: An array storing the targets for supervised learning.\nsplit.\n\nMethods\n\ndataset[i]: Return observation(s) i as a named tuple of features and targets.\ndataset[:]: Return all observations as a named tuple of features and targets.\nlength(dataset): Number of observations.\nconvert2image converts features to Gray images.\n\nExamples\n\nThe images are loaded as a multi-dimensional array of eltype Tx. If Tx <: Integer, then all values will be within 0 and 255,  otherwise the values are scaled to be between 0 and 1. MNIST().features is a 3D array (i.e. a Array{Tx,3}), in WHN format (width, height, num_images). Labels are stored as a vector of integers in MNIST().targets. \n\njulia> using MLDatasets: MNIST\n\njulia> dataset = MNIST(:train)\nMNIST:\n  metadata    =>    Dict{String, Any} with 3 entries\n  split       =>    :train\n  features    =>    28×28×60000 Array{Float32, 3}\n  targets     =>    60000-element Vector{Int64}\n\njulia> dataset[1:5].targets\n5-element Vector{Int64}:\n7\n2\n1\n0\n4\n\njulia> X, y = dataset[:];\n\njulia> dataset = MNIST(UInt8, :test)\nMNIST:\n  metadata    =>    Dict{String, Any} with 3 entries\n  split       =>    :test\n  features    =>    28×28×10000 Array{UInt8, 3}\n  targets     =>    10000-element Vector{Int64}\n\n\n\n\n\n","category":"type"},{"location":"datasets/vision/#MLDatasets.Omniglot","page":"Vision","title":"MLDatasets.Omniglot","text":"Omniglot(; Tx=Float32, split=:train, dir=nothing)\nOmniglot([Tx, split])\n\nOmniglot data set for one-shot learning\n\nAuthors: Brenden M. Lake, Ruslan Salakhutdinov, Joshua B. Tenenbaum\nWebsite: https://github.com/brendenlake/omniglot\n\nThe Omniglot data set is designed for developing more human-like learning algorithms. It contains 1623 different handwritten characters from 50 different alphabets. Each of the 1623 characters was drawn online via Amazon's Mechanical Turk by 20 different people. Each image is paired with stroke data, a sequences of [x,y,t] coordinates with time (t) in milliseconds.\n\nArguments\n\nYou can pass a specific dir where to load or download the dataset, otherwise uses the default one.\nsplit: selects the data partition. Can take the values :train, :test, :small1, or :small2. \n\nFields\n\nmetadata: A dictionary containing additional information on the dataset.\nfeatures: An array storing the data features.\ntargets: An array storing the targets for supervised learning.\nsplit.\n\nMethods\n\ndataset[i]: Return observation(s) i as a named tuple of features and targets.\ndataset[:]: Return all observations as a named tuple of features and targets.\nlength(dataset): Number of observations.\nconvert2image converts features to Gray images.\n\nExamples\n\nThe images are loaded as a multi-dimensional array of eltype Tx. All values will be 0 or 1. Omniglot().features is a 3D array (i.e. a Array{Tx,3}), in WHN format (width, height, num_images). Labels are stored as a vector of strings in Omniglot().targets. \n\njulia> using MLDatasets: Omniglot\n\njulia> dataset = Omniglot(:train)\nOmniglot:\n  metadata    =>    Dict{String, Any} with 3 entries\n  split       =>    :train\n  features    =>    105×105×19280 Array{Float32, 3}\n  targets     =>    19280-element Vector{Int64}\n\njulia> dataset[1:5].targets\n5-element Vector{String}:\n \"Arcadian\"\n \"Arcadian\"\n \"Arcadian\"\n \"Arcadian\"\n \"Arcadian\"\n\njulia> X, y = dataset[:];\n\njulia> dataset = Omniglot(UInt8, :test)\nOmniglot:\n  metadata    =>    Dict{String, Any} with 3 entries\n  split       =>    :test\n  features    =>    105×105×13180 Array{UInt8, 3}\n  targets     =>    13180-element Vector{Int64}\n\n\n\n\n\n","category":"type"},{"location":"datasets/vision/#MLDatasets.SVHN2","page":"Vision","title":"MLDatasets.SVHN2","text":"SVHN2(; Tx=Float32, split=:train, dir=nothing)\nSVHN2([Tx, split])\n\nThe Street View House Numbers (SVHN) Dataset.\n\nAuthors: Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, Andrew Y. Ng\nWebsite: http://ufldl.stanford.edu/housenumbers\n\nSVHN was obtained from house numbers in Google Street View images. As such they are quite diverse in terms of orientation and image background. Similar to MNIST, SVHN has 10 classes (the digits 0-9), but unlike MNIST there is more data and the images are a little bigger (32x32 instead of 28x28) with an additional RGB color channel. The dataset is split up into three subsets: 73257 digits for training, 26032 digits for testing, and 531131 additional to use as extra training data.\n\nArguments\n\nYou can pass a specific dir where to load or download the dataset, otherwise uses the default one.\nsplit: selects the data partition. Can take the values :train, :test or :extra. \n\nFields\n\nmetadata: A dictionary containing additional information on the dataset.\nfeatures: An array storing the data features.\ntargets: An array storing the targets for supervised learning.\nsplit.\n\nMethods\n\ndataset[i]: Return observation(s) i as a named tuple of features and targets.\ndataset[:]: Return all observations as a named tuple of features and targets.\nlength(dataset): Number of observations.\nconvert2image converts features to RGB images.\n\nExamples\n\njulia> using MLDatasets: SVHN2\n\njulia> using MLDatasets: SVHN2\n\njulia> dataset = SVHN2()\nSVHN2:\n  metadata    =>    Dict{String, Any} with 2 entries\n  split       =>    :train\n  features    =>    32×32×3×73257 Array{Float32, 4}\n  targets     =>    73257-element Vector{Int64}\n\njulia> dataset[1:5].targets\n5-element Vector{Int64}:\n 1\n 9\n 2\n 3\n 2\n\njulia> dataset.metadata\nDict{String, Any} with 2 entries:\n  \"n_observations\" => 73257\n  \"class_names\"    => [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"0\"]\n\n\n\n\n\n","category":"type"},{"location":"datasets/misc/#Miscellaneuous-Datasets","page":"Miscellaneous","title":"Miscellaneuous Datasets","text":"","category":"section"},{"location":"datasets/misc/#Index","page":"Miscellaneous","title":"Index","text":"","category":"section"},{"location":"datasets/misc/","page":"Miscellaneous","title":"Miscellaneous","text":"Pages = [\"misc.md\"]","category":"page"},{"location":"datasets/misc/#Documentation","page":"Miscellaneous","title":"Documentation","text":"","category":"section"},{"location":"datasets/misc/","page":"Miscellaneous","title":"Miscellaneous","text":"BostonHousing\nIris\nMutagenesis\nTitanic\nWine","category":"page"},{"location":"datasets/misc/#MLDatasets.BostonHousing","page":"Miscellaneous","title":"MLDatasets.BostonHousing","text":"BostonHousing(; as_df = true, dir = nothing)\n\nThe classical Boston Housing tabular dataset.\n\nSources:    (a) Origin:  This dataset was taken from the StatLib library which is                 maintained at Carnegie Mellon University.    (b) Creator:  Harrison, D. and Rubinfeld, D.L. 'Hedonic prices and the                   demand for clean air', J. Environ. Economics & Management,                  vol.5, 81-102, 1978.    (c) Date: July 7, 1993\n\nNumber of Instances: 506\n\nNumber of Attributes: 13 continuous attributes (including target                             attribute \"MEDV\"), 1 binary-valued attribute.\n\nArguments\n\nIf as_df = true, load the data as dataframes instead of plain arrays.\nYou can pass a specific dir where to load or download the dataset, otherwise uses the default one.\n\nFields\n\nmetadata: A dictionary containing additional information on the dataset.\nfeatures: The data features. An array if as_df=false, otherwise a dataframe.\ntargets: The targets for supervised learning. An array if as_df=false, otherwise a dataframe.\ndataframe: A dataframe containing both features and targets. It is nothing if as_df=false, otherwise a dataframed.\n\nMethods\n\ndataset[i]: Return observation(s) i as a named tuple of features and targets.\ndataset[:]: Return all observations as a named tuple of features and targets.\nlength(dataset): Number of observations.\n\nExamples\n\njulia> using MLDatasets: BostonHousing\n\njulia> dataset = BostonHousing()\nBostonHousing:\n  metadata => Dict{String, Any} with 5 entries\n  features => 506×13 DataFrame\n  targets => 506×1 DataFrame\n  dataframe => 506×14 DataFrame\n\n\njulia> dataset[1:5][1]\n5×13 DataFrame\n Row │ CRIM     ZN       INDUS    CHAS   NOX      RM       AGE      DIS      RAD    TAX    PTRATIO  B        LSTAT   \n     │ Float64  Float64  Float64  Int64  Float64  Float64  Float64  Float64  Int64  Int64  Float64  Float64  Float64 \n─────┼───────────────────────────────────────────────────────────────────────────────────────────────────────────────\n   1 │ 0.00632     18.0     2.31      0    0.538    6.575     65.2   4.09        1    296     15.3   396.9      4.98\n   2 │ 0.02731      0.0     7.07      0    0.469    6.421     78.9   4.9671      2    242     17.8   396.9      9.14\n   3 │ 0.02729      0.0     7.07      0    0.469    7.185     61.1   4.9671      2    242     17.8   392.83     4.03\n   4 │ 0.03237      0.0     2.18      0    0.458    6.998     45.8   6.0622      3    222     18.7   394.63     2.94\n   5 │ 0.06905      0.0     2.18      0    0.458    7.147     54.2   6.0622      3    222     18.7   396.9      5.33\n\njulia> dataset[1:5][2]\n5×1 DataFrame\nRow │ MEDV    \n    │ Float64 \n────┼─────────\n  1 │    24.0\n  2 │    21.6\n  3 │    34.7\n  4 │    33.4\n  5 │    36.2  \n\njulia> X, y = BostonHousing(as_df=false)[:]\n([0.00632 0.02731 … 0.10959 0.04741; 18.0 0.0 … 0.0 0.0; … ; 396.9 396.9 … 393.45 396.9; 4.98 9.14 … 6.48 7.88], [24.0 21.6 … 22.0 11.9])\n\n\n\n\n\n","category":"type"},{"location":"datasets/misc/#MLDatasets.Iris","page":"Miscellaneous","title":"MLDatasets.Iris","text":"Iris(; as_df = true, dir = nothing)\n\nFisher's classic iris dataset. \n\nMeasurements from 3 different species of iris: setosa, versicolor and virginica. There are 50 examples of each species.\n\nThere are 4 measurements for each example: sepal length, sepal width, petal length and petal width.  The measurements are in centimeters.\n\nThe module retrieves the data from the UCI Machine Learning Repository.\n\nNOTE: no pre-defined train-test split for this dataset. \n\nArguments\n\nIf as_df = true, load the data as dataframes instead of plain arrays.\nYou can pass a specific dir where to load or download the dataset, otherwise uses the default one.\n\nFields\n\nmetadata: A dictionary containing additional information on the dataset.\nfeatures: The data features. An array if as_df=false, otherwise a dataframe.\ntargets: The targets for supervised learning. An array if as_df=false, otherwise a dataframe.\ndataframe: A dataframe containing both features and targets. It is nothing if as_df=false, otherwise a dataframed.\n\nMethods\n\ndataset[i]: Return observation(s) i as a named tuple of features and targets.\ndataset[:]: Return all observations as a named tuple of features and targets.\nlength(dataset): Number of observations.\n\nExamples\n\njulia> dataset = Iris()\nIris:\n  metadata => Dict{String, Any} with 4 entries\n  features => 150×4 DataFrame\n  targets => 150×1 DataFrame\n  dataframe => 150×5 DataFrame\n\n\njulia> dataset[1:2]\n(2×4 DataFrame\n Row │ sepallength  sepalwidth  petallength  petalwidth \n     │ Float64      Float64     Float64      Float64    \n─────┼──────────────────────────────────────────────────\n   1 │         5.1         3.5          1.4         0.2\n   2 │         4.9         3.0          1.4         0.2, 2×1 DataFrame\n Row │ class       \n     │ String15    \n─────┼─────────────\n   1 │ Iris-setosa\n   2 │ Iris-setosa)\n\njulia> X, y = Iris(as_df=false)[:]\n([5.1 4.9 … 6.2 5.9; 3.5 3.0 … 3.4 3.0; 1.4 1.4 … 5.4 5.1; 0.2 0.2 … 2.3 1.8], InlineStrings.String15[\"Iris-setosa\" \"Iris-setosa\" … \"Iris-virginica\" \"Iris-virginica\"])\n\n\n\n\n\n","category":"type"},{"location":"datasets/misc/#MLDatasets.Mutagenesis","page":"Miscellaneous","title":"MLDatasets.Mutagenesis","text":"Mutagenesis(; split=:train, dir=nothing)\nMutagenesis(split; dir=nothing)\n\nThe Mutagenesis dataset comprises 188 molecules trialed for mutagenicity on Salmonella typhimurium, available from  relational.fit.cvut.cz and  CTUAvastLab/datasets.\n\nSet split to :train, :val, :test, or :all, to select the training,  validation, test partition respectively or the whole dataset. The indexes field in the result contains the indexes of the partition in the full dataset.\n\nWebsite: https://relational.fit.cvut.cz/dataset/Mutagenesis License: CC0\n\njulia> using MLDatasets: Mutagenesis\n\njulia> dataset = Mutagenesis(:train)\nMutagenesis dataset:\n  split : train\n  indexes : 100-element Vector{Int64}\n  features : 100-element Vector{Dict{Symbol, Any}}\n  targets : 100-element Vector{Int64}\n\njulia> dataset[1].features\nDict{Symbol, Any} with 5 entries:\n  :lumo  => -1.246\n  :inda  => 0\n  :logp  => 4.23\n  :ind1  => 1\n  :atoms => Dict{Symbol, Any}[Dict(:element=>\"c\", :bonds=>Dict{Symbol, Any}[Dict(:element=>\"c\", :bond_type=>7, :charge=>-0.117, :atom_type=>22), Dict(:element=>\"h\", :bond_type=>1, :charge=>0.142, :atom_type=>3)…\n\njulia> dataset[1].targets\n1\n\njulia> dataset = Mutagenesis(:all)\nMutagenesis dataset:\n  split : all\n  indexes : 188-element Vector{Int64}\n  features : 188-element Vector{Dict{Symbol, Any}}\n  targets : 188-element Vector{Int64}\n\n\n\n\n\n","category":"type"},{"location":"datasets/misc/#MLDatasets.Titanic","page":"Miscellaneous","title":"MLDatasets.Titanic","text":"Titanic(; as_df = true, dir = nothing)\n\nThe Titanic dataset, describing the survival of passengers on the Titanic ship.\n\nArguments\n\nIf as_df = true, load the data as dataframes instead of plain arrays.\nYou can pass a specific dir where to load or download the dataset, otherwise uses the default one.\n\nFields\n\nmetadata: A dictionary containing additional information on the dataset.\nfeatures: The data features. An array if as_df=false, otherwise a dataframe.\ntargets: The targets for supervised learning. An array if as_df=false, otherwise a dataframe.\ndataframe: A dataframe containing both features and targets. It is nothing if as_df=false, otherwise a dataframed.\n\nMethods\n\ndataset[i]: Return observation(s) i as a named tuple of features and targets.\ndataset[:]: Return all observations as a named tuple of features and targets.\nlength(dataset): Number of observations.\n\nExamples\n\njulia> using MLDatasets: Titanic\n\njulia> using DataFrames\n\njulia> dataset = Titanic()\nTitanic:\n  metadata => Dict{String, Any} with 5 entries\n  features => 891×11 DataFrame\n  targets => 891×1 DataFrame\n  dataframe => 891×12 DataFrame\n\n\njulia> describe(dataset.dataframe)\n12×7 DataFrame\n Row │ variable     mean      min                  median   max                          nmissing  eltype                   \n     │ Symbol       Union…    Any                  Union…   Any                          Int64     Type                     \n─────┼──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n   1 │ PassengerId  446.0     1                    446.0    891                                 0  Int64\n   2 │ Survived     0.383838  0                    0.0      1                                   0  Int64\n   3 │ Pclass       2.30864   1                    3.0      3                                   0  Int64\n   4 │ Name                   Abbing, Mr. Anthony           van Melkebeke, Mr. Philemon         0  String\n   5 │ Sex                    female                        male                                0  String7\n   6 │ Age          29.6991   0.42                 28.0     80.0                              177  Union{Missing, Float64}\n   7 │ SibSp        0.523008  0                    0.0      8                                   0  Int64\n   8 │ Parch        0.381594  0                    0.0      6                                   0  Int64\n   9 │ Ticket                 110152                        WE/P 5735                           0  String31\n  10 │ Fare         32.2042   0.0                  14.4542  512.329                             0  Float64\n  11 │ Cabin                  A10                           T                                 687  Union{Missing, String15}\n  12 │ Embarked               C                             S                                   2  Union{Missing, String1}\n\n\n\n\n\n","category":"type"},{"location":"datasets/misc/#MLDatasets.Wine","page":"Miscellaneous","title":"MLDatasets.Wine","text":"Wine(; as_df = true, dir = nothing)\n\nThe UCI Wine dataset.\n\nThese data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines.\n\nData source is the UCI Machine Learning Repository where further details can be retrieved.\n\nArguments\n\nIf as_df = true, load the data as dataframes instead of plain arrays.\nYou can pass a specific dir where to load or download the dataset, otherwise uses the default one.\n\nFields\n\nmetadata: A dictionary containing additional information on the dataset.\nfeatures: The data features. An array if as_df=false, otherwise a dataframe.\ntargets: The targets for supervised learning. An array if as_df=false, otherwise a dataframe.\ndataframe: A dataframe containing both features and targets. It is nothing if as_df=false, otherwise a dataframed.\n\nMethods\n\ndataset[i]: Return observation(s) i as a named tuple of features and targets.\ndataset[:]: Return all observations as a named tuple of features and targets.\nlength(dataset): Number of observations.\n\nExamples\n\njulia> using MLDatasets: Wine\n\njulia> using DataFrames\n\njulia> dataset = Wine()\ndataset Wine:\n  metadata   =>    Dict{String, Any} with 5 entries\n  features   =>    178×13 DataFrame\n  targets    =>    178×1 DataFrame\n  dataframe  =>    178×14 DataFrame\n\n\njulia> describe(dataset.dataframe)\n14×7 DataFrame\n Row │ variable              mean        min     median   max      nmissing  eltype   \n     │ Symbol                Float64     Real    Float64  Real     Int64     DataType \n─────┼────────────────────────────────────────────────────────────────────────────────\n   1 │ Wine                    1.9382      1       2.0       3            0  Int64\n   2 │ Alcohol                13.0006     11.03   13.05     14.83         0  Float64\n   3 │ Malic.acid              2.33635     0.74    1.865     5.8          0  Float64\n   4 │ Ash                     2.36652     1.36    2.36      3.23         0  Float64\n   5 │ Acl                    19.4949     10.6    19.5      30.0          0  Float64\n   6 │ Mg                     99.7416     70      98.0     162            0  Int64\n   7 │ Phenols                 2.29511     0.98    2.355     3.88         0  Float64\n   8 │ Flavanoids              2.02927     0.34    2.135     5.08         0  Float64\n   9 │ Nonflavanoid.phenols    0.361854    0.13    0.34      0.66         0  Float64\n  10 │ Proanth                 1.5909      0.41    1.555     3.58         0  Float64\n  11 │ Color.int               5.05809     1.28    4.69     13.0          0  Float64\n  12 │ Hue                     0.957449    0.48    0.965     1.71         0  Float64\n  13 │ OD                      2.61169     1.27    2.78      4.0          0  Float64\n  14 │ Proline               746.893     278     673.5    1680            0  Int64\n\n\n\n\n\n","category":"type"},{"location":"datasets/meshes/#Mesh-Datasets","page":"Meshes","title":"Mesh Datasets","text":"","category":"section"},{"location":"datasets/meshes/","page":"Meshes","title":"Meshes","text":"Mesh datasets contains data in the form of Meshes.Mesh. See Meshes.jl for a better understanding of Meshes.","category":"page"},{"location":"datasets/meshes/#Index","page":"Meshes","title":"Index","text":"","category":"section"},{"location":"datasets/meshes/","page":"Meshes","title":"Meshes","text":"Pages = [\"meshes.md\"]","category":"page"},{"location":"datasets/meshes/#Documentation","page":"Meshes","title":"Documentation","text":"","category":"section"},{"location":"datasets/meshes/","page":"Meshes","title":"Meshes","text":"FAUST","category":"page"},{"location":"datasets/meshes/#MLDatasets.FAUST","page":"Meshes","title":"MLDatasets.FAUST","text":"FAUST(split=:train; dir=nothing)\n\nThe MPI FAUST dataset (2014).\n\nFAUST contains 300 real, high-resolution human scans of 10 different subjects in 30 different poses, with automatically computed ground-truth correspondences.\n\nEach scan is a high-resolution, triangulated, non-watertight mesh acquired with a 3D multi-stereo system.\n\nFAUST is subdivided into a training and a test set. The training set includes 100 scans (10 per subject) with their corresponding ground-truth alignments. The test set includes 200 scans. The FAUST benchmark defines 100 preselected scan pairs, partitioned into two classes – 60 requiring intra-subject matching, 40 requiring inter-subject matching.\n\nThe dataset required to be downloaded manually from the website and extracted in the correct location. For information about where to place the dataset, refer to the example section.\n\nDataset Variables\n\nscans: Vector of non-watertight scans in the form of Mesh.\nregistrations: Vector of registrations corresponding to each scan in scans. registrations like scans are also in the form of Mesh.\nlabels: For each scan in the training set, we provide the boolean Vector of length equal to the number of vertices in the corresponding scan. It represents which vertices were reliably registered by the corresponding registration.\nmetadata: A dictionary containing additional information on the dataset. Currently only :test split has metadata containing information about the registrations required for the inter and intra challenge proposed by the author.\n\nExamples\n\nLoading the dataset\n\njulia> using MLDatasets\n\njulia> dataset = FAUST()\n[ Info: This program requested access to the data dependency MPI-FAUST\n[ Info: It could not be found on your system. It requires manual installation.\n┌ Info: Please install it to one of the directories in the DataDeps load path: /home/user/.julia/packages/DataDeps/EDWdQ/deps/data/MPI-FAUST,\n│ /home/user/.julia/datadeps/MPI-FAUST,\n│ /home/user/.julia/juliaup/julia-1.7.3+0.x86/local/share/julia/datadeps/MPI-FAUST,\n│ /home/user/.julia/juliaup/julia-1.7.3+0.x86/share/julia/datadeps/MPI-FAUST,\n│ /home/user/datadeps/MPI-FAUST,\n│ /scratch/datadeps/MPI-FAUST,\n│ /staging/datadeps/MPI-FAUST,\n│ /usr/share/datadeps/MPI-FAUST,\n└ or /usr/local/share/datadeps/MPI-FAUST\n[ Info: by following the instructions:\n┌ Info: Dataset: MPI-FAUST.\n└ Website: http://faust.is.tue.mpg.de/\nOnce installed please enter 'y' reattempt loading, or 'a' to abort\n[y/a]\n\nNow download and extract the dataset into one of the given locations. For unix link systems, an example command can be\n\nunzip -q <path-to-filename</filename.zip ~/.julia/datadeps\n\nThe corresponding folder tree should look like\n\n├── test\n│   ├── challenge_pairs\n│   └── scans\n└── training\n    ├── ground_truth_vertices\n    ├── registrations\n    └── scans\n\nPress y to re-attept loading.\n\ndataset FAUST:\n  scans          =>    100-element Vector{Any}\n  registrations  =>    100-element Vector{Any}\n  labels         =>    100-element Vector{Vector{Bool}}\n  metadata       =>    Dict{String, Any} with 0 entries\n\nLoad train and test split\n\njulia> train_faust = FAUST(:train)\ndataset FAUST:\n  scans          =>    100-element Vector{Any}\n  registrations  =>    100-element Vector{Any}\n  labels         =>    100-element Vector{Vector{Bool}}\n  metadata       =>    Dict{String, Any} with 0 entries\n\njulia> test_faust = FAUST(:test)\ndataset FAUST:\n  scans          =>    200-element Vector{Any}\n  registrations  =>    0-element Vector{Any}\n  labels         =>    0-element Vector{Vector{Bool}}\n  metadata       =>    Dict{String, Any} with 2 entries\n\nScan, registrations and ground-truth\n\njulia> dataset = FAUST(); # defaults to train split\n\njulia> scan = dataset.scans[1] # pick one scan\nMesh{3, Float32, Triangle}:\n Triangle(Float32[-0.0045452323, 0.08537669, 0.22134435], Float32[-0.0030340434, 0.08542955, 0.22206494],\nFloat32[-0.0042151767, 0.08697654, 0.22171047])\n Triangle(Float32[-0.05358432, 0.08490027, 0.17748278], Float32[-0.05379858, 0.083174236, 0.17670263],\nFloat32[-0.052645437, 0.08346437, 0.17816517])\n.\n.\n.\n Triangle(Float32[-0.07851, -1.0956081, 0.07093428], Float32[-0.06905176, -1.0986279, 0.07775441],\nFloat32[-0.069199145, -1.0928112, 0.06812464])\n\njulia> registration = dataset.registrations[1] # The corresponding registration\nMesh{3, Float32, Triangle}:\n Triangle(Float32[0.12491254, 0.51199615, 0.29041073], Float32[0.11376736, 0.5156298, 0.3007352],\nFloat32[0.119374536, 0.50043654, 0.29687837])\n Triangle(Float32[0.119374536, 0.50043654, 0.29687837], Float32[0.11376736, 0.5156298, 0.3007352],\nFloat32[0.10888693, 0.5008964, 0.30557302])\n.\n.\n.\n Triangle(Float32[0.033744745, 0.030968456, 0.2359996], Float32[0.058017172, 0.044458304, 0.23422624],\nFloat32[0.03615713, 0.04858183, 0.23596591])\n\njulia> label = dataset.labels[1] # The ground-truth/labels for each vertices in scan\n176387-element Vector{Bool}:\n 1\n 1\n 1\n .\n .\n .\n 0\n 0\n 0\n\nRefereneces\n\nMPI Faust Website\nBogo, Federica & Romero, Javier & Loper, Matthew & Black, Michael. (2014). FAUST: Dataset\n\nand evaluation for 3D mesh registration. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. 10.1109/CVPR.2014.491.\n\n\n\n\n\n","category":"type"},{"location":"containers/overview/#Dataset-Containers","page":"Dataset Containers","title":"Dataset Containers","text":"","category":"section"},{"location":"containers/overview/","page":"Dataset Containers","title":"Dataset Containers","text":"MLDatasets.jl contains several reusable data containers for accessing datasets in common storage formats. This feature is a work-in-progress and subject to change.","category":"page"},{"location":"containers/overview/","page":"Dataset Containers","title":"Dataset Containers","text":"FileDataset\nCachedDataset\nMLDatasets.make_cache","category":"page"},{"location":"containers/overview/#MLDatasets.FileDataset","page":"Dataset Containers","title":"MLDatasets.FileDataset","text":"FileDataset([loadfn = FileIO.load,] paths)\nFileDataset([loadfn = FileIO.load,] dir, pattern = \"*\", depth = 4)\n\nWrap a set of file paths as a dataset (traversed in the same order as paths). Alternatively, specify a dir and collect all paths that match a glob pattern (recursively globbing by depth). The glob order determines the traversal order.\n\n\n\n\n\n","category":"type"},{"location":"containers/overview/#MLDatasets.CachedDataset","page":"Dataset Containers","title":"MLDatasets.CachedDataset","text":"CachedDataset(source, cachesize = numbobs(source))\nCachedDataset(source, cacheidx = 1:numbobs(source))\nCachedDataset(source, cacheidx, cache)\n\nWrap a source data container and cache cachesize samples in memory. This can be useful for improving read speeds when source is a lazy data container, but your system memory is large enough to store a sizeable chunk of it.\n\nBy default the observation indices 1:cachesize are cached. You can manually pass in a set of cacheidx as well.\n\nSee also make_cache for customizing the default cache creation for source.\n\n\n\n\n\n","category":"type"},{"location":"containers/overview/#MLDatasets.make_cache","page":"Dataset Containers","title":"MLDatasets.make_cache","text":"make_cache(source, cacheidx)\n\nReturn a in-memory copy of source at observation indices cacheidx. Defaults to getobs(source, cacheidx).\n\n\n\n\n\n","category":"function"},{"location":"LICENSE/#LICENSE","page":"LICENSE","title":"LICENSE","text":"","category":"section"},{"location":"LICENSE/","page":"LICENSE","title":"LICENSE","text":"using Markdown\nMarkdown.parse_file(joinpath(@__DIR__, \"..\", \"..\", \"LICENSE\"))","category":"page"},{"location":"datasets/graphs/#Graph-Datasets","page":"Graphs","title":"Graph Datasets","text":"","category":"section"},{"location":"datasets/graphs/","page":"Graphs","title":"Graphs","text":"A collection of datasets with an underlying graph structure. Some of these datasets contain a single graph, that can be accessed with dataset[:] or dataset[1]. Others contain many graphs,  accessed through dataset[i]. Graphs are represented by the MLDatasets.Graph  and MLDatasets.HeteroGraph type.","category":"page"},{"location":"datasets/graphs/#Index","page":"Graphs","title":"Index","text":"","category":"section"},{"location":"datasets/graphs/","page":"Graphs","title":"Graphs","text":"Pages = [\"graphs.md\"]","category":"page"},{"location":"datasets/graphs/#Documentation","page":"Graphs","title":"Documentation","text":"","category":"section"},{"location":"datasets/graphs/","page":"Graphs","title":"Graphs","text":"MLDatasets.Graph\nMLDatasets.HeteroGraph","category":"page"},{"location":"datasets/graphs/#MLDatasets.Graph","page":"Graphs","title":"MLDatasets.Graph","text":"Graph(; kws...)\n\nA type that represents a graph and that can also store node and edge data. It doesn't distinguish between directed or undirected graph, therefore for undirected graphs will store edges in both directions. Nodes are indexed in 1:num_nodes.\n\nGraph datasets in MLDatasets.jl contain one or more Graph or HeteroGraph objects.\n\nKeyword Arguments\n\nnum_nodes: the number of nodes. If omitted, is inferred from edge_index.\nedge_index: a tuple containing two vectors with length equal to the number of edges.   The first vector contains the list of the source nodes of each edge, the second the target nodes.   Defaults to (Int[], Int[]).\nnode_data: node-related data. Can be nothing, a named tuple of arrays or a dictionary of arrays.           The arrays last dimension size should be equal to the number of nodes.           Default nothing.\nedge_data: edge-related data. Can be nothing, a named tuple of arrays or a dictionary of arrays.            The arrays' last dimension size should be equal to the number of edges.            Default nothing.\n\nExamples\n\nAll graph datasets in MLDatasets.jl contain Graph or HeteroGraph objects:\n\njulia> using MLDatasets: Cora\n\njulia> d = Cora() # the Cora dataset\ndataset Cora:\n  metadata    =>    Dict{String, Any} with 3 entries\n  graphs      =>    1-element Vector{Graph}\n\njulia> d[1]\nGraph:\n  num_nodes   =>    2708\n  num_edges   =>    10556\n  edge_index  =>    (\"10556-element Vector{Int64}\", \"10556-element Vector{Int64}\")\n  node_data   =>    (features = \"1433×2708 Matrix{Float32}\", targets = \"2708-element Vector{Int64}\", train_mask = \"2708-element BitVector with 140 trues\", val_mask = \"2708-element BitVector with 500 trues\", test_mask = \"2708-element BitVector with 1000 trues\")\n  edge_data   =>    nothing\n\nLet's se how to convert a Graphs.jl's graph to a MLDatasets.Graph and viceversa:\n\nimport Graphs, MLDatasets\n\n## From Graphs.jl to MLDatasets.Graphs\n\n# From a directed graph\ng = Graphs.erdos_renyi(10, 20, is_directed=true)\ns = [e.src for e in Graphs.edges(g)]\nt = [e.dst for e in Graphs.edges(g)]\nmlg = MLDatasets.Graph(num_nodes=10, edge_index=(s, t))\n\n# From an undirected graph\ng = Graphs.erdos_renyi(10, 20, is_directed=false)\ns = [e.src for e in Graphs.edges(g)]\nt = [e.dst for e in Graphs.edges(g)]\ns, t = [s; t], [t; s] # adding reverse edges\nmlg = MLDatasets.Graph(num_nodes=10, edge_index=(s, t))\n\n# From MLDatasets.Graphs to Graphs.jl\ns, t = mlg.edge_index\ng = Graphs.DiGraph(mlg.num_nodes)\nfor (i, j) in zip(s, t)\n    Graphs.add_edge!(g, i, j)\nend\n\n\n\n\n\n","category":"type"},{"location":"datasets/graphs/#MLDatasets.HeteroGraph","page":"Graphs","title":"MLDatasets.HeteroGraph","text":"HeteroGraph(; kws...)\n\nHeteroGraph is used for HeteroGeneous Graphs.\n\nHeteroGraph unlike Graph can have different types of nodes. Each node pertains to different types of information. \n\nEdges in HeteroGraph is defined by relations. A relation is a tuple of  (src_node_type, edge_type, target_node_type) where edge_type represents the relation between the src and target nodes. Edges between same node types are possible. \n\nA HeteroGraph can be directed or undirected. It doesn't distinguish between directed  or undirected graphs. Therefore, for undirected graphs, it will store edges in both directions. Nodes are indexed in 1:num_nodes.\n\nKeyword Arguments\n\nnum_nodes: Dictionary containing the number of nodes for each node type. If omitted, is inferred from edge_index.\nnum_edges: Dictionary containing the number of edges for each relation.\nedge_indices: Dictionary containing the edge_index for each edge relation. An edge_index is a tuple containing two vectors with length equal to the number of edges for the relation.   The first vector contains the list of the source nodes of each edge, the second contains the target nodes.\nnode_data: node-related data. Can be nothing, Dictionary of a dictionary of arrays. Data of a speific type of node can be accessed            using nodedata[nodetype].The array's last dimension size should be equal to the number of nodes.           Default nothing.\nedge_data: Can be nothing, Dictionary of a dictionary of arrays. Data of a speific type of edge can be accessed            using edgedata[edgetype].The array's last dimension size should be equal to the number of nodes.           Default nothing.\n\n\n\n\n\n","category":"type"},{"location":"datasets/graphs/","page":"Graphs","title":"Graphs","text":"CiteSeer\nCora\nKarateClub\nMovieLens\nOGBDataset\nOrganicMaterialsDB\nPolBlogs\nPubMed\nReddit\nTUDataset\nMETRLA","category":"page"},{"location":"datasets/graphs/#MLDatasets.CiteSeer","page":"Graphs","title":"MLDatasets.CiteSeer","text":"CiteSeer(; dir=nothing)\n\nThe CiteSeer citation network dataset from Ref. [1]. Nodes represent documents and edges represent citation links. The dataset is designed for the node classification task.  The task is to predict the category of certain paper. The dataset is retrieved from Ref. [2].\n\nReferences\n\n[1]: Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking \n\n[2]: Planetoid\n\n\n\n\n\n","category":"type"},{"location":"datasets/graphs/#MLDatasets.Cora","page":"Graphs","title":"MLDatasets.Cora","text":"Cora()\n\nThe Cora citation network dataset from Ref. [1]. Nodes represent documents and edges represent citation links. Each node has a predefined feature with 1433 dimensions.  The dataset is designed for the node classification task.  The task is to predict the category of certain paper. The dataset is retrieved from Ref. [2].\n\nStatistics\n\nNodes: 2708\nEdges: 10556\nNumber of Classes: 7\nLabel split:\nTrain:  140\nVal:    500\nTest:  1000\n\nThe split is the one used in the original paper [1] and  doesn't consider all nodes.\n\nReferences\n\n[1]: Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking\n\n[2]: Planetoid\n\n\n\n\n\n","category":"type"},{"location":"datasets/graphs/#MLDatasets.KarateClub","page":"Graphs","title":"MLDatasets.KarateClub","text":"KarateClub()\n\nThe Zachary's karate club dataset originally appeared in Ref [1].\n\nThe network contains 34 nodes (members of the karate club). The nodes are connected by 78 undirected and unweighted edges. The edges indicate if the two members interacted outside the club.\n\nThe node labels indicate which community or the karate club the member belongs to. The club based labels are as per the original dataset in Ref [1]. The community labels are obtained by modularity-based clustering following Ref [2]. The data is retrieved from Ref [3] and [4]. One node per unique label is used as training data.\n\nReferences\n\n[1]: An Information Flow Model for Conflict and Fission in Small Groups\n\n[2]: Semi-supervised Classification with Graph Convolutional Networks\n\n[3]: PyTorch Geometric Karate Club Dataset\n\n[4]: NetworkX Zachary's Karate Club Dataset\n\n\n\n\n\n","category":"type"},{"location":"datasets/graphs/#MLDatasets.MovieLens","page":"Graphs","title":"MLDatasets.MovieLens","text":"MovieLens(name; dir=nothing)\n\nDatasets from the MovieLens website collected and maintained by GroupLens.  The MovieLens datasets are presented in a Graph format.  For license and usage restrictions please refer to the Readme.md of the datasets.\n\nThere are 6 versions of MovieLens datasets currently supported: \"100k\",  \"1m\",  \"10m\", \"20m\", \"25m\", \"latest-small\".  The 100k and 1k datasets contain movie data and rating data along with demographic data. Starting from the 10m dataset, MovieLens datasets no longer contain the demographic data.  These datasets contain movie data, rating data, and tag information. \n\nThe 20m and 25m datasets additionally contain genome tag scores.  Each movie in these datasets contains tag relevance scores for every tag.\n\nEach dataset contains an heterogeneous graph, with two kinds of nodes,  movie and user. The rating is represented by an edge between them: (user, rating, movie).  20m, 25m, and latest-small datasets also contain tag nodes and edges of type (user, tag, movie) and  optionally (movie, score, tag).\n\nExamples\n\nMovieLens 100K dataset\n\njulia> data = MovieLens(\"100k\")\nMovieLens 100k:\n  metadata    =>    Dict{String, Any} with 2 entries\n  graphs      =>    1-element Vector{MLDatasets.HeteroGraph}\n\njulia> metadata = data.metadata\nDict{String, Any} with 2 entries:\n  \"genre_labels\"      => [\"Unknown\", \"Action\", \"Adventure\", \"Animation\", \"Children\", \"Comedy\", \"Crime\", \"Documentary\", \"Drama\", \"Fa…\n  \"movie_id_to_title\" => Dict(1144=>\"Quiet Room, The (1996)\", 1175=>\"Hugo Pool (1997)\", 719=>\"Canadian Bacon (1994)\", 1546=>\"Shadow…\n\njulia> g = data[:]\n  Heterogeneous Graph:\n    node_types    =>    2-element Vector{String}\n    edge_types    =>    1-element Vector{Tuple{String, String, String}}\n    num_nodes     =>    Dict{String, Int64} with 2 entries\n    num_edges     =>    Dict{Tuple{String, String, String}, Int64} with 1 entry\n    edge_indices  =>    Dict{Tuple{String, String, String}, Tuple{Vector{Int64}, Vector{Int64}}} with 1 entry\n    node_data     =>    Dict{String, Dict} with 2 entries\n    edge_data     =>    Dict{Tuple{String, String, String}, Dict} with 1 entry\n\n# Access the user information\njulia> user_data = g.node_data[\"user\"]\nDict{Symbol, AbstractVector} with 4 entries:\n  :age        => [24, 53, 23, 24, 33, 42, 57, 36, 29, 53  …  61, 42, 24, 48, 38, 26, 32, 20, 48, 22]\n  :occupation => [\"technician\", \"other\", \"writer\", \"technician\", \"other\", \"executive\", \"administrator\", \"administrator\", \"student\",…\n  :zipcode    => [\"85711\", \"94043\", \"32067\", \"43537\", \"15213\", \"98101\", \"91344\", \"05201\", \"01002\", \"90703\"  …  \"22902\", \"66221\", \"3…\n  :gender     => Bool[1, 0, 1, 1, 0, 1, 1, 1, 1, 1  …  1, 1, 1, 1, 0, 0, 1, 1, 0, 1]\n\n# Access rating information\njulia> g.edge_data[(\"user\", \"rating\", \"movie\")]\nDict{Symbol, Vector} with 2 entries:\n  :timestamp => [881250949, 891717742, 878887116, 880606923, 886397596, 884182806, 881171488, 891628467, 886324817, 883603013  …  8…\n  :rating    => Float16[3.0, 3.0, 1.0, 2.0, 1.0, 4.0, 2.0, 5.0, 3.0, 3.0  …  4.0, 4.0, 3.0, 2.0, 3.0, 3.0, 5.0, 1.0, 2.0, 3.0]\n\nMovieLens 20m dataset\n\njulia> data = MovieLens(\"20m\")\nMovieLens 20m:\n  metadata    =>    Dict{String, Any} with 4 entries\n  graphs      =>    1-element Vector{MLDatasets.HeteroGraph}\n\n# There is only 1 graph in MovieLens dataset\njulia> g = data[1]\nHeterogeneous Graph:\n  node_types    =>    3-element Vector{String}\n  edge_types    =>    3-element Vector{Tuple{String, String, String}}\n  num_nodes     =>    Dict{String, Int64} with 3 entries\n  num_edges     =>    Dict{Tuple{String, String, String}, Int64} with 3 entries\n  edge_indices  =>    Dict{Tuple{String, String, String}, Tuple{Vector{Int64}, Vector{Int64}}} with 3 entries\n  node_data     =>    Dict{String, Dict} with 0 entries\n  edge_data     =>    Dict{Tuple{String, String, String}, Dict} with 3 entries\n\n# Apart from user rating a movie, a user assigns tag to movies and there are genome-scores for movie-tag pairs \njulia> g.edge_indices\n  Dict{Tuple{String, String, String}, Tuple{Vector{Int64}, Vector{Int64}}} with 3 entries:\n    (\"movie\", \"score\", \"tag\")   => ([1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  131170, 131170, 131170, 131170, 131170, 131170, 131170, 131170,…\n    (\"user\", \"tag\", \"movie\")    => ([18, 65, 65, 65, 65, 65, 65, 65, 65, 65  …  3489, 7045, 7045, 7164, 7164, 55999, 55999, 55999, 55…\n    (\"user\", \"rating\", \"movie\") => ([1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  60816, 61160, 65682, 66762, 68319, 68954, 69526, 69644, 70286, …\n\n# Access the rating\njulia> g.edge_data[(\"user\", \"rating\", \"movie\")]\nDict{Symbol, Vector} with 2 entries:\n  :timestamp => [1112486027, 1112484676, 1112484819, 1112484727, 1112484580, 1094785740, 1094785734, 1112485573, 1112484940, 111248…\n  :rating    => Float16[3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 4.0, 4.0, 4.0, 4.0  …  4.5, 4.0, 4.5, 4.5, 4.5, 4.5, 4.5, 3.0, 5.0, 2.5]\n\n# Access the movie-tag scores\nscore = g.edge_data[(\"movie\", \"score\", \"tag\")][:score]\n23419536-element Vector{Float64}:\n 0.025000000000000022\n 0.025000000000000022\n 0.057750000000000024\n ⋮\n\nReferences\n\n[1] GroupLens Website\n\n[2] TensorFlow MovieLens Implementation   \n\n[3] Jesse Vig, Shilad Sen, and John Riedl. 2012. The Tag Genome: Encoding Community Knowledge to Support Novel Interaction. ACM Trans. Interact. Intell. Syst. 2, 3, Article 13 (September 2012), 44 pages. https://doi.org/10.1145/2362394.2362395.   \n\n[4] F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Trans. Interact. Intell. Syst. 5, 4, Article 19 (January 2016), 19 pages. https://doi.org/10.1145/2827872  \n\n\n\n\n\n","category":"type"},{"location":"datasets/graphs/#MLDatasets.OGBDataset","page":"Graphs","title":"MLDatasets.OGBDataset","text":"OGBDataset(name; dir=nothing)\n\nThe collection of datasets from the Open Graph Benchmark: Datasets for Machine Learning on Graphs paper.\n\nname is the name  of one of the datasets (listed here) available for node prediction, edge prediction, or graph prediction tasks.\n\nExamples\n\nNode prediction tasks\n\njulia> data = OGBDataset(\"ogbn-arxiv\")\nOGBDataset ogbn-arxiv:\n  metadata    =>    Dict{String, Any} with 17 entries\n  graphs      =>    1-element Vector{MLDatasets.Graph}\n  graph_data  =>    nothing\n\njulia> data[:]\nGraph:\n  num_nodes   =>    169343\n  num_edges   =>    1166243\n  edge_index  =>    (\"1166243-element Vector{Int64}\", \"1166243-element Vector{Int64}\")\n  node_data   =>    (val_mask = \"29799-trues BitVector\", test_mask = \"48603-trues BitVector\", year = \"169343-element Vector{Int64}\", features = \"128×169343 Matrix{Float32}\", label = \"169343-element Vector{Int64}\", train_mask = \"90941-trues BitVector\")\n  edge_data   =>    nothing\n\njulia> data.metadata\nDict{String, Any} with 17 entries:\n  \"download_name\"         => \"arxiv\"\n  \"num classes\"           => 40\n  \"num tasks\"             => 1\n  \"binary\"                => false\n  \"url\"                   => \"http://snap.stanford.edu/ogb/data/nodeproppred/arxiv.zip\"\n  \"additional node files\" => [\"node_year\"]\n  \"is hetero\"             => false\n  \"task level\"            => \"node\"\n  ⋮                       => ⋮\n\njulia> data = OGBDataset(\"ogbn-mag\") OGBDataset ogbn-mag:   metadata    =>    Dict{String, Any} with 17 entries   graphs      =>    1-element Vector{MLDatasets.HeteroGraph}   graph_data  =>    nothing\n\njulia> data[:] Heterogeneous Graph:   numnodes     =>    Dict{String, Int64} with 4 entries   numedges     =>    Dict{Tuple{String, String, String}, Int64} with 4 entries   edgeindices  =>    Dict{Tuple{String, String, String}, Tuple{Vector{Int64}, Vector{Int64}}} with 4 entries   nodedata     =>    (year = \"Dict{String, Vector{Float32}} with 1 entry\", features = \"Dict{String, Matrix{Float32}} with 1 entry\", label = \"Dict{String, Vector{Int64}} with 1 entry\")   edge_data     =>    (reltype = \"Dict{Tuple{String, String, String}, Vector{Float32}} with 4 entries\",)\n\nEdge prediction task\n\njulia> data = OGBDataset(\"ogbl-collab\")\nOGBDataset ogbl-collab:\n  metadata    =>    Dict{String, Any} with 15 entries\n  graphs      =>    1-element Vector{MLDatasets.Graph}\n  graph_data  =>    nothing\n\njulia> data[:]\nGraph:\n  num_nodes   =>    235868\n  num_edges   =>    2358104\n  edge_index  =>    (\"2358104-element Vector{Int64}\", \"2358104-element Vector{Int64}\")\n  node_data   =>    (features = \"128×235868 Matrix{Float32}\",)\n  edge_data   =>    (year = \"2×1179052 Matrix{Int64}\", weight = \"2×1179052 Matrix{Int64}\")\n\nGraph prediction task\n\njulia> data = OGBDataset(\"ogbg-molhiv\")\nOGBDataset ogbg-molhiv:\n  metadata    =>    Dict{String, Any} with 17 entries\n  graphs      =>    41127-element Vector{MLDatasets.Graph}\n  graph_data  =>    (labels = \"41127-element Vector{Int64}\", train_mask = \"32901-trues BitVector\", val_mask = \"4113-trues BitVector\", test_mask = \"4113-trues BitVector\")\n\njulia> data[1]\n(graphs = Graph(19, 40), labels = 0)\n\n\n\n\n\n","category":"type"},{"location":"datasets/graphs/#MLDatasets.OrganicMaterialsDB","page":"Graphs","title":"MLDatasets.OrganicMaterialsDB","text":"OrganicMaterialsDB(; split=:train, dir=nothing)\n\nThe OMDB-GAP1 v1.1 dataset from the Organic Materials Database (OMDB) of bulk organic crystals.\n\nThe dataset has to be manually downloaded from https://omdb.mathub.io/dataset,  then unzipped and  its file content placed in the OrganicMaterialsDB folder.\n\nThe dataset contains the following files:\n\nFilename Description\nstructures.xyz 12500 crystal structures. Use the first 10000 as training examples and the remaining 2500 as test set.\nbandgaps.csv 12500 DFT band gaps corresponding to structures.xyz\nCODids.csv 12500 COD ids cross referencing the Crystallographic Open Database (in the same order as structures.xyz)\n\nPlease cite the paper introducing this dataset: https://arxiv.org/abs/1810.12814\n\n\n\n\n\n","category":"type"},{"location":"datasets/graphs/#MLDatasets.PolBlogs","page":"Graphs","title":"MLDatasets.PolBlogs","text":"PolBlogs(; dir=nothing)\n\nThe Political Blogs dataset from the The Political Blogosphere and the 2004 US Election: Divided they Blog paper.\n\nPolBlogs is a graph with 1,490 vertices (representing political blogs) and 19,025 edges (links between blogs).\n\nThe links are automatically extracted from a crawl of the front page of the blog. \n\nEach vertex receives a label indicating the political leaning of the blog: liberal or conservative.\n\n\n\n\n\n","category":"type"},{"location":"datasets/graphs/#MLDatasets.PubMed","page":"Graphs","title":"MLDatasets.PubMed","text":"PubMed(; dir=nothing, reverse_edges=true)\n\nThe PubMed citation network dataset from Ref. [1]. Nodes represent documents and edges represent citation links. The dataset is designed for the node classification task.  The task is to predict the category of certain paper. The dataset is retrieved from Ref. [2].\n\nReferences\n\n[1]: Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking\n\n[2]: Planetoid\n\n\n\n\n\n","category":"type"},{"location":"datasets/graphs/#MLDatasets.Reddit","page":"Graphs","title":"MLDatasets.Reddit","text":"Reddit(; full=true, dir=nothing)\n\nThe Reddit dataset was introduced in Ref [1]. It is a graph dataset of Reddit posts made in the month of September, 2014. The dataset contains a single post-to-post graph, connecting posts if the same user comments on both.  The node label in this case is one of the 41 communities, or “subreddit”s, that a post belongs to.   This dataset contains 232,965 posts. The first 20 days are used for training and the remaining days for testing (with 30% used for validation). Each node is represented by a 602 word vector.\n\nUse full=false to load only a subsample of the dataset.\n\nReferences\n\n[1]: Inductive Representation Learning on Large Graphs\n\n[2]: Benchmarks on the Reddit Dataset\n\n\n\n\n\n","category":"type"},{"location":"datasets/graphs/#MLDatasets.TUDataset","page":"Graphs","title":"MLDatasets.TUDataset","text":"TUDataset(name; dir=nothing)\n\nA variety of graph benchmark datasets, .e.g. \"QM9\", \"IMDB-BINARY\", \"REDDIT-BINARY\" or \"PROTEINS\", collected from the TU Dortmund University. Retrieve from the TUDataset collection the dataset name, where name is any of the datasets available here. \n\nA TUDataset object can be indexed to retrieve a specific graph or a subset of graphs.\n\nSee here for an in-depth  description of the format. \n\nUsage Example\n\njulia> data = TUDataset(\"PROTEINS\")\ndataset TUDataset:\n  name        =>    PROTEINS\n  metadata    =>    Dict{String, Any} with 1 entry\n  graphs      =>    1113-element Vector{MLDatasets.Graph}\n  graph_data  =>    (targets = \"1113-element Vector{Int64}\",)\n  num_nodes   =>    43471\n  num_edges   =>    162088\n  num_graphs  =>    1113\n\njulia> data[1]\n(graphs = Graph(42, 162), targets = 1)\n\n\n\n\n\n","category":"type"},{"location":"datasets/graphs/#MLDatasets.METRLA","page":"Graphs","title":"MLDatasets.METRLA","text":"METRLA(; num_timesteps_in::Int = 12, num_timesteps_out::Int=12, dir=nothing)\n\nThe METR-LA dataset from the Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting paper.\n\nMETRLA is a graph with 207 nodes representing traffic sensors in Los Angeles. \n\nThe edge weights w are contained as a feature array in edge_data and represent the distance between the sensors. \n\nThe node features are the traffic speed and the time of the measurements collected by the sensors, divided into num_timesteps_in time steps. \n\nThe target values are the traffic speed and the time of the measurements collected by the sensors, divided into num_timesteps_out time steps.\n\n\n\n\n\n","category":"type"},{"location":"datasets/text/#Text-Datasets","page":"Text","title":"Text Datasets","text":"","category":"section"},{"location":"datasets/text/#Index","page":"Text","title":"Index","text":"","category":"section"},{"location":"datasets/text/","page":"Text","title":"Text","text":"Pages = [\"text.md\"]","category":"page"},{"location":"datasets/text/#Documentation","page":"Text","title":"Documentation","text":"","category":"section"},{"location":"datasets/text/","page":"Text","title":"Text","text":"PTBLM\nSMSSpamCollection\nUD_English","category":"page"},{"location":"datasets/text/#MLDatasets.PTBLM","page":"Text","title":"MLDatasets.PTBLM","text":"PTBLM(; split=:train, dir=nothing)\nPTBLM(split; [dir])\n\nThe PTBLM dataset consists of Penn Treebank sentences for language modeling, available from https://github.com/tomsercu/lstm. The unknown words are replaced with <unk> so that the total vocaburary size becomes 10000.\n\n\n\n\n\n","category":"type"},{"location":"datasets/text/#MLDatasets.SMSSpamCollection","page":"Text","title":"MLDatasets.SMSSpamCollection","text":"SMSSpamCollection(; dir=nothing)\n\nThe SMS Spam Collection v.1 (hereafter the corpus) is a set of SMS tagged messages  that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged acording being ham (legitimate) or spam. The corpus has a total of 4,827 SMS legitimate messages (86.6%) and a total of 747 (13.4%) spam messages.\n\nThe corpus has been collected by Tiago Agostinho de Almeida (http://www.dt.fee.unicamp.br/~tiago)  and José María Gómez Hidalgo (http://www.esp.uem.es/jmgomez).\n\n```julia-repl julia> using MLDatasets: SMSSpamCollection\n\njulia> targets = SMSSpamCollection.targets();\n\njulia> summary(targets) \"5574-element Vector{Any}\"\n\njulia> targets[1] \"ham\"\n\njulia> summary(features) \"5574-element Vector{Any}\"\n\n\n\n\n\n","category":"type"},{"location":"datasets/text/#MLDatasets.UD_English","page":"Text","title":"MLDatasets.UD_English","text":"UD_English(; split=:train, dir=nothing)\nUD_English(split=; [dir])\n\nA Gold Standard Universal Dependencies Corpus for English, built over the source material of the English Web Treebank LDC2012T13 (https://catalog.ldc.upenn.edu/LDC2012T13).\n\nThe corpus comprises 254,825 words and 16,621 sentences,  taken from five genres of web media: weblogs, newsgroups, emails, reviews, and Yahoo! answers.  See the LDC2012T13 documentation for more details on the sources of the sentences.  The trees were automatically converted into Stanford Dependencies and then hand-corrected to Universal Dependencies.  All the basic dependency annotations have been single-annotated, a limited portion of them have been double-annotated,  and subsequent correction has been done to improve consistency. Other aspects of the treebank, such as Universal POS,  features and enhanced dependencies, has mainly been done automatically, with very limited hand-correction.\n\nAuthors: Natalia Silveira and Timothy Dozat and             Marie-Catherine de Marneffe and Samuel             Bowman and Miriam Connor and John Bauer and             Christopher D. Manning Website: https://github.com/UniversalDependencies/UD_English-EWT\n\n\n\n\n\n","category":"type"},{"location":"#MLDatasets.jl's-Documentation","page":"Home","title":"MLDatasets.jl's Documentation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package represents a community effort to provide a common interface for accessing common Machine Learning (ML) datasets. In contrast to other data-related Julia packages, the focus of MLDatasets.jl is specifically on downloading, unpacking, and accessing benchmark dataset. Functionality for the purpose of data processing or visualization is only provided to a degree that is special to some dataset.","category":"page"},{"location":"","page":"Home","title":"Home","text":"This package is a part of the JuliaML ecosystem.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To install MLDatasets.jl, start up Julia and type the following code snippet into the REPL. It makes use of the native Julia package manger.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Pkg.add(\"MLDatasets\")","category":"page"},{"location":"#Available-Datasets","page":"Home","title":"Available Datasets","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Datasets are grouped into different categories. Click on the links below for a full list of datasets available in each category.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Graph Datasets - datasets with an underlying graph structure: Cora, PubMed, CiteSeer, ...\nMiscellaneuous Datasets - datasets that do not fall into any of the other categories: Iris, BostonHousing, ...\nText Datasets - datasets for language models. \nVision Datasets - vision related datasets such as MNIST, CIFAR10, CIFAR100, ... ","category":"page"},{"location":"#Basic-Usage","page":"Home","title":"Basic Usage","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The way MLDatasets.jl is organized is that each dataset is its own type.  Where possible, those types share a common interface (fields and methods). ","category":"page"},{"location":"","page":"Home","title":"Home","text":"Once a dataset has been instantiated, e.g. by dataset = MNIST(),   an observation i can be retrieved using the indexing syntax dataset[i]. By indexing with no arguments, dataset[:], the whole set of observations is collected. The total number of observations is given by length(dataset).","category":"page"},{"location":"","page":"Home","title":"Home","text":"For example you can load the training set of the MNIST database of handwritten digits using the following commands:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> using MLDatasets\n\njulia> trainset = MNIST(:train)\ndataset MNIST:\n  metadata    =>    Dict{String, Any} with 3 entries\n  split       =>    :train\n  features    =>    28×28×60000 Array{Float32, 3}\n  targets     =>    60000-element Vector{Int64}\n\njulia> length(d)\n60000\n\njulia> trainset[1]  # return first observation as a NamedTuple\n(features = Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], \n targets = 5)\n\njulia> X_train, y_train = trainset[:] # return all observations\n(features = [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; … ;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], \n targets = [5, 0, 4, 1, 9, 2, 1, 3, 1, 4  …  9, 2, 9, 5, 1, 8, 3, 5, 6, 8])\n\njulia> summary(X_train)\n\"28×28×60000 Array{Float32, 3}\"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Input features are commonly denoted by features, while classification labels and regression targets are denoted by targets.","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> using MLDatasets, DataFrames\n\njulia> iris = Iris()\ndataset Iris:\n  metadata    =>    Dict{String, Any} with 4 entries\n  features    =>    150×4 DataFrame\n  targets     =>    150×1 DataFrame\n  dataframe   =>    150×5 DataFrame\n\njulia> iris.features\n150×4 DataFrame\n Row │ sepallength  sepalwidth  petallength  petalwidth \n     │ Float64      Float64     Float64      Float64    \n─────┼──────────────────────────────────────────────────\n   1 │         5.1         3.5          1.4         0.2\n   2 │         4.9         3.0          1.4         0.2\n   3 │         4.7         3.2          1.3         0.2\n   4 │         4.6         3.1          1.5         0.2\n   5 │         5.0         3.6          1.4         0.2\n   6 │         5.4         3.9          1.7         0.4\n   7 │         4.6         3.4          1.4         0.3\n   8 │         5.0         3.4          1.5         0.2\n   9 │         4.4         2.9          1.4         0.2\n  ⋮  │      ⋮           ⋮            ⋮           ⋮\n 142 │         6.9         3.1          5.1         2.3\n 143 │         5.8         2.7          5.1         1.9\n 144 │         6.8         3.2          5.9         2.3\n 145 │         6.7         3.3          5.7         2.5\n 146 │         6.7         3.0          5.2         2.3\n 147 │         6.3         2.5          5.0         1.9\n 148 │         6.5         3.0          5.2         2.0\n 149 │         6.2         3.4          5.4         2.3\n 150 │         5.9         3.0          5.1         1.8\n                                        132 rows omitted\n\njulia> iris.targets\n150×1 DataFrame\n Row │ class          \n     │ String15       \n─────┼────────────────\n   1 │ Iris-setosa\n   2 │ Iris-setosa\n   3 │ Iris-setosa\n   4 │ Iris-setosa\n   5 │ Iris-setosa\n   6 │ Iris-setosa\n   7 │ Iris-setosa\n   8 │ Iris-setosa\n   9 │ Iris-setosa\n  ⋮  │       ⋮\n 142 │ Iris-virginica\n 143 │ Iris-virginica\n 144 │ Iris-virginica\n 145 │ Iris-virginica\n 146 │ Iris-virginica\n 147 │ Iris-virginica\n 148 │ Iris-virginica\n 149 │ Iris-virginica\n 150 │ Iris-virginica\n      132 rows omitted","category":"page"},{"location":"#MLUtils-compatibility","page":"Home","title":"MLUtils compatibility","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"MLDatasets.jl garuantees compatibility with the getobs and numobs interface defined in MLUtils.jl. In practice, applying getobs and numobs on datasets is equivalent to applying indexing and length.","category":"page"},{"location":"#Conditional-module-loading","page":"Home","title":"Conditional module loading","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"MLDatasets.jl relies on many different packages in order to load and process the diverse type of datasets it supports. Most likely, any single user of the library will use a limited subset of these functionalities. In order to reduce the time taken by using MLDatasets in users' code, we use a lazy import system that defers the import of packages inside MLDatasets.jl as much as possible.   For some of the packages, some manual intervention is needed from the user.  As an example, the following code will produce an error:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> using MLDataset\n\njulia> MNIST(); # fine, MNIST doesn't require DataFrames\n\njulia> Iris() # ERROR: Add `import DataFrames` or `using DataFrames` to your code to unlock this functionality.","category":"page"},{"location":"","page":"Home","title":"Home","text":"We can easily fix the error with an additional import as recommended by the error message:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> using MLDataset, DataFrames\n\njulia> Iris()\ndataset Iris:\n  metadata    =>    Dict{String, Any} with 4 entries\n  features    =>    150×4 DataFrame\n  targets     =>    150×1 DataFrame\n  dataframe   =>    150×5 DataFrame","category":"page"},{"location":"#Download-location","page":"Home","title":"Download location","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"MLDatasets.jl is build on top of the package DataDeps.jl. To load the data the package looks for the necessary files in various locations (see DataDeps.jl for more information on how to configure such defaults). If the data can't be found in any of those locations, then the package will trigger a download dialog to ~/.julia/datadeps/<DATASETNAME>. To overwrite this on a case by case basis, it is possible to specify a data directory directly in the dataset constructor (e.g. MNIST(dir = <directory>)).","category":"page"},{"location":"","page":"Home","title":"Home","text":"In order to download datasets without having to manually confirm the download,  you can set to true the following enviromental variable:","category":"page"},{"location":"","page":"Home","title":"Home","text":"ENV[\"DATADEPS_ALWAYS_ACCEPT\"] = true","category":"page"}]
}

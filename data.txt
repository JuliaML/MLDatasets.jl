[.\docs\make.jl]
using Documenter, MLDatasets

## Commented out since gives warning
# DocMeta.setdocmeta!(MLDatasets, :DocTestSetup, :(using MLDatasets); recursive=true)

# Build documentation.
# ====================

makedocs(modules = [MLDatasets],
         doctest = true,
         clean = false,
         sitename = "MLDatasets.jl",
         format = Documenter.HTML(canonical = "https://juliadata.github.io/MLDatasets.jl/stable/",
                                  assets = ["assets/favicon.ico"],
                                  prettyurls = get(ENV, "CI", nothing) == "true",
                                  collapselevel = 3),
         authors = "Hiroyuki Shindo, Christof Stocker, Carlo Lucibello",
         pages = Any["Home" => "index.md",
                     "Datasets" => Any["Graphs" => "datasets/graphs.md",
                                       "Meshes" => "datasets/meshes.md",
                                       "Miscellaneous" => "datasets/misc.md",
                                       "Text" => "datasets/text.md",
                                       "Vision" => "datasets/vision.md"],
                     "Creating Datasets" => Any["containers/overview.md"], # still experimental
                     "LICENSE.md"],
         checkdocs = :exports)

deploydocs(repo = "github.com/JuliaML/MLDatasets.jl.git")

[.\src\abstract_datasets.jl]
"""
    AbstractDataset

Super-type from which all datasets in MLDatasets.jl inherit.

Implements the following functionality:
- `getobs(d)` and `getobs(d, i)` falling back to `d[:]` and `d[i]` 
- Pretty printing.
"""
abstract type AbstractDataset <: AbstractDataContainer end

MLUtils.getobs(d::AbstractDataset) = d[:]
MLUtils.getobs(d::AbstractDataset, i) = d[i]

function Base.show(io::IO, d::D) where {D <: AbstractDataset}
    print(io, "$(D.name.name)()")
end

function Base.show(io::IO, ::MIME"text/plain", d::D) where {D <: AbstractDataset}
    recur_io = IOContext(io, :compact => false)

    print(io, "dataset $(D.name.name):")  # if the type is parameterized don't print the parameters

    fnames = filter(!startswith("_"), string.(fieldnames(D)))
    f_length = max(length.(fnames)...)
    for f in fnames
        fstring = leftalign(f, f_length)
        print(recur_io, "\n  $fstring  =>    ")
        # show(recur_io, MIME"text/plain"(), getfield(d, f))
        # println(recur_io)
        print(recur_io, "$(_summary(getfield(d, Symbol(f))))")
    end
end

function leftalign(s::AbstractString, n::Int)
    m = length(s)
    if m > n
        return s[1:n]
    else
        return s * repeat(" ", n - m)
    end
end

_summary(x) = Tables.istable(x) ? summary(x) : x
_summary(x::Symbol) = ":$x"
_summary(x::Dict) = summary(x)
_summary(x::Tuple) = map(_summary, x)
_summary(x::NamedTuple) = map(_summary, x)
_summary(x::AbstractArray) = summary(x)
_summary(x::BitVector) = "$(count(x))-trues BitVector"

"""
    SupervisedDataset <: AbstractDataset

An abstract dataset type for supervised learning tasks. 
Concrete dataset types inheriting from it must provide
a `features` and a `targets` fields.
"""
abstract type SupervisedDataset <: AbstractDataset end

function Base.length(d::SupervisedDataset)
    Tables.istable(d.features) ? numobs_table(d.features) :
    numobs((d.features, d.targets))
end

# We return named tuples
function Base.getindex(d::SupervisedDataset, ::Colon)
    Tables.istable(d.features) ?
    (features = d.features, targets = d.targets) :
    getobs((; d.features, d.targets))
end

function Base.getindex(d::SupervisedDataset, i)
    Tables.istable(d.features) ?
    (features = getobs_table(d.features, i), targets = getobs_table(d.targets, i)) :
    getobs((; d.features, d.targets), i)
end

"""
    UnsupervisedDataset <: AbstractDataset

An abstract dataset type for unsupervised or self-supervised learning tasks. 
Concrete dataset types inheriting from it must provide a `features` field.
"""
abstract type UnsupervisedDataset <: AbstractDataset end

Base.length(d::UnsupervisedDataset) = numobs(d.features)

Base.getindex(d::UnsupervisedDataset, ::Colon) = getobs(d.features)
Base.getindex(d::UnsupervisedDataset, i) = getobs(d.features, i)

### DOCSTRING TEMPLATES ######################

# SUPERVISED TABLE
const ARGUMENTS_SUPERVISED_TABLE = """
- If `as_df = true`, load the data as dataframes instead of plain arrays.

- You can pass a specific `dir` where to load or download the dataset, otherwise uses the default one.
"""

const FIELDS_SUPERVISED_TABLE = """
- `metadata`: A dictionary containing additional information on the dataset.
- `features`: The data features. An array if `as_df=false`, otherwise a dataframe.
- `targets`: The targets for supervised learning. An array if `as_df=false`, otherwise a dataframe.
- `dataframe`: A dataframe containing both `features` and `targets`. It is `nothing` if `as_df=false`, otherwise a dataframed.
"""

const METHODS_SUPERVISED_TABLE = """
- `dataset[i]`: Return observation(s) `i` as a named tuple of features and targets.
- `dataset[:]`: Return all observations as a named tuple of features and targets.
- `length(dataset)`: Number of observations.
"""

# SUPERVISED ARRAY DATASET

const ARGUMENTS_SUPERVISED_ARRAY = """
- You can pass a specific `dir` where to load or download the dataset, otherwise uses the default one.
"""

const FIELDS_SUPERVISED_ARRAY = """
- `metadata`: A dictionary containing additional information on the dataset.
- `features`: An array storing the data features.
- `targets`: An array storing the targets for supervised learning.
"""

const METHODS_SUPERVISED_ARRAY = """
- `dataset[i]`: Return observation(s) `i` as a named tuple of features and targets.
- `dataset[:]`: Return all observations as a named tuple of features and targets.
- `length(dataset)`: Number of observations.
"""

[.\src\download.jl]
function with_accept(f, manual_overwrite)
    auto_accept = if manual_overwrite === nothing
        get(ENV, "DATADEPS_ALWAYS_ACCEPT", false)
    else
        manual_overwrite
    end
    withenv(f, "DATADEPS_ALWAYS_ACCEPT" => string(auto_accept))
end

function datadir(depname, dir = nothing; i_accept_the_terms_of_use = nothing)
    with_accept(i_accept_the_terms_of_use) do
        if dir === nothing
            # use DataDeps defaults
            @datadep_str depname
        else
            # use user-provided dir
            if isdir(dir)
                dir
            else
                DataDeps.env_bool("DATADEPS_DISABLE_DOWNLOAD") &&
                    error("DATADEPS_DISABLE_DOWNLOAD enviroment variable set. Can not trigger download.")
                DataDeps.download(DataDeps.registry[depname], dir)
                dir
            end
        end
    end::String
end

function datafile(depname, filename, dir = nothing; recurse = true, kw...)
    path = joinpath(datadir(depname, dir; kw...), filename)
    if !isfile(path)
        @warn "The file \"$path\" does not exist, even though the dataset-specific folder does. This is an unusual situation that may have been caused by a manual creation of an empty folder, or manual deletion of the given file \"$filename\"."
        if dir === nothing
            @info "Retriggering DataDeps.jl for \"$depname\""
            download_dep(depname; kw...)
        else
            @info "Retriggering DataDeps.jl for \"$depname\" to \"$dir\"."
            download_dep(depname, dir; kw...)
        end
        if recurse
            datafile(depname, filename, dir; recurse = false, kw...)
        else
            error("The file \"$path\" still does not exist. One possible explaination could be a spelling error in the name of the requested file.")
        end
    else
        path
    end::String
end

function download_dep(depname, dir = DataDeps.determine_save_path(depname); kw...)
    DataDeps.download(DataDeps.registry[depname], dir; kw...)
end

function download_docstring(modname, depname)
    """
    The corresponding resource file(s) of the dataset is/are
    expected to be located in the specified directory `dir`. If
    `dir` is omitted the directories in
    `DataDeps.default_loadpath` will be searched for an existing
    `$(depname)` subfolder. In case no such subfolder is found,
    `dir` will default to `~/.julia/datadeps/$(depname)`. In the
    case that `dir` does not yet exist, a download prompt will be
    triggered. You can also use `$(modname).download([dir])`
    explicitly for pre-downloading (or re-downloading) the
    dataset. Please take a look at the documentation of the
    package DataDeps.jl for more detail and configuration
    options.
    """
end

[.\src\graph.jl]

abstract type AbstractGraph end

"""
    Graph(; kws...)

A type that represents a graph and that can also store node and edge data.
It doesn't distinguish between directed or undirected graph, therefore for
undirected graphs will store edges in both directions.
Nodes are indexed in `1:num_nodes`.

Graph datasets in MLDatasets.jl contain one or more `Graph` or [`HeteroGraph`](@ref) objects.

# Keyword Arguments

- `num_nodes`: the number of nodes. If omitted, is inferred from `edge_index`.
- `edge_index`: a tuple containing two vectors with length equal to the number of edges.
    The first vector contains the list of the source nodes of each edge, the second the target nodes.
    Defaults to `(Int[], Int[])`.
- `node_data`: node-related data. Can be `nothing`, a named tuple of arrays or a dictionary of arrays.
            The arrays last dimension size should be equal to the number of nodes.
            Default `nothing`.
- `edge_data`: edge-related data. Can be `nothing`, a named tuple of arrays or a dictionary of arrays.
             The arrays' last dimension size should be equal to the number of edges.
             Default `nothing`.

# Examples

All graph datasets in MLDatasets.jl contain `Graph` or `HeteroGraph` objects:
```julia-repl
julia> using MLDatasets: Cora

julia> d = Cora() # the Cora dataset
dataset Cora:
  metadata    =>    Dict{String, Any} with 3 entries
  graphs      =>    1-element Vector{Graph}

julia> d[1]
Graph:
  num_nodes   =>    2708
  num_edges   =>    10556
  edge_index  =>    ("10556-element Vector{Int64}", "10556-element Vector{Int64}")
  node_data   =>    (features = "1433×2708 Matrix{Float32}", targets = "2708-element Vector{Int64}", train_mask = "2708-element BitVector with 140 trues", val_mask = "2708-element BitVector with 500 trues", test_mask = "2708-element BitVector with 1000 trues")
  edge_data   =>    nothing
```

Let's se how to convert a Graphs.jl's graph to a `MLDatasets.Graph` and viceversa:

```julia
import Graphs, MLDatasets

## From Graphs.jl to MLDatasets.Graphs

# From a directed graph
g = Graphs.erdos_renyi(10, 20, is_directed=true)
s = [e.src for e in Graphs.edges(g)]
t = [e.dst for e in Graphs.edges(g)]
mlg = MLDatasets.Graph(num_nodes=10, edge_index=(s, t))

# From an undirected graph
g = Graphs.erdos_renyi(10, 20, is_directed=false)
s = [e.src for e in Graphs.edges(g)]
t = [e.dst for e in Graphs.edges(g)]
s, t = [s; t], [t; s] # adding reverse edges
mlg = MLDatasets.Graph(num_nodes=10, edge_index=(s, t))

# From MLDatasets.Graphs to Graphs.jl
s, t = mlg.edge_index
g = Graphs.DiGraph(mlg.num_nodes)
for (i, j) in zip(s, t)
    Graphs.add_edge!(g, i, j)
end
```
"""
struct Graph <: AbstractGraph
    num_nodes::Int
    num_edges::Int
    edge_index::Tuple{Vector{Int}, Vector{Int}}
    node_data::Any
    edge_data::Any
end

function Graph(;
               num_nodes::Union{Int, Nothing} = nothing,
               edge_index::Tuple{Vector{Int}, Vector{Int}} = (Int[], Int[]),
               node_data = nothing,
               edge_data = nothing)
    s, t = edge_index
    @assert length(s) == length(t)
    num_edges = length(s)
    if num_nodes === nothing
        if num_edges == 0
            num_nodes = 0
        else
            num_nodes = max(maximum(s), maximum(t))
        end
    end
    return Graph(num_nodes, num_edges, edge_index, node_data, edge_data)
end

function Base.show(io::IO, d::Graph)
    print(io, "Graph($(d.num_nodes), $(d.num_edges))")
end

function Base.show(io::IO, ::MIME"text/plain", d::Graph)
    recur_io = IOContext(io, :compact => false)
    print(io, "Graph:")
    for f in fieldnames(Graph)
        if !startswith(string(f), "_")
            fstring = leftalign(string(f), 10)
            print(recur_io, "\n  $fstring  =>    ")
            print(recur_io, "$(_summary(getfield(d, f)))")
        end
    end
end

"""
    HeteroGraph(; kws...)

HeteroGraph is used for HeteroGeneous Graphs.

`HeteroGraph` unlike `Graph` can have different types of nodes. Each node pertains to different types of information. 

Edges in `HeteroGraph` is defined by relations. A relation is a tuple of 
(`src_node_type`, `edge_type`, `target_node_type`) where `edge_type` represents the relation
between the src and target nodes. Edges between same node types are possible. 

A `HeteroGraph` can be directed or undirected. It doesn't distinguish between directed 
or undirected graphs. Therefore, for undirected graphs, it will store edges in both directions.
Nodes are indexed in `1:num_nodes`.

# Keyword Arguments

- `num_nodes`: Dictionary containing the number of nodes for each node type. If omitted, is inferred from `edge_index`.
- `num_edges`: Dictionary containing the number of edges for each relation.
- `edge_indices`: Dictionary containing the `edge_index` for each edge relation. An `edge_index` is a tuple containing two vectors with length equal to the number of edges for the relation.
    The first vector contains the list of the source nodes of each edge, the second contains the target nodes.
- `node_data`: node-related data. Can be `nothing`, Dictionary of a dictionary of arrays. Data of a specific type of node can be accessed 
            using node_data[node_type].The array's last dimension size should be equal to the number of nodes.
            Default `nothing`.
- `edge_data`: Can be `nothing`, Dictionary of a dictionary of arrays. Data of a specific type of edge can be accessed 
            using edge_data[edge_type].The array's last dimension size should be equal to the number of nodes.
            Default `nothing`.
"""
struct HeteroGraph <: AbstractGraph
    node_types::Vector{String}
    edge_types::Vector{Tuple{String, String, String}}
    num_nodes::Dict{String, Int}
    num_edges::Dict{Tuple{String, String, String}, Int}
    edge_indices::Dict{Tuple{String, String, String}, Tuple{Vector{Int}, Vector{Int}}}
    node_data::Dict{String, Dict}
    edge_data::Dict{Tuple{String, String, String}, Dict}
end

function HeteroGraph(;
                     num_nodes::Union{Dict{String, Int}, Nothing} = nothing,
                     edge_indices,
                     node_data,
                     edge_data)
    num_edges = Dict()
    node_types = isnothing(num_nodes) ? nothing : keys(num_nodes) |> collect
    edge_types = keys(edge_indices) |> collect
    isnothing(num_nodes) && (num_nodes = Dict{String, Int}())
    for (relation, edge_index) in edge_indices
        (from, _, to) = relation
        s, t = edge_index
        @assert length(s) == length(t)
        num_edges[relation] = length(s)

        if !isnothing(node_types)
            @assert to ∈ node_types
            @assert from ∈ node_types
        else
            # try to infer number of nodes from edge indices
            num_nodes[from] = max(maximum(s), get(num_nodes, from, 0))
            num_nodes[to] = max(maximum(t), get(num_nodes, to, 0))
        end
    end
    isnothing(node_types) && (node_types = keys(num_nodes) |> collect)
    return HeteroGraph(node_types, edge_types, num_nodes, num_edges, edge_indices,
                       node_data, edge_data)
end

function Base.show(io::IO, d::HeteroGraph)
    print(io,
          "HeteroGraph($(length(d.num_nodes)) node types, $(length(d.num_edges)) relations)")
end

function Base.show(io::IO, ::MIME"text/plain", d::HeteroGraph)
    recur_io = IOContext(io, :compact => false)
    print(io, "Heterogeneous Graph:")
    for f in fieldnames(HeteroGraph)
        if !startswith(string(f), "_")
            fstring = leftalign(string(f), 12)
            print(recur_io, "\n  $fstring  =>    ")
            print(recur_io, "$(_summary(getfield(d, f)))")
        end
    end
end

struct TemporalSnapshotsGraph <: AbstractGraph
    num_nodes::Vector{Int}
    num_edges::Vector{Int}
    num_snapshots::Int
    snapshots::Vector{Graph}
    graph_data::Any
end


"""
    TemporalSnapshotsGraph(; kws...)

A type that represents a temporal snapshot graph as a sequence of [`Graph`](@ref)s and can store graph data.

Nodes are indexed in `1:num_nodes` and snapshots are indexed in `1:num_snapshots`.

# Keyword Arguments

- `num_nodes`: a vector containing the number of nodes at each snapshot.
- `edge_index`: a tuple containing three vectors.
    The first vector contains the list of the source nodes of each edge, the second the target nodes at the third contains the snapshot at which each edge exists.
    Defaults to `(Int[], Int[], Int[])`.
- `node_data`: node-related data. Can be `nothing`, a vector of named tuples of arrays or a dictionary of arrays.
            The arrays' last dimension size should be equal to the number of nodes.
            Default `nothing`.
- `edge_data`: edge-related data. Can be `nothing`, a vector of named tuples of arrays or a dictionary of arrays.
             The arrays' last dimension size should be equal to the number of edges.
             Default `nothing`.
- `graph_data`: graph-related data. Can be `nothing`, or a named tuple of arrays or a dictionary of arrays.

# Examples

```julia-repl
julia> tg = MLDatasets.TemporalSnapshotsGraph(num_nodes = [10,10,10], edge_index= ([1,3,4,5,6,7,8],[2,6,7,1,2,10,9],[1,1,1,2,2,3,3]), node_data=[rand(3,10), rand(4,10), rand(2,10)])
TemporalSnapshotsGraph:
  num_nodes   =>    3-element Vector{Int64}
  num_edges   =>    3-element Vector{Int64}
  num_snapsh  =>    3
  snapshots   =>    3-element Vector{Main.MLDatasets.Graph}
  graph_data  =>    nothing

julia> tg.snapshots[1] # access the first snapshot
Graph:
  num_nodes   =>    10
  num_edges   =>    3
  edge_index  =>    ("3-element Vector{Int64}", "3-element Vector{Int64}")
  node_data   =>    3×10 Matrix{Float64}
  edge_data   =>    nothing
```    
"""
function TemporalSnapshotsGraph(;
    num_nodes::Vector{Int},
    edge_index::Tuple{Vector{Int}, Vector{Int}, Vector{Int}} = (Int[], Int[], Int[]),
    node_data:: Union{Vector,Nothing} = nothing,
    edge_data:: Union{Vector,Nothing} = nothing,
    graph_data = nothing)

    u, v, t = edge_index
    @assert length(u) == length(v) == length(t)
    num_snapshots = maximum(t)
    if !isnothing(node_data) && !isnothing(edge_data)
        @assert length(node_data) == length(edge_data) == num_snapshots
    end

    snapshots = Vector{Graph}(undef, num_snapshots)
    num_edges = Vector{Int}(undef, num_snapshots)
    for i in 1:num_snapshots
        if !isnothing(node_data) && !isnothing(edge_data)
            snapshot = Graph(num_nodes[i], sum(t.==i), (u[t.==i], v[t.==i]), node_data[i], edge_data[i])
        elseif !isnothing(node_data)
            snapshot = Graph(num_nodes[i], sum(t.==i), (u[t.==i], v[t.==i]), node_data[i],nothing)
        elseif !isnothing(edge_data)
            snapshot = Graph(num_nodes[i], sum(t.==i), (u[t.==i], v[t.==i]), nothing, edge_data[i])
        else
            snapshot = Graph(num_nodes[i], sum(t.==i), (u[t.==i], v[t.==i]), nothing, nothing)
        end
        snapshots[i] = snapshot
        num_edges[i] = sum(t.==i)
    end
return TemporalSnapshotsGraph(num_nodes, num_edges, num_snapshots, snapshots, graph_data)
end

function Base.show(io::IO, d::TemporalSnapshotsGraph)
    print(io, "TemporalSnapshotsGraph($(d.num_nodes), $(d.num_edges), $(d.num_snapshots))")
end

function Base.show(io::IO, ::MIME"text/plain", d::TemporalSnapshotsGraph)
    recur_io = IOContext(io, :compact => false)
    print(io, "TemporalSnapshotsGraph:")
    for f in fieldnames(TemporalSnapshotsGraph)
        if !startswith(string(f), "_")
            fstring = leftalign(string(f), 10)
            print(recur_io, "\n  $fstring  =>    ")
            print(recur_io, "$(_summary(getfield(d, f)))")
        end
    end
end

# Transform an adjacency list to edge index.
# If inneigs = true, assume neighbors from incoming edges.
function adjlist2edgeindex(adj; inneigs = false)
    s, t = Int[], Int[]
    for i in 1:length(adj)
        for j in adj[i]
            push!(s, i)
            push!(t, j)
        end
    end

    if inneigs
        s, t = t, s
    end

    return s, t
end

function edgeindex2adjlist(s, t, num_nodes; inneigs = false)
    adj = [Int[] for _ in 1:num_nodes]
    if inneigs
        s, t = t, s
    end
    for (i, j) in zip(s, t)
        push!(adj[i], j)
    end
    return adj
end

function adjmatrix2edgeindex(adj::AbstractMatrix{T}; weighted = true, inneigs = false) where T
    s, t = Int[], Int[]
    if weighted 
        w = T[]
    end
    for i in 1:size(adj,1)
        for j in 1:size(adj,2)
            if adj[i,j] != 0
                push!(s, i)
                push!(t, j)
                if weighted
                    push!(w, adj[i,j])
                end
            end
        end
    end

    if inneigs
        s, t = t, s
    end
    if weighted
        return s, t, w
    else
        return s, t
    end
end
[.\src\io.jl]

function read_csv(path; kws...)
    return read_csv_asdf(path; kws...)
end

# function read_csv(path, sink::Type{<:AbstractMatrix{T}}; delim=nothing, kws...) where T
#     x = delim === nothing ? readdlm(path, T; kws...) : readdlm(path, delim, T; kws...)
#     return x
# end

function read_csv(path, sink::Type{A}; kws...) where {A <: AbstractMatrix}
    return A(read_csv(path; kws...))
end

function read_csv_asdf(path; kws...)
    return CSV.read(path, DataFrames.DataFrame; kws...)
end

function read_npy(path)
    return FileIO.load(File{format"NPY"}(path)) # FileIO does lazy import of NPZ.jl
end

function read_npz(path)
    return FileIO.load(File{format"NPZ"}(path)) # FileIO does lazy import of NPZ.jl
end

function read_pytorch(path)
    assert_imported(Pickle._lazy_pkgid)
    return Pickle.Torch.THload(path)
end

function read_pickle(path)
    return Pickle.npyload(path)
end

function read_mat(path)
    return MAT.matread(path)
end

function read_json(path)
    return open(JSON3.read, path)
end

function read_chemfile(path)
    return Chemfiles.Trajectory(path)
end

[.\src\MLDatasets.jl]
module MLDatasets

using FixedPointNumbers
using SparseArrays
using Tables
using DataDeps
import MLUtils
using MLUtils: getobs, numobs, AbstractDataContainer
using Printf
using Glob
using DelimitedFiles: readdlm
using FileIO
import CSV
using LazyModules: @lazy
using Statistics

include("require.jl") # export @require

# Use `@lazy import SomePkg` whenever the returned types are not its own types,
# since for methods applied on the returned types we would encounter in world-age issues
# (see discussion in  https://github.com/JuliaML/MLDatasets.jl/pull/128).
# In the other case instead, use `require import SomePkg` to force
# the use to manually import.

@require import JSON3 = "0f8b85d8-7281-11e9-16c2-39a750bddbf1"
@require import DataFrames = "a93c6f00-e57d-5684-b7b6-d8193f3e46c0"
@require import ImageShow = "4e3cecfd-b093-5904-9786-8bbb286a6a31"
@require import Chemfiles = "46823bd8-5fb3-5f92-9aa0-96921f3dd015"
@require import NPZ = "15e1cf62-19b3-5cfa-8e77-841668bca605"

# lazy imported by FileIO
@lazy import Pickle = "fbb45041-c46e-462f-888f-7c521cafbc2c"
@lazy import MAT = "23992714-dd62-5051-b70f-ba57cb901cac"
@lazy import HDF5 = "f67ccb44-e63f-5c2f-98bd-6dc0ccc4ba2f"
# @lazy import JLD2

export getobs, numobs # From MLUtils.jl

include("abstract_datasets.jl")
# export AbstractDataset,
#        SupervisedDataset

include("utils.jl")
export convert2image

include("io.jl")
# export read_csv, read_npy, ...

include("download.jl")

include("containers/filedataset.jl")
export FileDataset
include("containers/cacheddataset.jl")
export CachedDataset
# include("containers/tabledataset.jl")
# export TableDataset

## TODO add back when compatible with `@lazy` or `@require`
## which means that they cannot dispatch on types from JLD2 and HDF5
# include("containers/hdf5dataset.jl")
# export HDF5Dataset
# include("containers/jld2dataset.jl")
# export JLD2Dataset

## Misc.

include("datasets/misc/boston_housing.jl")
export BostonHousing
include("datasets/misc/iris.jl")
export Iris
include("datasets/misc/mutagenesis.jl")
export Mutagenesis
include("datasets/misc/titanic.jl")
export Titanic
include("datasets/misc/wine.jl")
export Wine

## Vision

include("datasets/vision/cifar10_reader/CIFAR10Reader.jl")
include("datasets/vision/cifar10.jl")
export CIFAR10
include("datasets/vision/cifar100_reader/CIFAR100Reader.jl")
include("datasets/vision/cifar100.jl")
export CIFAR100
include("datasets/vision/emnist.jl")
export EMNIST
include("datasets/vision/fashion_mnist.jl")
export FashionMNIST
include("datasets/vision/mnist_reader/MNISTReader.jl")
include("datasets/vision/mnist.jl")
export MNIST
include("datasets/vision/omniglot.jl")
export Omniglot
include("datasets/vision/svhn2.jl")
export SVHN2

## Text

include("datasets/text/ptblm.jl")
export PTBLM
include("datasets/text/udenglish.jl")
export UD_English
include("datasets/text/smsspamcollection.jl")
export SMSSpamCollection

# Graphs
include("graph.jl")
# export Graph

include("datasets/graphs/planetoid.jl")
include("datasets/graphs/traffic.jl")
# export read_planetoid_data
include("datasets/graphs/chickenpox.jl")
export ChickenPox
include("datasets/graphs/cora.jl")
export Cora
include("datasets/graphs/citeseer.jl")
export CiteSeer
include("datasets/graphs/karateclub.jl")
export KarateClub
include("datasets/graphs/AQSOL.jl")
export AQSOL
include("datasets/graphs/movielens.jl")
export MovieLens
include("datasets/graphs/ogbdataset.jl")
export OGBDataset
include("datasets/graphs/organicmaterialsdb.jl")
export OrganicMaterialsDB
include("datasets/graphs/polblogs.jl")
export PolBlogs
include("datasets/graphs/pubmed.jl")
export PubMed
include("datasets/graphs/reddit.jl")
export Reddit
include("datasets/graphs/tudataset.jl")
export TUDataset
include("datasets/graphs/metrla.jl")
export METRLA
include("datasets/graphs/pemsbay.jl")
export PEMSBAY
include("datasets/graphs/temporalbrains.jl")
export TemporalBrains
include("datasets/graphs/windmillenergy.jl")
export WindMillEnergy

# Meshes

include("datasets/meshes/faust.jl")
export FAUST

function __init__()
    # TODO automatically find and execute __init__xxx functions

    # graph
    __init__chickenpox()
    __init__citeseer()
    __init__cora()
    __init__movielens()
    __init__ogbdataset()
    __init__omdb()
    __init__polblogs()
    __init__pubmed()
    __init__reddit()
    __init__tudataset()
    __init__metrla()
    __init__pemsbay()
    __init__temporalbrains()
    __init__windmillenergy()


    # misc
    __init__iris()
    __init__mutagenesis()

    #text
    __init__ptblm()
    __init__smsspam()
    __init__udenglish()

    # vision
    __init__cifar10()
    __init__cifar100()
    __init__emnist()
    __init__fashionmnist()
    __init__mnist()
    __init__omniglot()
    __init__svhn2()

    # mesh
    __init__faust()
end

end #module

[.\src\require.jl]

using Base: invokelatest
using Base: PkgId, UUID

# export @require

const _LOAD_LOCKER = Threads.ReentrantLock()

mutable struct RequireModule
    _require_pkgid::PkgId
    _require_loaded::Bool
end
RequireModule(uuid::UUID, name::String) = RequireModule(PkgId(uuid, name), false)
function Base.Docs.Binding(m::RequireModule, v::Symbol)
    Base.Docs.Binding(checked_import(m._require_pkgid), v)
end
function Base.show(io::IO, m::RequireModule)
    print(io, "RequireModule(", m._require_pkgid.name, ")")
end

function Base.getproperty(m::RequireModule, s::Symbol)
    if s in (:_require_pkgid, :_require_loaded)
        return getfield(m, s)
    end
    lm = require_import(m)
    return getproperty(lm, s)
end

function assert_imported(pkgid::PkgId)
    if !Base.root_module_exists(pkgid)
        name = pkgid.name
        error("Add `import $name` or `using $name` to your code to unlock this functionality.")
    end
end

function require_import(m::RequireModule)
    pkgid = getfield(m, :_require_pkgid)
    if !getfield(m, :_require_loaded)
        assert_imported(pkgid)
        setfield!(m, :_require_loaded, true)
    end
    return Base.root_module(pkgid)
end

"""
    @require import PkgName=UUID

Force the user to add to their code `import package PkgName`.
only if they want the functionality provided by PkgName, 
otherwise PkgName won't be imported and won't affect loading time.

```julia
module MyRequirePkg
    @require import Plots="91a5bcdd-55d7-5caf-9e0b-520d859cae80"
    
    draw_figure(data) = Plots.plot(data, title="MyPkg Plot")
end
```
"""
macro require(ex)
    if ex.head in (:import, :using)
        error("require `@require $(ex)=UUID` format.")
    end
    if ex.head != :(=)
        error("unrecognized expression: $(ex)")
    end
    uuid = UUID(ex.args[2])
    ex = ex.args[1]

    if ex.head != :import
        @warn "only `import` command is supported, fallback to eager mode"
        return ex
    end
    args = ex.args
    if length(args) != 1
        @warn "only single package import is supported, fallback to eager mode"
        return ex
    end
    x = args[1]
    if x.head == :.
        # usage: @require import Foo
        m = _require_load(__module__, x.args[1], uuid, x.args[1])
        # TODO(johnnychen94): the background eager loading seems to work only for Main scope
        isa(m, Module) && return m
        isnothing(m) && return ex
        return m
    elseif x.head == :(:)
        # usage: @require import Foo: foo, bar
        @warn "lazily importing symbols are not supported, fallback to eager mode"
        return ex
    elseif x.head == :as # compat: Julia at least v1.6
        # usage: @require import Foo as RequireFoo
        m = _require_load(__module__, x.args[2], uuid, x.args[1].args[1])
        isa(m, Module) && return m
        isnothing(m) && return ex
        return m
    else
        @warn "unrecognized syntax $ex"
        return ex
    end
end

function _require_load(mod, name::Symbol, uuid::UUID, sym::Symbol)
    if isdefined(mod, name)
        # otherwise, Revise will constantly trigger the constant redefinition warning
        m = getfield(mod, name)
        if m isa RequireModule || m isa Module
            return m
        else
            @warn "Failed to import module, the name `$name` already exists, do nothing"
            return nothing
        end
    end
    try
        m = RequireModule(uuid, String(sym))
        Core.eval(mod, :(const $(name) = $m))
        return m
    catch err
        @warn err
        return nothing
    end
end

[.\src\utils.jl]

function parse_pystring(s::AbstractString)
    s == "False" && return false
    s == "True" && return true
    s == "None" && return nothing
    try
        return parse(Int, s)
    catch
    end
    try
        return parse(Float64, s)
    catch
    end
    return s
end

function restrict_array_type(res::AbstractArray)
    # attempt conversion
    if all(x -> x isa Integer, res)
        return Int.(res)
    elseif all(x -> x isa AbstractFloat, res)
        return Float32.(res)
    elseif all(x -> x isa String, res)
        return String.(res)
    else
        return res
    end
end

function df_to_matrix(df)
    x = Matrix(df)
    if size(x, 2) == 1
        return reshape(x, 1, size(x, 1))
    else
        return permutedims(x, (2, 1))
    end
end

bytes_to_type(::Type{UInt8}, A::Array{UInt8}) = A
bytes_to_type(::Type{N0f8}, A::Array{UInt8}) = reinterpret(N0f8, A)
bytes_to_type(::Type{T}, A::Array{UInt8}) where {T <: Integer} = convert(Array{T}, A)
bytes_to_type(::Type{T}, A::Array{UInt8}) where {T <: AbstractFloat} = A ./ T(255)
function bytes_to_type(::Type{T}, A::Array{UInt8}) where {T <: Number}
    convert(Array{T}, reinterpret(N0f8, A))
end

function clean_nt(nt::NamedTuple)
    res = (; (p for p in pairs(nt) if p[2] !== nothing)...)
    if isempty(res)
        return nothing
    else
        return res
    end
end

function indexes2mask(idxs::AbstractVector{Int}, n)
    mask = falses(n)
    mask[idxs] .= true
    return mask
end

function mask2indexes(mask::BitVector)
    n = length(mask)
    return (1:n)[mask]
end

maybesqueeze(x) = x
maybesqueeze(x::AbstractMatrix) = size(x, 1) == 1 ? vec(x) : x

## Need this until we don't have an interface in Tables.jl
## https://github.com/JuliaData/Tables.jl/pull/278
getobs_table(table) = table
getobs_table(table, i) = table[i, :]
numobs_table(table) = size(table, 1)

"""
    convert2image(d, i)
    convert2image(d, x)
    convert2image(DType, x)

Convert the observation(s) `i` from dataset `d` to image(s).
It can also convert a numerical array `x`.

In order to support a new dataset, e.g. `MyDataset`, 
implement `convert2image(::Type{MyDataset}, x::AbstractArray)`.

# Examples

```julia-repl
julia> using MLDatasets, ImageInTerminal

julia> d = MNIST()

julia> convert2image(d, 1:2) 
# You should see 2 images in the terminal

julia> x = d[1].features;

julia> convert2image(MNIST, x) # or convert2image(d, x)
```
"""
function convert2image end

convert2image(d::SupervisedDataset, i::Integer) = convert2image(typeof(d), d[i].features)
function convert2image(d::SupervisedDataset, i::AbstractVector)
    convert2image(typeof(d), d[i].features)
end
convert2image(d::SupervisedDataset, x::AbstractArray) = convert2image(typeof(d), x)

"""
    creates_default_dir(data_name)

Creates the default datadir for the DataHub or Dataset.
"""
function create_default_dir(data_name::AbstractString)::String
    # don't overrride methods for ManualDataDeps
    dir = DataDeps.determine_save_path(data_name)
    isdir(dir) || mkpath(dir)
    return dir
end

[.\src\containers\cacheddataset.jl]
"""
    make_cache(source, cacheidx)

Return a in-memory copy of `source` at observation indices `cacheidx`.
Defaults to `getobs(source, cacheidx)`.
"""
make_cache(source, cacheidx) = getobs(source, cacheidx)

"""
    CachedDataset(source, cachesize = numbobs(source))
    CachedDataset(source, cacheidx = 1:numbobs(source))
    CachedDataset(source, cacheidx, cache)

Wrap a `source` data container and cache `cachesize` samples in memory.
This can be useful for improving read speeds when `source` is a lazy data container,
but your system memory is large enough to store a sizeable chunk of it.

By default the observation indices `1:cachesize` are cached.
You can manually pass in a set of `cacheidx` as well.

See also [`make_cache`](@ref) for customizing the default cache creation for `source`.
"""
struct CachedDataset{T, S} <: AbstractDataContainer
    source::T
    cacheidx::Vector{Int}
    cache::S
end

CachedDataset(source, cachesize::Int) = CachedDataset(source, 1:cachesize)

function CachedDataset(source, cacheidx::AbstractVector{<:Integer} = 1:numobs(source))
    CachedDataset(source, collect(cacheidx), make_cache(source, cacheidx))
end

function Base.getindex(dataset::CachedDataset, i::Integer)
    _i = findfirst(==(i), dataset.cacheidx)

    return isnothing(_i) ? getobs(dataset.source, i) : getobs(dataset.cache, _i)
end
Base.length(dataset::CachedDataset) = numobs(dataset.source)

[.\src\containers\filedataset.jl]
"""
    rglob(filepattern, dir = pwd(), depth = 4)

Recursive glob up to `depth` layers deep within `dir`.
"""
function rglob(filepattern = "*", dir = pwd(), depth = 4)
    patterns = [repeat("*/", i) * filepattern for i in 0:(depth - 1)]

    return vcat([glob(pattern, dir) for pattern in patterns]...)
end

"""
    FileDataset([loadfn = FileIO.load,] paths)
    FileDataset([loadfn = FileIO.load,] dir, pattern = "*", depth = 4)

Wrap a set of file `paths` as a dataset (traversed in the same order as `paths`).
Alternatively, specify a `dir` and collect all paths that match a glob `pattern`
(recursively globbing by `depth`). The glob order determines the traversal order.
"""
struct FileDataset{F, T <: AbstractString} <: AbstractDataContainer
    loadfn::F
    paths::Vector{T}
end

FileDataset(paths) = FileDataset(FileIO.load, paths)
function FileDataset(loadfn,
                     dir::AbstractString,
                     pattern::AbstractString = "*",
                     depth = 4)
    FileDataset(loadfn, rglob(pattern, string(dir), depth))
end
function FileDataset(dir::AbstractString, pattern::AbstractString = "*", depth = 4)
    FileDataset(FileIO.load, dir, pattern, depth)
end

Base.getindex(dataset::FileDataset, i::Integer) = dataset.loadfn(dataset.paths[i])
function Base.getindex(dataset::FileDataset, is::AbstractVector)
    map(Base.Fix1(getobs, dataset), is)
end
Base.length(dataset::FileDataset) = length(dataset.paths)

[.\src\containers\hdf5dataset.jl]
function _check_hdf5_shapes(shapes)
    nobs = map(last, filter(!isempty, shapes))

    return all(==(first(nobs)), nobs[2:end])
end

"""
    HDF5Dataset(file::AbstractString, paths)
    HDF5Dataset(fid::HDF5.File, paths::Union{HDF5.Dataset, Vector{HDF5.Dataset}})
    HDF5Dataset(fid::HDF5.File, paths::Union{AbstractString, Vector{<:AbstractString}})
    HDF5Dataset(fid::HDF5.File, paths::Union{HDF5.Dataset, Vector{HDF5.Dataset}}, shapes)

Wrap several HDF5 datasets (`paths`) as a single dataset container.
Each dataset `p` in `paths` should be accessible as `fid[p]`.
Calling `getobs` on a `HDF5Dataset` returns a tuple with each element corresponding
to the observation from each dataset in `paths`.
See [`close(::HDF5Dataset)`](@ref) for closing the underlying HDF5 file pointer.

For array datasets, the last dimension is assumed to be the observation dimension.
For scalar datasets, the stored value is returned by `getobs` for any index.
"""
struct HDF5Dataset{T <: Union{HDF5.Dataset, Vector{HDF5.Dataset}}} <: AbstractDataContainer
    fid::HDF5.File
    paths::T
    shapes::Vector{Tuple}

    function HDF5Dataset(fid::HDF5.File, paths::T,
                         shapes::Vector) where {
                                                T <:
                                                Union{HDF5.Dataset, Vector{HDF5.Dataset}}}
        _check_hdf5_shapes(shapes) ||
            throw(ArgumentError("Cannot create HDF5Dataset for datasets with mismatched number of observations."))

        new{T}(fid, paths, shapes)
    end
end

HDF5Dataset(fid::HDF5.File, path::HDF5.Dataset) = HDF5Dataset(fid, path, [size(path)])
function HDF5Dataset(fid::HDF5.File, paths::Vector{HDF5.Dataset})
    HDF5Dataset(fid, paths, map(size, paths))
end
HDF5Dataset(fid::HDF5.File, path::AbstractString) = HDF5Dataset(fid, fid[path])
function HDF5Dataset(fid::HDF5.File, paths::Vector{<:AbstractString})
    HDF5Dataset(fid, map(p -> fid[p], paths))
end
HDF5Dataset(file::AbstractString, paths) = HDF5Dataset(h5open(file, "r"), paths)

_getobs_hdf5(dataset::HDF5.Dataset, ::Tuple{}, i) = read(dataset)
function _getobs_hdf5(dataset::HDF5.Dataset, shape, i)
    I = map(s -> 1:s, shape[1:(end - 1)])

    return dataset[I..., i]
end
function Base.getindex(dataset::HDF5Dataset{HDF5.Dataset}, i)
    _getobs_hdf5(dataset.paths, only(dataset.shapes), i)
end
function Base.getindex(dataset::HDF5Dataset{<:Vector}, i)
    Tuple(map((p, s) -> _getobs_hdf5(p, s, i), dataset.paths, dataset.shapes))
end
Base.length(dataset::HDF5Dataset) = last(first(filter(!isempty, dataset.shapes)))

"""
    close(dataset::HDF5Dataset)

Close the underlying HDF5 file pointer for `dataset`.
"""
Base.close(dataset::HDF5Dataset) = close(dataset.fid)

[.\src\containers\jld2dataset.jl]
_check_jld2_nobs(nobs) = all(==(first(nobs)), nobs[2:end])

"""
    JLD2Dataset(file::AbstractString, paths)
    JLD2Dataset(fid::JLD2.JLDFile, paths::Union{String, Vector{String}})

Wrap several JLD2 datasets (`paths`) as a single dataset container.
Each dataset `p` in `paths` should be accessible as `fid[p]`.
Calling `getobs` on a `JLD2Dataset` is equivalent to mapping `getobs` on
each dataset in `paths`.
See [`close(::JLD2Dataset)`](@ref) for closing the underlying JLD2 file pointer.
"""
struct JLD2Dataset{T <: JLD2.JLDFile, S <: Tuple} <: AbstractDataContainer
    fid::T
    paths::S

    function JLD2Dataset(fid::JLD2.JLDFile, paths)
        _paths = Tuple(map(p -> fid[p], paths))
        nobs = map(numobs, _paths)
        _check_jld2_nobs(nobs) ||
            throw(ArgumentError("Cannot create JLD2Dataset for datasets with mismatched number of observations (got $nobs)."))

        new{typeof(fid), typeof(_paths)}(fid, _paths)
    end
end

JLD2Dataset(file::JLD2.JLDFile, path::String) = JLD2Dataset(file, (path,))
JLD2Dataset(file::AbstractString, paths) = JLD2Dataset(jldopen(file, "r"), paths)

function Base.getindex(dataset::JLD2Dataset{<:JLD2.JLDFile, <:NTuple{1}}, i)
    getobs(only(dataset.paths), i)
end
Base.getindex(dataset::JLD2Dataset, i) = map(Base.Fix2(getobs, i), dataset.paths)
Base.length(dataset::JLD2Dataset) = numobs(dataset.paths[1])

"""
    close(dataset::JLD2Dataset)

Close the underlying JLD2 file pointer for `dataset`.
"""
Base.close(dataset::JLD2Dataset) = close(dataset.fid)

[.\src\containers\tabledataset.jl]
"""
    TableDataset(table)
    TableDataset(path::AbstractString)

Wrap a Tables.jl-compatible `table` as a dataset container.
Alternatively, specify the `path` to a CSV file directly
to load it with CSV.jl + DataFrames.jl.
"""
struct TableDataset{T} <: AbstractDataContainer
    table::T

    # TableDatasets must implement the Tables.jl interface
    function TableDataset{T}(table::T) where {T}
        Tables.istable(table) ||
            throw(ArgumentError("The input must implement the Tables.jl interface"))

        new{T}(table)
    end
end

TableDataset(table::T) where {T} = TableDataset{T}(table)
TableDataset(path::AbstractString) = TableDataset(read_csv(path))

# slow accesses based on Tables.jl
_getobs_row(x, i) = first(Iterators.peel(Iterators.drop(x, i - 1)))
function _getobs_column(x, i)
    colnames = Tuple(Tables.columnnames(x))
    rowvals = ntuple(j -> Tables.getcolumn(x, j)[i], length(colnames))

    return NamedTuple{colnames}(rowvals)
end

function getobs_table(table, i)
    if Tables.rowaccess(table)
        return _getobs_row(Tables.rows(table), i)
    elseif Tables.columnaccess(table)
        return _getobs_column(table, i)
    else
        error("The Tables.jl implementation used should have either rowaccess or columnaccess.")
    end
end

function numobs_table(table)
    if Tables.columnaccess(table)
        return length(Tables.getcolumn(table, 1))
    elseif Tables.rowaccess(table)
        # length might not be defined, but has to be for this to work.
        return length(Tables.rows(table))
    else
        error("The Tables.jl implementation used should have either rowaccess or columnaccess.")
    end
end

Base.getindex(dataset::TableDataset, i) = getobs_table(dataset.table, i)
Base.length(dataset::TableDataset) = numobs_table(dataset.table)

# fast access for DataFrame
# Base.getindex(dataset::TableDataset{<:DataFrame}, i) = dataset.table[i, :]
# Base.length(dataset::TableDataset{<:DataFrame}) = nrow(dataset.table)

# fast access for CSV.File
# Base.getindex(dataset::TableDataset{<:CSV.File}, i) = dataset.table[i]
# Base.length(dataset::TableDataset{<:CSV.File}) = length(dataset.table)

## Tables.jl interface

Tables.istable(::TableDataset) = true

for fn in (:rowaccess, :rows, :columnaccess, :columns, :schema, :materializer)
    @eval Tables.$fn(dataset::TableDataset) = Tables.$fn(dataset.table)
end

[.\src\datasets\graphs\AQSOL.jl]
export AQSOL

[.\src\datasets\graphs\chickenpox.jl]
function __init__chickenpox()
    DEPNAME = "ChickenPox"
    LINK = "https://graphmining.ai/temporal_datasets/"
    register(ManualDataDep(DEPNAME,
                           """
                           Dataset: $DEPNAME
                           Website : $LINK
                           """))
end

function chickenpox_datadir(dir = nothing)
    dir = isnothing(dir) ? datadep"ChickenPox" : dir
    LINK = "http://www-sop.inria.fr/members/Aurora.Rossi/data/chickenpox.json"
    if length(readdir((dir))) == 0
        DataDeps.fetch_default(LINK, dir)
    end
    @assert isdir(dir)
    return dir
end

function generate_task_chickenpox(data::AbstractArray, num_timesteps_in::Int, num_timesteps_out::Int)
    features = []
    targets = []
    for i in 1:(size(data,3)-num_timesteps_in-num_timesteps_out)
        push!(features, data[:,:,i:i+num_timesteps_in-1])
        push!(targets, data[:,:,i+num_timesteps_in:i+num_timesteps_in+num_timesteps_out-1])
    end
    return features, targets
end

function create_chickenpox_dataset( normalize::Bool, num_timesteps_in::Int, num_timesteps_out::Int, dir)
    name_file = joinpath(dir, "chickenpox.json")
    data = read_json(name_file)
    src = zeros(Int, length(data["edges"]))
    dst = zeros(Int, length(data["edges"]))
    for (i, edge) in enumerate(data["edges"])
        src[i] = edge[1] + 1
        dst[i] = edge[2] + 1
    end
    f = Float32.(stack(data["FX"]))
    f = reshape(f, 1, size(f, 1), size(f, 2))

    metadata = Dict(key => value + 1 for (key, value) in data["node_ids"])

    if normalize
        f = (f .- Statistics.mean(f, dims=(2))) ./ Statistics.std(f, dims=(2)) #Z-score normalization
    end

    x, y = generate_task_chickenpox(f, num_timesteps_in, num_timesteps_out)

    g = Graph(; edge_index = (src, dst),
                node_data = (features = x, targets = y))
    return g, metadata
end

"""
    ChickenPox(; normalize= true, num_timesteps_in = 8 , num_timesteps_out = 8, dir = nothing)

The ChickenPox dataset contains county-level chickenpox cases in Hungary between 2004 and 2014.

`ChickenPox` is composed of a graph with nodes representing counties and edges representing the neighborhoods, and a metadata dictionary containing the correspondence between the node indices and the county names. 

The node features are the number of weekly chickenpox cases in each county. They are represented as an array of arrays of size `(1, num_nodes, num_timesteps_in)`. The target values are the number of weekly chickenpox cases in each county. They are represented as an array of arrays of size `(1, num_nodes, num_timesteps_out)`. In both cases. two consecutive arrays are shifted by one-time step.

The dataset was taken from the [Pytorch Geometric Temporal repository](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/dataset.html#torch_geometric_temporal.dataset.chickenpox.ChickenpoxDatasetLoader) and more information about the dataset can be found in the paper ["Chickenpox Cases in Hungary: a Benchmark Dataset for
Spatiotemporal Signal Processing with Graph Neural Networks"](https://arxiv.org/pdf/2102.08100).


# Keyword Arguments
- `normalize::Bool`: Whether to normalize the data using Z-score normalization. Default is `true`.
- `num_timesteps_in::Int`: The number of time steps, in this case, the number of weeks, for the input features. Default is `8`.
- `num_timesteps_out::Int`: The number of time steps, in this case, the number of weeks, for the target values. Default is `8`.
- `dir::String`: The directory to save the dataset. Default is `nothing`.

# Examples
```julia-repl
julia> using JSON3 # import JSON3

julia> dataset = ChickenPox()
dataset ChickenPox:
  metadata  =>    Dict{Symbol, Any} with 20 entries
  graphs    =>    1-element Vector{MLDatasets.Graph}

julia> dataset.graphs[1].num_nodes # 20 counties
20

julia> size(dataset.graphs[1].node_data.features[1]) 
(1, 20, 8)

julia> dataset.metadata[:BUDAPEST] # The node 5 correponds to Budapest county 
5
```
"""
struct ChickenPox <: AbstractDataset
    metadata::Dict{Symbol, Any}
    graphs::Vector{Graph}
end

function ChickenPox(; normalize::Bool = true, num_timesteps_in::Int = 8 , num_timesteps_out::Int = 8, dir = nothing)
    create_default_dir("ChickenPox")
    dir = chickenpox_datadir(dir)
    g, metadata = create_chickenpox_dataset(normalize, num_timesteps_in, num_timesteps_out, dir)
    return ChickenPox(metadata, [g])
end

Base.length(d::ChickenPox) = length(d.graphs)
Base.getindex(d::ChickenPox, ::Colon) = d.graphs[1]
Base.getindex(d::ChickenPox, i) = getindex(d.graphs, i)
[.\src\datasets\graphs\citeseer.jl]
function __init__citeseer()
    DEPNAME = "CiteSeer"
    LINK = "https://github.com/kimiyoung/planetoid/raw/master/data"
    DOCS = "https://github.com/kimiyoung/planetoid"
    DATA = "ind.citeseer." .* ["x", "y", "tx", "allx", "ty", "ally", "graph", "test.index"]

    register(DataDep(DEPNAME,
                     """
                     Dataset: The $DEPNAME dataset.
                     Website: $DOCS
                     """,
                     map(x -> "$LINK/$x", DATA),
                     "7f7ec4df97215c573eee316de35754d89382011dfd9fb2b954a4a491057e3eb3"  # if checksum omitted, will be generated by DataDeps
                     # post_fetch_method = unpack
                     ))
end

"""
    CiteSeer(; dir=nothing)

The CiteSeer citation network dataset from Ref. [1].
Nodes represent documents and edges represent citation links.
The dataset is designed for the node classification task. 
The task is to predict the category of certain paper.
The dataset is retrieved from Ref. [2].

# References

[1]: [Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking](https://arxiv.org/abs/1707.03815) 
 
[2]: [Planetoid](https://github.com/kimiyoung/planetoid)
"""
struct CiteSeer <: AbstractDataset
    metadata::Dict{String, Any}
    graphs::Vector{Graph}
end

function CiteSeer(; dir = nothing, reverse_edges = true)
    metadata, g = read_planetoid_data("CiteSeer", dir = dir, reverse_edges = reverse_edges)
    return CiteSeer(metadata, [g])
end

Base.length(d::CiteSeer) = length(d.graphs)
Base.getindex(d::CiteSeer, ::Colon) = d.graphs[1]
Base.getindex(d::CiteSeer, i) = d.graphs[i]

# DEPRECATED in v0.6.0
function Base.getproperty(::Type{CiteSeer}, s::Symbol)
    if s === :dataset
        @warn "CiteSeer.dataset() is deprecated, use `CiteSeer()` instead."
        function dataset(; dir = nothing)
            d = CiteSeer(; dir)
            g = d[1]
            adjacency_list = edgeindex2adjlist(g.edge_index..., g.num_nodes)
            return (; g.num_nodes, g.num_edges,
                    node_features = g.node_data.features,
                    node_labels = g.node_data.targets,
                    train_indices = mask2indexes(g.node_data.train_mask),
                    val_indices = mask2indexes(g.node_data.val_mask),
                    test_indices = mask2indexes(g.node_data.test_mask),
                    adjacency_list)
        end
        return dataset
    else
        return getfield(CiteSeer, s)
    end
end

[.\src\datasets\graphs\cora.jl]
function __init__cora()
    DEPNAME = "Cora"
    # LINK = "https://github.com/shchur/gnn-benchmark/raw/master/data/npz"
    # LINK = "https://github.com/abojchevski/graph2gauss/raw/master/data/"
    LINK = "https://github.com/kimiyoung/planetoid/raw/master/data"
    DOCS = "https://github.com/kimiyoung/planetoid"
    DATA = "ind.cora." .* ["x", "y", "tx", "allx", "ty", "ally", "graph", "test.index"]

    register(DataDep(DEPNAME,
                     """
                     Dataset: The $DEPNAME dataset.
                     Website: $DOCS
                     """,
                     map(x -> "$LINK/$x", DATA),
                     "81de017067dc045ebdb8ffd5c0e69a209973ffdb1fe2d5b434e94d3614f3f5c7"  # if checksum omitted, will be generated by DataDeps
                     # post_fetch_method = unpack
                     ))
end

"""
    Cora()

The Cora citation network dataset from Ref. [1].
Nodes represent documents and edges represent citation links.
Each node has a predefined feature with 1433 dimensions. 
The dataset is designed for the node classification task. 
The task is to predict the category of certain paper.
The dataset is retrieved from Ref. [2].

# Statistics 

- Nodes: 2708
- Edges: 10556
- Number of Classes: 7
- Label split:
    - Train:  140
    - Val:    500
    - Test:  1000

The split is the one used in the original paper [1] and 
doesn't consider all nodes.


# References

[1]: [Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking](https://arxiv.org/abs/1707.03815)

[2]: [Planetoid](https://github.com/kimiyoung/planetoid)
"""
struct Cora <: AbstractDataset
    metadata::Dict{String, Any}
    graphs::Vector{Graph}
end

function Cora(; dir = nothing, reverse_edges = true)
    metadata, g = read_planetoid_data("Cora", dir = dir, reverse_edges = reverse_edges)
    return Cora(metadata, [g])
end

Base.length(d::Cora) = length(d.graphs)
Base.getindex(d::Cora, ::Colon) = d.graphs[1]
Base.getindex(d::Cora, i) = getindex(d.graphs, i)

# DEPRECATED in v0.6.0
function Base.getproperty(::Type{Cora}, s::Symbol)
    if s === :dataset
        @warn "Cora.dataset() is deprecated, use `Cora()` instead."
        function dataset(; dir = nothing)
            d = Cora(; dir)
            g = d[1]
            adjacency_list = edgeindex2adjlist(g.edge_index..., g.num_nodes)
            return (; g.num_nodes, g.num_edges,
                    node_features = g.node_data.features,
                    node_labels = g.node_data.targets,
                    train_indices = mask2indexes(g.node_data.train_mask),
                    val_indices = mask2indexes(g.node_data.val_mask),
                    test_indices = mask2indexes(g.node_data.test_mask),
                    adjacency_list)
        end
        return dataset
    else
        return getfield(Cora, s)
    end
end

[.\src\datasets\graphs\karateclub.jl]
export KarateClub

"""
    KarateClub()

The Zachary's karate club dataset originally appeared in Ref [1].

The network contains 34 nodes (members of the karate club).
The nodes are connected by 78 undirected and unweighted edges.
The edges indicate if the two members interacted outside the club.

The node labels indicate which community or the karate club the member belongs to.
The club based labels are as per the original dataset in Ref [1].
The community labels are obtained by modularity-based clustering following Ref [2].
The data is retrieved from Ref [3] and [4].
One node per unique label is used as training data.

# References

[1]: [An Information Flow Model for Conflict and Fission in Small Groups](http://www1.ind.ku.dk/complexLearning/zachary1977.pdf)

[2]: [Semi-supervised Classification with Graph Convolutional Networks](https://arxiv.org/abs/1609.02907)

[3]: [PyTorch Geometric Karate Club Dataset](https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/datasets/karate.html#KarateClub)

[4]: [NetworkX Zachary's Karate Club Dataset](https://networkx.org/documentation/stable/_modules/networkx/generators/social.html#karate_club_graph)
"""
struct KarateClub <: AbstractDataset
    metadata::Dict{String, Any}
    graphs::Vector{Graph}
end

function KarateClub()
    src = [
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5,
        6, 6, 6, 6, 7, 7, 7, 7, 8, 8, 8, 8, 9, 9, 9, 9, 9, 10, 10, 11, 11,
        11, 12, 13, 13, 14, 14, 14, 14, 14, 15, 15, 16, 16, 17, 17, 18, 18,
        19, 19, 20, 20, 20, 21, 21, 22, 22, 23, 23, 24, 24, 24, 24, 24, 25,
        25, 25, 26, 26, 26, 27, 27, 28, 28, 28, 28, 29, 29, 29, 30, 30, 30,
        30, 31, 31, 31, 31, 32, 32, 32, 32, 32, 32, 33, 33, 33, 33, 33, 33,
        33, 33, 33, 33, 33, 33, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,
        34, 34, 34, 34, 34, 34,
    ]
    target = [
        2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 18, 20, 22, 32, 1, 3, 4, 8,
        14, 18, 20, 22, 31, 1, 2, 4, 8, 9, 10, 14, 28, 29, 33, 1, 2, 3, 8,
        13, 14, 1, 7, 11, 1, 7, 11, 17, 1, 5, 6, 17, 1, 2, 3, 4, 1, 3, 31,
        33, 34, 3, 34, 1, 5, 6, 1, 1, 4, 1, 2, 3, 4, 34, 33, 34, 33, 34, 6,
        7, 1, 2, 33, 34, 1, 2, 34, 33, 34, 1, 2, 33, 34, 26, 28, 30, 33,
        34, 26, 28, 32, 24, 25, 32, 30, 34, 3, 24, 25, 34, 3, 32, 34, 24,
        27, 33, 34, 2, 9, 33, 34, 1, 25, 26, 29, 33, 34, 3, 9, 15, 16, 19,
        21, 23, 24, 30, 31, 32, 34, 9, 10, 14, 15, 16, 19, 20, 21, 23, 24,
        27, 28, 29, 30, 31, 32, 33,
    ]

    labels_clubs = [
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    labels_comm = [
        1, 1, 1, 1, 3, 3, 3, 1, 0, 1, 3, 1, 1, 1, 0, 0, 3, 1, 0, 1, 0, 1,
        0, 0, 2, 2, 0, 0, 2, 0, 0, 2, 0, 0]

    node_data = (; labels_clubs, labels_comm)
    g = Graph(; num_nodes = 34, edge_index = (src, target), node_data)

    metadata = Dict{String, Any}()
    return KarateClub(metadata, [g])
end

Base.length(d::KarateClub) = length(d.graphs)
Base.getindex(d::KarateClub, ::Colon) = d.graphs[1]
Base.getindex(d::KarateClub, i) = d.graphs[i]

[.\src\datasets\graphs\metrla.jl]
function __init__metrla()
    DEPNAME = "METRLA"
    LINK = "http://www-sop.inria.fr/members/Aurora.Rossi/index.html"
    register(ManualDataDep(DEPNAME,
                           """
                           Dataset: $DEPNAME
                           Website : $LINK
                           """))
end

"""
    METRLA(; num_timesteps_in::Int = 12, num_timesteps_out::Int=12, dir=nothing, normalize = true)

The METR-LA dataset from the [Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting](https://arxiv.org/abs/1707.01926) paper.

`METRLA` is a graph with 207 nodes representing traffic sensors in Los Angeles. 

The edge weights `w` are contained as a feature array in `edge_data` and represent the distance between the sensors. 

The node features are the traffic speed and the time of the measurements collected by the sensors, divided into `num_timesteps_in` time steps. 

The target values are the traffic speed of the measurements collected by the sensors, divided into `num_timesteps_out` time steps.

The `normalize` flag indicates whether the data are normalized using Z-score normalization.
"""
struct METRLA <: AbstractDataset
    graphs::Vector{Graph}
end

function METRLA(;num_timesteps::Int = 12, dir = nothing, normalize = true)
    s, t, w, x, y = processed_traffic("METRLA", num_timesteps, dir, normalize)

    g = Graph(; num_nodes = 207,
              edge_index = (s, t),
              edge_data = w,
            node_data = (features = x, targets = y))
            
    return METRLA([g])
end

Base.length(d::METRLA) = length(d.graphs)
Base.getindex(d::METRLA, ::Colon) = d.graphs[1]
Base.getindex(d::METRLA, i) = getindex(d.graphs, i)

[.\src\datasets\graphs\movielens.jl]
function __init__movielens()
    DEPNAME = "MovieLens"
    DOCS = "https://grouplens.org/datasets/movielens/"

    register(ManualDataDep(DEPNAME,
                           """
                           Datahub: $DEPNAME.
                           Website: $DOCS
                           """))
end

"""
    MovieLens(name; dir=nothing)

Datasets from the [MovieLens website](https://movielens.org) collected and maintained by [GroupLens](https://grouplens.org/datasets/movielens/). 
The MovieLens datasets are presented in a Graph format. 
For license and usage restrictions please refer to the Readme.md of the datasets.

There are 6 versions of MovieLens datasets currently supported: "100k",  "1m",  "10m", "20m", "25m", "latest-small". 
The 100k and 1k datasets contain movie data and rating data along with demographic data.
Starting from the 10m dataset, MovieLens datasets no longer contain the demographic data. 
These datasets contain movie data, rating data, and tag information. 

The 20m and 25m datasets additionally contain [genome tag scores](http://files.grouplens.org/papers/tag_genome.pdf). 
Each movie in these datasets contains tag relevance scores for every tag.

Each dataset contains an heterogeneous graph, with two kinds of nodes, 
`movie` and `user`. The rating is represented by an edge between them: `(user, rating, movie)`. 
20m, 25m, and latest-small datasets also contain `tag` nodes and edges of type `(user, tag, movie)` and 
optionally `(movie, score, tag)`.

# Examples

## MovieLens 100K dataset

```julia-repl
julia> data = MovieLens("100k")
MovieLens 100k:
  metadata    =>    Dict{String, Any} with 2 entries
  graphs      =>    1-element Vector{MLDatasets.HeteroGraph}

julia> metadata = data.metadata
Dict{String, Any} with 2 entries:
  "genre_labels"      => ["Unknown", "Action", "Adventure", "Animation", "Children", "Comedy", "Crime", "Documentary", "Drama", "Fa…
  "movie_id_to_title" => Dict(1144=>"Quiet Room, The (1996)", 1175=>"Hugo Pool (1997)", 719=>"Canadian Bacon (1994)", 1546=>"Shadow…

julia> g = data[:]
  Heterogeneous Graph:
    node_types    =>    2-element Vector{String}
    edge_types    =>    1-element Vector{Tuple{String, String, String}}
    num_nodes     =>    Dict{String, Int64} with 2 entries
    num_edges     =>    Dict{Tuple{String, String, String}, Int64} with 1 entry
    edge_indices  =>    Dict{Tuple{String, String, String}, Tuple{Vector{Int64}, Vector{Int64}}} with 1 entry
    node_data     =>    Dict{String, Dict} with 2 entries
    edge_data     =>    Dict{Tuple{String, String, String}, Dict} with 1 entry

# Access the user information
julia> user_data = g.node_data["user"]
Dict{Symbol, AbstractVector} with 4 entries:
  :age        => [24, 53, 23, 24, 33, 42, 57, 36, 29, 53  …  61, 42, 24, 48, 38, 26, 32, 20, 48, 22]
  :occupation => ["technician", "other", "writer", "technician", "other", "executive", "administrator", "administrator", "student",…
  :zipcode    => ["85711", "94043", "32067", "43537", "15213", "98101", "91344", "05201", "01002", "90703"  …  "22902", "66221", "3…
  :gender     => Bool[1, 0, 1, 1, 0, 1, 1, 1, 1, 1  …  1, 1, 1, 1, 0, 0, 1, 1, 0, 1]

# Access rating information
julia> g.edge_data[("user", "rating", "movie")]
Dict{Symbol, Vector} with 2 entries:
  :timestamp => [881250949, 891717742, 878887116, 880606923, 886397596, 884182806, 881171488, 891628467, 886324817, 883603013  …  8…
  :rating    => Float16[3.0, 3.0, 1.0, 2.0, 1.0, 4.0, 2.0, 5.0, 3.0, 3.0  …  4.0, 4.0, 3.0, 2.0, 3.0, 3.0, 5.0, 1.0, 2.0, 3.0]
```

## MovieLens 20m dataset

```julia-repl
julia> data = MovieLens("20m")
MovieLens 20m:
  metadata    =>    Dict{String, Any} with 4 entries
  graphs      =>    1-element Vector{MLDatasets.HeteroGraph}

# There is only 1 graph in MovieLens dataset
julia> g = data[1]
Heterogeneous Graph:
  node_types    =>    3-element Vector{String}
  edge_types    =>    3-element Vector{Tuple{String, String, String}}
  num_nodes     =>    Dict{String, Int64} with 3 entries
  num_edges     =>    Dict{Tuple{String, String, String}, Int64} with 3 entries
  edge_indices  =>    Dict{Tuple{String, String, String}, Tuple{Vector{Int64}, Vector{Int64}}} with 3 entries
  node_data     =>    Dict{String, Dict} with 0 entries
  edge_data     =>    Dict{Tuple{String, String, String}, Dict} with 3 entries

# Apart from user rating a movie, a user assigns tag to movies and there are genome-scores for movie-tag pairs 
julia> g.edge_indices
  Dict{Tuple{String, String, String}, Tuple{Vector{Int64}, Vector{Int64}}} with 3 entries:
    ("movie", "score", "tag")   => ([1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  131170, 131170, 131170, 131170, 131170, 131170, 131170, 131170,…
    ("user", "tag", "movie")    => ([18, 65, 65, 65, 65, 65, 65, 65, 65, 65  …  3489, 7045, 7045, 7164, 7164, 55999, 55999, 55999, 55…
    ("user", "rating", "movie") => ([1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  60816, 61160, 65682, 66762, 68319, 68954, 69526, 69644, 70286, …

# Access the rating
julia> g.edge_data[("user", "rating", "movie")]
Dict{Symbol, Vector} with 2 entries:
  :timestamp => [1112486027, 1112484676, 1112484819, 1112484727, 1112484580, 1094785740, 1094785734, 1112485573, 1112484940, 111248…
  :rating    => Float16[3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 4.0, 4.0, 4.0, 4.0  …  4.5, 4.0, 4.5, 4.5, 4.5, 4.5, 4.5, 3.0, 5.0, 2.5]

# Access the movie-tag scores
score = g.edge_data[("movie", "score", "tag")][:score]
23419536-element Vector{Float64}:
 0.025000000000000022
 0.025000000000000022
 0.057750000000000024
 ⋮
```

## References

[1] [GroupLens Website](https://grouplens.org/datasets/movielens/)

[2] [TensorFlow MovieLens Implementation](https://www.tensorflow.org/datasets/catalog/movielens)   

[3] Jesse Vig, Shilad Sen, and John Riedl. 2012. The Tag Genome: Encoding Community Knowledge to Support Novel Interaction. ACM Trans. Interact. Intell. Syst. 2, 3, Article 13 (September 2012), 44 pages. https://doi.org/10.1145/2362394.2362395.   

[4] F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Trans. Interact. Intell. Syst. 5, 4, Article 19 (January 2016), 19 pages. https://doi.org/10.1145/2827872  
"""
struct MovieLens
    name::String
    metadata::Dict{String, Any}
    graphs::Vector{HeteroGraph}
end

function MovieLens(name::String; dir = nothing)
    name = lowercase(name)
    create_default_dir("MovieLens")
    dir = movielens_datadir(name, dir)
    # format varied in older generations of MovieLens datasets
    if name == "100k"
        data = read_100k_data(dir)
    elseif name == "1m"
        data = read_1m_data(dir)
    elseif name == "10m"
        data = read_10m_data(dir)
    elseif name in ["20m", "25m", "latest-small"]
        data = read_current_data(dir)
    else
        error("Functionality for ml-$name has not been implemented yet")
    end
    g = generate_movielens_graph(data...)
    metadata = get_movielens_metadata(data)
    return MovieLens(name, metadata, [g])
end

function read_100k_data(dir::String)
    user_data = read_100k_user_data(dir)
    movie_data = read_100k_movie_data(dir)
    rating_data = read_100k_rating_data(dir)
    return (user_data, movie_data, rating_data)
end

function read_1m_data(dir::String)
    user_data = read_1m_user_data(dir)
    movie_data = read_1m_movie_data(dir)
    rating_data = read_1m_rating_data(dir)
    return (user_data, movie_data, rating_data)
end

function read_10m_data(dir::String)
    rating_data = read_10m_rating_data(dir)
    movies_data = read_10m_movie_data(dir)
    user_tag_data = read_10m_user_tag_data(dir)
    return (movies_data, rating_data, user_tag_data, Dict(), Dict())
end

function read_current_data(dir::String)
    rating_data = read_current_rating_data(dir)
    genome_tag_data = read_genome_tag_data(dir)
    user_tag_data = read_current_user_tag_data(dir)
    movies_data = read_current_movie_data(dir)
    link_data = read_link_data(dir)
    return (movies_data, rating_data, user_tag_data, genome_tag_data, link_data)
end

function read_10m_rating_data(dir::String)
    rating_data_file = "ratings.dat"
    rating_df = read_csv_asdf(joinpath(dir, rating_data_file), header = false, delim = "::",
                              quoted = false)
    @assert size(rating_df)[2] == 4

    rating_data = Dict()
    rating_data["user_movie"] = rating_df[!, 1:2] |> Matrix{Int}
    rating_data["rating"] = rating_df[!, 3] |> Vector{Float16}
    rating_data["timestamp"] = rating_df[!, 4] |> Vector{Int}
    return rating_data
end

function read_10m_movie_data(dir::String)
    movie_data_file = "movies.dat"
    movie_df = read_csv_asdf(joinpath(dir, movie_data_file), header = false, delim = "::",
                             quoted = false)
    movie_data = Dict()

    movie_ids = movie_df[!, 1]
    @assert minimum(movie_ids) == 1
    movie_data["num_movies"] = length(movie_ids)
    movie_titles = movie_df[!, 2]

    # bit of data cleaning
    movie_titles[9811] = "\"Great Performances\" Cats (1998)"
    genres = movie_df[!, 3]
    # children's and children are same
    genres = replace.(genres, "Children's" => "Children")
    genres = split.(genres, "|")
    movie_data["genres"] = genres |> Vector{Vector{String}}# user needs to do the one hot matrix using Flux.onehot

    movie_data["metadata"] = Dict("movie_id_to_title" => Dict(movie_ids .=> movie_df[!, 2]))
    return movie_data
end

function read_10m_user_tag_data(dir::String)
    tag_data_file = "tags.dat"
    tag_df = read_csv(joinpath(dir, tag_data_file), header = false, delim = "::",
                      quoted = false)

    tag_data = Dict{String, Any}()
    tag_data["user_movie"] = tag_df[!, 1:2] |> Matrix{Int}
    tag_data["tag_name"] = tag_df[!, 3] |> Vector{String}
    tag_data["timestamp"] = tag_df[!, 4] |> Vector
    return tag_data
end

function read_link_data(dir::String)::Dict
    link_csv = "links.csv"
    link_df = read_csv(joinpath(dir, link_csv))

    movieId = link_df[:, :movieId]
    imdbId = link_df[:, :imdbId]
    tmdbId = link_df[:, :imdbId]
    metadata = Dict()
    metadata["movieId_to_imdbID"] = Dict(movieId .=> imdbId)
    metadata["movieId_to_tmdbID"] = Dict(movieId .=> tmdbId)
    return Dict("metadata" => metadata)
end

function read_current_rating_data(dir::String)::Dict
    rating_csv = "ratings.csv"
    rating_df = read_csv(joinpath(dir, rating_csv))
    @assert size(rating_df)[2] == 4

    rating_data = Dict()
    rating_data["user_movie"] = rating_df[:, [:userId, :movieId]] |> Matrix{Int}
    rating_data["rating"] = rating_df[:, :rating] |> Vector{Float16}
    rating_data["timestamp"] = rating_df[:, :timestamp] |> Vector{Int}
    return rating_data
end

function read_genome_tag_data(dir::String)::Dict
    tag_csv = "genome-tags.csv"
    score_csv = "genome-scores.csv"
    # return empty dict if the genome files are not present
    isfile(joinpath(dir, tag_csv)) || return Dict()

    tag_df = read_csv(joinpath(dir, tag_csv))
    score_df = read_csv(joinpath(dir, score_csv))

    tag_data = Dict{String, Any}()
    tag_ids = tag_df[:, :tagId]
    @assert minimum(tag_ids) == 1
    tag_data["num_tags"] = maximum(tag_ids)
    tag_data["metadata"] = Dict("tag_id_to_name" => Dict(tag_ids .=>
                                                             tag_df[:, :tag] |> Vector))
    tag_data["movie_tag"] = score_df[:, [:movieId, :tagId]] |> Matrix{Int}
    tag_data["score"] = score_df[:, :relevance] |> Vector

    return tag_data
end

function read_current_user_tag_data(dir::String)::Dict
    # user tagged a movie to be a certain category
    tag_data_csv = "tags.csv"
    tag_df = read_csv(joinpath(dir, tag_data_csv))

    tag_data = Dict{String, Any}()
    tag_data["tag_name"] = tag_df[:, :tag] |> Vector{String}
    tag_data["user_movie"] = tag_df[:, [:userId, :movieId]] |> Matrix{Int}
    tag_data["timestamp"] = tag_df[:, :timestamp] |> Vector
    return tag_data
end

function read_current_movie_data(dir::String)::Dict
    movie_csv = "movies.csv"
    movie_df = read_csv(joinpath(dir, movie_csv))

    movie_data = Dict{String, Any}()
    movie_data["metadata"] = Dict{String, Vector{String}}()
    movie_ids = movie_df[:, :movieId] |> Vector{Int}
    @assert minimum(movie_ids) == 1
    # @assert maximum(movie_ids) == length(movie_ids)
    movie_data["num_movies"] = length(movie_ids)
    movie_data["metadata"] = Dict("movie_id_to_title" => Dict(movie_ids .=>
                                                                  movie_df[:, :title] |>
                                                                  Vector{String}))

    return movie_data
end

function read_1m_user_data(dir::String)::Dict
    user_data_file = "users.dat"
    user_df = read_csv_asdf(joinpath(dir, user_data_file), header = false, delim = "::")
    user_data = Dict()

    occupation_names = [
        "other/not specified",
        "academic/educator",
        "artist",
        "clerical/admin",
        "college/grad student",
        "customer service",
        "doctor/health care",
        "executive/managerial",
        "farmer",
        "homemaker",
        "K-12 student",
        "lawyer",
        "programmer",
        "retired",
        "sales/marketing",
        "scientist",
        "self-employed",
        "technician/engineer",
        "tradesman/craftsman",
        "unemployed",
        "writer",
    ]
    user_ids = user_df[!, 1]
    @assert minimum(user_ids) == 1
    @assert maximum(user_ids) == length(user_ids)
    user_data["num_users"] = maximum(user_ids)
    user_data["gender"] = user_df[!, 2] .== "M"
    user_data["age"] = user_df[!, 3] |> Vector{Int}
    ouccupation_ids = user_df[!, 4] .+ 1
    user_data["occupation"] = [occupation_names[i] for i in ouccupation_ids]
    user_data["zipcode"] = user_df[!, 5] |> Vector{String}
    return user_data
end

function read_1m_movie_data(dir::String)::Dict
    movie_data_file = "movies.dat"
    movie_df = read_csv_asdf(joinpath(dir, movie_data_file), header = false, delim = "::")
    movie_data = Dict()

    movie_ids = movie_df[!, 1]
    @assert minimum(movie_ids) == 1
    movie_data["num_movies"] = length(movie_ids)
    genres = movie_df[!, 3]
    # children's and children are same
    genres = replace.(genres, "Children's" => "Children")
    genres = split.(genres, "|")
    movie_data["genres"] = genres |> Vector{Vector{String}} # user needs to do the one hot matrix using Flux.onehot

    movie_data["metadata"] = Dict("movie_id_to_title" => Dict(movie_ids .=> movie_df[!, 2]))
    return movie_data
end

function read_1m_rating_data(dir::String)::Dict
    rating_data_file = "ratings.dat"

    rating_info = read_csv(joinpath(dir, rating_data_file), Matrix{Int}, header = false,
                           delim = "::")
    @assert size(rating_info)[2] == 4

    rating_data = Dict()
    rating_data["user_movie"] = rating_info[:, 1:2]
    rating_data["rating"] = rating_info[:, 3] |> Vector{Float16}
    rating_data["timestamp"] = rating_info[:, 4]
    return rating_data
end

function read_100k_user_data(dir::String)::Dict
    user_data_file = "u.user"
    user_data = Dict()
    user_df = read_csv_asdf(joinpath(dir, user_data_file), header = false)

    user_ids = user_df[!, 1]
    @assert minimum(user_ids) == 1
    @assert maximum(user_ids) == length(user_ids)
    user_data["num_users"] = maximum(user_ids)
    user_data["age"] = user_df[!, 2] |> Vector{Int}
    user_data["gender"] = user_df[!, 3] .== "M" # I hope I don't get cancelled for binarizing this field
    user_data["occupation"] = user_df[!, 4] |> Vector{String}
    user_data["zipcode"] = user_df[!, 5] |> Vector{String}
    return user_data
end

function read_100k_rating_data(dir::String)::Dict
    rating_data_file = "u.data"
    rating_info = read_csv(joinpath(dir, rating_data_file), Matrix{Int}, header = false)
    @assert size(rating_info)[2] == 4

    rating_data = Dict()
    rating_data["user_movie"] = rating_info[:, 1:2]
    rating_data["rating"] = rating_info[:, 3] |> Vector{Float16}
    rating_data["timestamp"] = rating_info[:, 4]
    return rating_data
end

function read_100k_movie_data(dir::String)::Dict
    movie_data_file = "u.item"
    movie_df = read_csv_asdf(joinpath(dir, movie_data_file), header = false,
                             dateformat = "dd-u-yyyy")
    movie_data = Dict()

    genre_labels = ["Unknown", "Action", "Adventure", "Animation", "Children", "Comedy",
        "Crime", "Documentary", "Drama", "Fantasy", "Film-Noir", "Horror", "Musical",
        "Mystery", "Romance", "Sci-Fi", "Thriller", "War", "Western"]
    movie_ids = movie_df[!, 1]
    @assert minimum(movie_ids) == 1
    @assert maximum(movie_ids) == length(movie_ids)
    movie_data["num_movies"] = maximum(movie_ids)
    movie_data["release_date"] = movie_df[!, 3]
    genres = movie_df[!, 6:end] |> Matrix{Bool}
    genres = permutedims(genres, (2, 1))
    movie_data["genres"] = genres

    movie_data["metadata"] = Dict()
    movie_data["metadata"]["movie_id_to_title"] = Dict(movie_ids .=> movie_df[!, 2])
    movie_data["metadata"]["genre_labels"] = genre_labels
    return movie_data
end

function generate_movielens_graph(user_data::Dict, movie_data::Dict,
                                  rating_data::Dict)::HeteroGraph
    num_nodes = Dict("user" => user_data["num_users"], "movie" => movie_data["num_movies"])
    node_data = Dict()
    node_data["user"] = Dict(Symbol(k) => maybesqueeze(v)
                             for (k, v) in user_data if k ∉ ["num_users", "metadata"])
    node_data["movie"] = Dict(Symbol(k) => maybesqueeze(v)
                              for (k, v) in movie_data if k ∉ ["num_movies", "metadata"])

    user_rates_movie = rating_data["user_movie"]
    user_ids, movie_ids = user_rates_movie[:, 1], user_rates_movie[:, 2]
    edge_indices = Dict(("user", "rating", "movie") => (user_ids, movie_ids))

    edge_data = Dict(("user", "rating", "movie") => Dict(Symbol(k) => maybesqueeze(v)
                                                         for (k, v) in rating_data
                                                         if k ∉ ["user_movie", "metadata"]))

    return HeteroGraph(; num_nodes, edge_indices, node_data, edge_data)
end

function generate_movielens_graph(movie_data::Dict, rating_data::Dict, user_tag_data::Dict,
                                  genome_tag_data::Dict, link_data::Dict)::HeteroGraph
    edge_indices = Dict()
    user_rates_movie = rating_data["user_movie"]
    user_ids, movie_ids = user_rates_movie[:, 1], user_rates_movie[:, 2]
    num_users = user_ids |> unique |> length # Calculate the number of users
    edge_indices[("user", "rating", "movie")] = (user_ids, movie_ids)

    user_tags_movie = user_tag_data["user_movie"]
    user_ids, movie_ids = user_tags_movie[:, 1], user_tags_movie[:, 2]
    num_users = max(num_users, user_ids |> unique |> length)
    edge_indices[("user", "tag", "movie")] = (user_ids, movie_ids)

    if !isempty(genome_tag_data)
        movie_score_tag = genome_tag_data["movie_tag"]
        movie_ids, tag_ids = movie_score_tag[:, 1], movie_score_tag[:, 1]
        edge_indices[("movie", "score", "tag")] = (movie_ids, tag_ids)
    end

    # ideally the HeteroGraph function should be able to compute the number of egdes,
    # but we know other 2 values, so it is ifefficient to compute other two again
    # num_nodes = Dict("user" => num_users, "tag" => genome_tag_data["num_tags"], "movie" => movie_data["num_movies"])
    num_nodes = Dict("user" => num_users, "tag" => length(user_tag_data["tag_name"]),
                     "movie" => movie_data["num_movies"])

    _edge_data = Dict()
    _edge_data[("user", "rating", "movie")] = Dict(Symbol(k) => maybesqueeze(v)
                                                   for (k, v) in rating_data
                                                   if k ∉ ["user_movie", "metadata"])
    _edge_data[("user", "tag", "movie")] = Dict(Symbol(k) => maybesqueeze(v)
                                                for (k, v) in user_tag_data
                                                if k ∉ ["user_movie", "metadata"])
    isempty(genome_tag_data) ||
        (_edge_data[("movie", "score", "tag")] = Dict(Symbol(k) => maybesqueeze(v)
                                                      for (k, v) in genome_tag_data
                                                      if k ∉ [
                                                             "movie_tag",
                                                             "metadata",
                                                             "num_tags",
                                                         ]))

    edge_data = Dict(k => v for (k, v) in _edge_data if !isempty(v))

    _node_data = Dict()
    _node_data["movie"] = Dict(Symbol(k) => maybesqueeze(v)
                               for (k, v) in movie_data if k ∉ ["num_movies", "metadata"])
    node_data = Dict(k => v for (k, v) in _node_data if !isempty(v))

    return HeteroGraph(; num_nodes, edge_indices, node_data, edge_data)
end

function get_movielens_metadata(data::Tuple)::Dict
    # the recieved data in generally user_data, movie_data and rating_data
    metadata = Dict()
    for d in data
        if haskey(d, "metadata")
            for (k, v) in d["metadata"]
                metadata[k] = v
            end
        end
    end
    return metadata
end

function movielens_datadir(name, dir = nothing)
    dir = isnothing(dir) ? datadep"MovieLens" : dir
    dname = "ml-" * name
    LINK = "https://files.grouplens.org/datasets/movielens/$dname.zip"
    d = joinpath(dir, dname)
    if !isdir(d)
        DataDeps.fetch_default(LINK, dir)
        currdir = pwd()
        cd(dir) # Needed since `unpack` extracts in working dir
        DataDeps.unpack(joinpath(dir, "$dname.zip"))
        # conditions when unzipped folder is our required data dir
        if name == "10m"
            unzipped = joinpath(dir, "ml-10M100K")
            mv(unzipped, d) # none of them are relative path
        end
        cd(currdir)
    end
    @assert isdir(d)
    return d
end

function Base.show(io::IO, ::MIME"text/plain", d::MovieLens)
    recur_io = IOContext(io, :compact => false)

    print(io, "MovieLens $(d.name):")  # if the type is parameterized don't print the parameters

    for f in fieldnames(MovieLens)
        if !startswith(string(f), "_") && f != :name
            fstring = leftalign(string(f), 10)
            print(recur_io, "\n  $fstring  =>    ")
            # show(recur_io, MIME"text/plain"(), getfield(d, f))
            # println(recur_io)
            print(recur_io, "$(_summary(getfield(d, f)))")
        end
    end
end

Base.length(data::MovieLens) = length(data.graphs)
function Base.getindex(data::MovieLens, ::Colon)
    length(data.graphs) == 1 ? data.graphs[1] : data.graphs
end
Base.getindex(data::MovieLens, i) = getobs(data.graphs, i)
[.\src\datasets\graphs\ogbdataset.jl]
function __init__ogbdataset()
    DEPNAME = "OGBDataset"
    LINK = "http://snap.stanford.edu/ogb/data"
    DOCS = "https://ogb.stanford.edu/docs/dataset_overview/"

    # NOT WORKING AS EXPECTED
    function fetch_method(remote_filepath, local_directory_path)
        if endswith(remote_filepath, "nodeproppred/master.csv")
            local_filepath = joinpath(local_directory_path, "nodeproppred_metadata.csv")
            download(remote_filepath, local_filepath)
        elseif endswith(remote_filepath, "linkproppred/master.csv")
            local_filepath = joinpath(local_directory_path, "linkproppred_metadata.csv")
            download(remote_filepath, local_filepath)
        elseif endswith(remote_filepath, "graphproppred/master.csv")
            local_filepath = joinpath(local_directory_path, "graphproppred_metadata.csv")
            download(remote_filepath, local_filepath)
        else
            local_filepath = DataDeps.fetch_default(remote_filepath, local_directory_path)
        end
        return local_filepath
    end

    register(DataDep(DEPNAME,
                     """
                     Dataset: The $DEPNAME dataset.
                     Website: $DOCS
                     Download Link: $LINK
                     """,
                     [
                         "https://raw.githubusercontent.com/snap-stanford/ogb/master/ogb/nodeproppred/master.csv",
                         "https://raw.githubusercontent.com/snap-stanford/ogb/master/ogb/linkproppred/master.csv",
                         "https://raw.githubusercontent.com/snap-stanford/ogb/master/ogb/graphproppred/master.csv",
                     ],
                     fetch_method = fetch_method))
end

"""
    OGBDataset(name; dir=nothing)

The collection of datasets from the [Open Graph Benchmark: Datasets for Machine Learning on Graphs](https://arxiv.org/abs/2005.00687)
paper.

`name` is the name  of one of the datasets (listed [here](https://ogb.stanford.edu/docs/dataset_overview/))
available for node prediction, edge prediction, or graph prediction tasks.

# Examples

## Node prediction tasks

```julia-repl
julia> data = OGBDataset("ogbn-arxiv")
OGBDataset ogbn-arxiv:
  metadata    =>    Dict{String, Any} with 17 entries
  graphs      =>    1-element Vector{MLDatasets.Graph}
  graph_data  =>    nothing

julia> data[:]
Graph:
  num_nodes   =>    169343
  num_edges   =>    1166243
  edge_index  =>    ("1166243-element Vector{Int64}", "1166243-element Vector{Int64}")
  node_data   =>    (val_mask = "29799-trues BitVector", test_mask = "48603-trues BitVector", year = "169343-element Vector{Int64}", features = "128×169343 Matrix{Float32}", label = "169343-element Vector{Int64}", train_mask = "90941-trues BitVector")
  edge_data   =>    nothing

julia> data.metadata
Dict{String, Any} with 17 entries:
  "download_name"         => "arxiv"
  "num classes"           => 40
  "num tasks"             => 1
  "binary"                => false
  "url"                   => "http://snap.stanford.edu/ogb/data/nodeproppred/arxiv.zip"
  "additional node files" => ["node_year"]
  "is hetero"             => false
  "task level"            => "node"
  ⋮                       => ⋮

julia> data = OGBDataset("ogbn-mag")
OGBDataset ogbn-mag:
  metadata    =>    Dict{String, Any} with 17 entries
  graphs      =>    1-element Vector{MLDatasets.HeteroGraph}
  graph_data  =>    nothing

julia> data[:]
Heterogeneous Graph:
  num_nodes     =>    Dict{String, Int64} with 4 entries
  num_edges     =>    Dict{Tuple{String, String, String}, Int64} with 4 entries
  edge_indices  =>    Dict{Tuple{String, String, String}, Tuple{Vector{Int64}, Vector{Int64}}} with 4 entries
  node_data     =>    (year = "Dict{String, Vector{Float32}} with 1 entry", features = "Dict{String, Matrix{Float32}} with 1 entry", label = "Dict{String, Vector{Int64}} with 1 entry")
  edge_data     =>    (reltype = "Dict{Tuple{String, String, String}, Vector{Float32}} with 4 entries",)
```

## Edge prediction task

```julia-repl
julia> data = OGBDataset("ogbl-collab")
OGBDataset ogbl-collab:
  metadata    =>    Dict{String, Any} with 15 entries
  graphs      =>    1-element Vector{MLDatasets.Graph}
  graph_data  =>    nothing

julia> data[:]
Graph:
  num_nodes   =>    235868
  num_edges   =>    2358104
  edge_index  =>    ("2358104-element Vector{Int64}", "2358104-element Vector{Int64}")
  node_data   =>    (features = "128×235868 Matrix{Float32}",)
  edge_data   =>    (year = "2×1179052 Matrix{Int64}", weight = "2×1179052 Matrix{Int64}")
```

## Graph prediction task

```julia-repl
julia> data = OGBDataset("ogbg-molhiv")
OGBDataset ogbg-molhiv:
  metadata    =>    Dict{String, Any} with 17 entries
  graphs      =>    41127-element Vector{MLDatasets.Graph}
  graph_data  =>    (labels = "41127-element Vector{Int64}", train_mask = "32901-trues BitVector", val_mask = "4113-trues BitVector", test_mask = "4113-trues BitVector")

julia> data[1]
(graphs = Graph(19, 40), labels = 0)
```
"""
struct OGBDataset{GD} <: AbstractDataset
    name::String
    metadata::Dict{String, Any}
    graphs::Vector{<:AbstractGraph}
    graph_data::GD
end

function OGBDataset(fullname; dir = nothing)
    metadata = read_ogb_metadata(fullname, dir)
    path = makedir_ogb(fullname, metadata["url"], dir)
    metadata["path"] = path
    if get(metadata, "is hetero", false)
        graph_dicts, graph_data = read_ogb_hetero_graph(path, metadata)
        graphs = ogbdict2heterograph.(graph_dicts)
    else
        graph_dicts, graph_data = read_ogb_graph(path, metadata)
        graphs = ogbdict2graph.(graph_dicts)
    end
    return OGBDataset(fullname, metadata, graphs, graph_data)
end

function read_ogb_metadata(fullname, dir = nothing)
    dir = isnothing(dir) ? datadep"OGBDataset" : dir
    @assert contains(fullname, "-") "The full name should be provided, e.g. ogbn-arxiv"
    prefix, name = split(fullname, "-")
    if prefix == "ogbn"
        path_metadata = joinpath(dir, "nodeproppred_metadata.csv")
    elseif prefix == "ogbl"
        path_metadata = joinpath(dir, "linkproppred_metadata.csv")
    elseif prefix == "ogbg"
        path_metadata = joinpath(dir, "graphproppred_metadata.csv")
    else
        @assert "The dataset name should start with ogbn, ogbl, or ogbg."
    end
    df = read_csv(path_metadata)
    @assert fullname ∈ names(df)
    metadata = Dict{String, Any}(String(r[1]) => r[2] isa AbstractString ?
                                                 parse_pystring(r[2]) : r[2]
                                 for r in eachrow(df[!, [names(df)[1], fullname]]))
    # edge cases for additional node and edge files
    for additional_key in ["additional edge files", "additional node files"]
        if !isnothing(metadata[additional_key])
            metadata[additional_key] = Vector{String}(split(metadata[additional_key], ","))
        else
            metadata[additional_key] = Vector{String}()
        end
    end
    if prefix == "ogbn"
        metadata["task level"] = "node"
    elseif prefix == "ogbl"
        metadata["task level"] = "link"
    elseif prefix == "ogbg"
        metadata["task level"] = "graph"
    end
    return metadata
end

function makedir_ogb(fullname, url, dir = nothing)
    root_dir = isnothing(dir) ? datadep"OGBDataset" : dir
    @assert contains(fullname, "-") "The full name should be provided, e.g. ogbn-arxiv"
    prefix, name = split(fullname, "-")
    data_dir = joinpath(root_dir, name)
    if !isdir(data_dir)
        local_filepath = DataDeps.fetch_default(url, root_dir)
        currdir = pwd()
        cd(root_dir) # Needed since `unpack` extracts in working dir
        DataDeps.unpack(local_filepath)
        unzipped = local_filepath[1:(findlast('.', local_filepath) - 1)]
        # conditions when unzipped folder is our required data dir
        (unzipped == data_dir) || mv(unzipped, data_dir) # none of them are relative path
        for (root, dirs, files) in walkdir(data_dir)
            for file in files
                if endswith(file, r"zip|gz")
                    cd(root)
                    DataDeps.unpack(joinpath(root, file))
                end
            end
        end
        cd(currdir)
    end
    @assert isdir(data_dir)

    return data_dir
end

## See https://github.com/snap-stanford/ogb/blob/master/ogb/io/read_graph_raw.py
function read_ogb_graph(path, metadata)
    dict = Dict{String, Any}()

    dict["edge_index"] = read_ogb_file(joinpath(path, "raw", "edge.csv"), Int,
                                       transp = false)
    dict["edge_index"] = dict["edge_index"] .+ 1 # from 0-indexing to 1-indexing

    dict["node_features"] = read_ogb_file(joinpath(path, "raw", "node-feat.csv"), Float32)
    dict["edge_features"] = read_ogb_file(joinpath(path, "raw", "edge-feat.csv"), Float32)

    dict["num_nodes"] = read_ogb_file(joinpath(path, "raw", "num-node-list.csv"), Int;
                                      tovec = true)
    dict["num_edges"] = read_ogb_file(joinpath(path, "raw", "num-edge-list.csv"), Int;
                                      tovec = true)

    # replace later with data from metadata?
    for file in readdir(joinpath(path, "raw"))
        if file ∉ [
            "edge.csv",
            "num-node-list.csv",
            "num-edge-list.csv",
            "node-feat.csv",
            "edge-feat.csv",
        ] && file[(end - 3):end] == ".csv"
            propname = replace(split(file, ".")[1], "-" => "_")
            dict[propname] = read_ogb_file(joinpath(path, "raw", file), Any)

            # from https://github.com/snap-stanford/ogb/blob/5e9d78e80ffd88787d9a1fdfdf4079f42d171565/ogb/io/read_graph_raw.py
            # # hack for ogbn-proteins
            # if additional_file == 'node_species' and osp.exists(osp.join(raw_dir, 'species.csv.gz')):
            #     os.rename(osp.join(raw_dir, 'species.csv.gz'), osp.join(raw_dir, 'node_species.csv.gz'))
        end
    end

    node_keys = [k for k in keys(dict) if startswith(k, "node_")]
    edge_keys = [k for k in keys(dict) if startswith(k, "edge_") && k != "edge_index"]
    # graph_keys = [k for k in keys(dict) if startswith(k, "graph_")] # no graph-level features in official implementation

    num_graphs = length(dict["num_nodes"])
    @assert num_graphs == length(dict["num_edges"])

    graphs = Dict[]
    num_node_accum = 0
    num_edge_accum = 0
    for i in 1:num_graphs
        graph = Dict{String, Any}()
        n, m = dict["num_nodes"][i], dict["num_edges"][i]
        graph["num_nodes"] = n
        graph["num_edges"] = metadata["add_inverse_edge"] ? 2 * m : m

        # EDGE FEATURES
        if metadata["add_inverse_edge"]
            ei = dict["edge_index"][(num_edge_accum + 1):(num_edge_accum + m), :]
            s, t = ei[:, 1], ei[:, 2]
            graph["edge_index"] = [s t; t s]

            for k in edge_keys
                v = dict[k]
                if v === nothing
                    graph[k] = nothing
                else
                    x = v[:, (num_edge_accum + 1):(num_edge_accum + m)]
                    graph[k] = [x x]
                end
            end
        else
            graph["edge_index"] = dict["edge_index"][(num_edge_accum + 1):(num_edge_accum + m),
                                                     :]

            for k in edge_keys
                v = dict[k]
                if v === nothing
                    graph[k] = nothing
                else
                    graph[k] = v[:, (num_edge_accum + 1):(num_edge_accum + m)]
                end
            end
        end
        num_edge_accum += m

        # NODE FEATURES
        for k in node_keys
            v = dict[k]
            if v === nothing
                graph[k] = nothing
            else
                graph[k] = v[:, (num_node_accum + 1):(num_node_accum + n)]
            end
        end
        num_node_accum += n

        push!(graphs, graph)
    end

    # PROCESS LABELS
    dlabels = Dict{String, Any}()
    for k in keys(dict)
        if contains(k, "label")
            if k ∉ [node_keys; edge_keys]
                dlabels[k] = dict[k]
            end
        end
    end
    labels = isempty(dlabels) ? nothing :
             length(dlabels) == 1 ? first(dlabels)[2] : dlabels

    splits = readdir(joinpath(path, "split"))
    @assert length(splits)==1 "Current implementation supports only 1 split"

    graph_data = nothing
    if metadata["task level"] in ["node", "graph"]
        split_idx = (train = read_ogb_file(joinpath(path, "split", splits[1], "train.csv"),
                                           Int; tovec = true),
                     val = read_ogb_file(joinpath(path, "split", splits[1], "valid.csv"),
                                         Int; tovec = true),
                     test = read_ogb_file(joinpath(path, "split", splits[1], "test.csv"),
                                          Int; tovec = true))

        if metadata["task level"] == "node"
            # During the time of writing this piece of code,
            # node level OGBDataset had only 1 graph.
            # We need to implement splits for multiple graphs if that changes
            @assert length(graphs) == 1
            g = graphs[1]

            # TODO: Implement for more than one split
            for key in keys(split_idx)
                if !isnothing(split_idx[key])
                    g["node_$(key)_mask"] = indexes2mask(split_idx[key] .+ 1,
                                                         g["num_nodes"])
                end
            end
        else
            split_mask = Dict()
            for key in keys(split_idx)
                if !isnothing(split_idx[key])
                    split_mask[Symbol("$(key)_mask")] = indexes2mask(split_idx[key] .+ 1,
                                                                     num_graphs)
                end
            end
            graph_data = clean_nt((; labels = maybesqueeze(labels), split_mask...))
        end
    elseif metadata["task level"] == "link"
        # During the time of writing this piece of code,
        # link level OGBDataset had only 1 graph.
        # We need to implement splits for multiple graphs if that changes
        @assert length(graphs) == 1
        g = graphs[1]

        split_dict = (train = read_pytorch(joinpath(path, "split", splits[1], "train.pt")),
                      val = read_pytorch(joinpath(path, "split", splits[1], "valid.pt")),
                      test = read_pytorch(joinpath(path, "split", splits[1], "test.pt")))

        for key in keys(split_dict)
            if !isnothing(split_dict[key])
                for k in keys(split_dict[key])
                    if k in ["edge", "edge_neg"]
                        split_dict[key][k] .+= 1
                        ei = split_dict[key][k]
                        s, t = ei[:, 1], ei[:, 2]
                        if metadata["add_inverse_edge"]
                            split_dict[key][k] = ([s; t], [t; s])
                        else
                            split_dict[key][k] = (s, t)
                        end
                    else
                        if metadata["add_inverse_edge"]
                            v = split_dict[key][k]
                            split_dict[key][k] = [v v]
                        end
                    end
                end
                g["edge_$(key)_dict"] = split_dict[key]
            end
        end
    end
    return graphs, graph_data
end

function read_ogb_hetero_graph(path, metadata)
    dict = Dict{String, Any}()
    num_node_df = read_csv_asdf(joinpath(path, "raw", "num-node-dict.csv"))
    dict["num_nodes"] = Dict(String(node) => Vector{Int}(num)
                             for (node, num) in pairs(eachcol(num_node_df)))
    node_types = sort(collect(keys(dict["num_nodes"])))
    num_graphs = length(dict["num_nodes"][node_types[1]])

    triplet_mat = read_ogb_file(joinpath(path, "raw", "triplet-type-list.csv"), String,
                                transp = false)
    @assert size(triplet_mat)[2] == 3
    triplets = sort([Tuple(triplet[:]) for triplet in eachrow(triplet_mat)])

    dict["edge_indices"] = Dict{Tuple{String, String, String}, Matrix{Int}}()
    dict["num_edges"] = Dict{Tuple{String, String, String}, Any}()
    dict["edge_features"] = Dict{Tuple{String, String, String}, Any}()

    for triplet in triplets
        subdir = joinpath(path, "raw", "relations", join(triplet, "___"))
        dict["edge_indices"][triplet] = read_ogb_file(joinpath(subdir, "edge.csv"), Int,
                                                      transp = false) .+ 1
        dict["num_edges"][triplet] = read_ogb_file(joinpath(subdir, "num-edge-list.csv"),
                                                   Int)
        dict["edge_features"][triplet] = read_ogb_file(joinpath(subdir, "edge-feat.csv"),
                                                       AbstractFloat)
    end

    # Check if the number of graphs are consistent accross node and edge data
    @assert length(dict["num_nodes"][node_types[1]]) ==
            length(dict["num_edges"][triplets[1]])

    dict["node_features"] = Dict{String, Any}()
    for node_type in node_types
        subdir = joinpath(path, "raw", "node-feat", node_type)
        dict["node_features"][node_type] = read_ogb_file(joinpath(subdir, "node-feat.csv"),
                                                         AbstractFloat)
    end

    for additional_file in metadata["additional node files"]
        node_add_feat = Dict()
        @assert additional_file[1:5] == "node_"

        for node_type in node_types
            subdir = joinpath(path, "raw", "node-feat", node_type)
            node_feat = read_ogb_file(joinpath(subdir, additional_file * ".csv"), Float32)
            isnothing(node_feat) ||
                @assert length(node_feat) == sum(dict["num_nodes"][node_type])
            node_add_feat[node_type] = node_feat
        end
        dict[additional_file] = node_add_feat
    end

    for additional_file in metadata["additional edge files"]
        edge_add_feats = Dict()
        @assert additional_file[1:5] == "edge_"

        for triplet in triplets
            subdir = joinpath(path, "raw", "relations", join(triplet, "___"))

            edge_feat = read_ogb_file(joinpath(subdir, additional_file * ".csv"),
                                      AbstractFloat)
            @assert length(edge_feat) == sum(dict["num_edges"][triplet])
            edge_add_feats[triplet] = edge_feat
        end
        dict[additional_file] = edge_add_feats
    end

    if metadata["task level"] == "node"
        dict["node_label"] = Dict()
        nodetype_has_label_df = read_csv_asdf(joinpath(path, "raw",
                                                       "nodetype-has-label.csv"))
        nodetype_has_label = Dict(String(node) => num[1]
                                  for (node, num) in pairs(eachcol(nodetype_has_label_df)))
        for (node_type, has_label) in nodetype_has_label
            @assert node_type ∈ node_types
            if has_label
                dict["node_label"][node_type] = read_ogb_file(joinpath(path, "raw",
                                                                       "node-label",
                                                                       node_type,
                                                                       "node-label.csv"),
                                                              Int)
            else
                dict["node_label"][node_type] = nothing
            end
        end
    end

    node_keys = [k for k in keys(dict) if startswith(k, "node_")]
    edge_keys = [k for k in keys(dict) if startswith(k, "edge_") && k != "edge_indices"]

    graphs = Dict[]
    num_node_accums = Dict(node_type => 0 for node_type in node_types)
    num_edge_accums = Dict(triplet => 0 for triplet in triplets)
    graph_data = nothing

    for i in 1:num_graphs
        graph = Dict{String, Any}()
        graph["num_nodes"] = Dict(k => v[i] for (k, v) in dict["num_nodes"])
        graph["edge_indices"] = Dict()
        for key in vcat(node_keys, edge_keys)
            graph[key] = Dict()
        end

        for triplet in triplets
            edge_indices = dict["edge_indices"][triplet]
            num_edge = dict["num_edges"][triplet][i]
            num_edge_accum = num_edge_accums[triplet]

            if metadata["add_inverse_edge"]
                edge_index = edge_indices[(num_edge_accum + 1):(num_edge_accum + num_edge),
                                          :]
                # Compensate for the duplicate/inverse the edges
                s, t = edge_index[:, 1], edge_index[:, 2]
                graph["edge_indices"][triplet] = [s t; t s]

                for k in edge_keys
                    v = dict[k][triplet]
                    if v === nothing
                        graph[k][triplet] = nothing
                    else
                        x = v[:, (num_edge_accum + 1):(num_edge_accum + num_edge)]
                        graph[k][triplet] = [x; x]
                    end
                end
            else
                graph["edge_indices"][triplet] = edge_indices[(num_edge_accum + 1):(num_edge_accum + num_edge),
                                                              :]
                for k in edge_keys
                    v = dict[k][triplet]
                    if v === nothing
                        graph[k][triplet] = nothing
                    else
                        graph[k][triplet] = v[:,
                                              (num_edge_accum + 1):(num_edge_accum + num_edge)]
                    end
                end
            end
            num_edge_accums[triplet] += num_edge
        end

        for node_type in node_types
            num_node = dict["num_nodes"][node_type][i]
            num_node_accum = num_node_accums[node_type]
            for k in node_keys
                v = dict[k][node_type]
                if v === nothing
                    graph[k][node_type] = nothing
                else
                    graph[k][node_type] = v[:,
                                            (num_node_accum + 1):(num_node_accum + num_node)]
                end
            end
            num_node_accums[node_type] += num_node
        end

        push!(graphs, graph)
    end

    dlabels = Dict{String, Any}()
    for k in keys(dict)
        if contains(k, "label")
            if k ∉ [node_keys; edge_keys]
                dlabels[k] = dict[k]
            end
        end
    end
    labels = isempty(dlabels) ? nothing :
             length(dlabels) == 1 ? first(dlabels)[2] : dlabels

    # Similar to OGB Graphs
    # Also see split implementation for normal ogb graphs
    # for any possible issues
    splits = readdir(joinpath(path, "split"))
    @assert length(splits)==1 "Current implementation supports only 1 split"

    graph_data = nothing
    split_dir = joinpath(path, "split", splits[1])
    if metadata["task level"] == "node"
        @assert length(graphs) == 1
        g = graphs[1]
        split_idx_dict = Dict{String, Dict{String, Vector{Int}}}()
        split_idx_dict["train"] = Dict()
        split_idx_dict["test"] = Dict()
        split_idx_dict["val"] = Dict()

        nodetype_has_label_df = read_csv_asdf(joinpath(path, "raw",
                                                       "nodetype-has-label.csv"))
        nodetype_has_label = Dict(String(node) => num[1]
                                  for (node, num) in pairs(eachcol(nodetype_has_label_df)))
        for (node_type, has_label) in nodetype_has_label
            @assert node_type ∈ node_types
            if has_label
                split_idx_dict["train"][node_type] = read_ogb_file(joinpath(split_dir,
                                                                            node_type,
                                                                            "train.csv"),
                                                                   Int; tovec = true)
                split_idx_dict["test"][node_type] = read_ogb_file(joinpath(split_dir,
                                                                           node_type,
                                                                           "test.csv"), Int;
                                                                  tovec = true)
                split_idx_dict["val"][node_type] = read_ogb_file(joinpath(split_dir,
                                                                          node_type,
                                                                          "valid.csv"), Int;
                                                                 tovec = true)
            end
        end

        for key in keys(split_idx_dict)
            g["node_$(key)_mask"] = Dict()
            for node_type in keys(split_idx_dict[key])
                num_nodes = dict["num_nodes"][node_type][1]
                g["node_$(key)_mask"][node_type] = indexes2mask(split_idx_dict[key][node_type] .+
                                                                1, num_nodes)
            end
        end
    elseif metadata["task level"] == "graph"
        split_mask = Dict()
        for key in keys(split_idx)
            if !isnothing(split_idx[key])
                split_mask[Symbol("$(key)_mask")] = indexes2mask(split_idx[key] .+ 1,
                                                                 num_graphs)
            end
        end
        graph_data = clean_nt((; labels = maybesqueeze(labels), split_mask...))
    elseif metadata["task level"] == "link"
        @assert length(graphs) == 1
        @warn "Link split for HeteroData has not been implemented yet."

        # g = graphs[1]

        # split_dict = (train = read_pytorch(joinpath(split_dir, "train.pt")),
        #               val = read_pytorch(joinpath(split_dir, "valid.pt")),
        #               test = read_pytorch(joinpath(split_dir, "test.pt")))

    end
    return graphs, graph_data
end

function read_ogb_file(p, T; tovec = false, transp = true)
    res = isfile(p) ? read_csv(p, Matrix{T}, header = false) : nothing
    if tovec && res !== nothing
        @assert size(res, 1) == 1 || size(res, 2) == 1
        res = vec(res)
    end
    if transp && res !== nothing && !tovec
        res = collect(res')
    end
    if res !== nothing && T === Any
        res = restrict_array_type(res)
    end
    return res
end

function ogbdict2graph(d::Dict)
    edge_index = d["edge_index"][:, 1], d["edge_index"][:, 2]
    num_nodes = d["num_nodes"]
    node_data = Dict(Symbol(k[6:end]) => maybesqueeze(v)
                     for (k, v) in d if startswith(k, "node_") && v !== nothing)
    edge_data = Dict(Symbol(k[6:end]) => maybesqueeze(v)
                     for (k, v) in d
                     if startswith(k, "edge_") && k != "edge_index" && v !== nothing)
    node_data = isempty(node_data) ? nothing : (; node_data...)
    edge_data = isempty(edge_data) ? nothing : (; edge_data...)
    return Graph(; num_nodes, edge_index, node_data, edge_data)
end

function ogbdict2heterograph(d::Dict)
    num_nodes = d["num_nodes"]
    edge_indices = Dict(triplet => (ei[:, 1], ei[:, 2])
                        for (triplet, ei) in d["edge_indices"])

    edge_data = Dict{Tuple{String, String, String}, Dict}(k => Dict{Symbol, Any}()
                                                          for k in keys(edge_indices))
    for (feature_name, v) in d
        # v is a dict
        # the number of nothing values should not be equal to total number of values
        if startswith(feature_name, "edge_") && feature_name != "edge_indices" &&
           sum(isnothing.(values(v))) < length(v)
            for (edge_key, edge_value) in v
                if !isnothing(edge_value)
                    edge_data[edge_key][Symbol(feature_name[6:end])] = maybesqueeze(edge_value)
                end
            end
        end
    end

    node_data = Dict(k => Dict{Symbol, Any}() for k in keys(num_nodes))
    for (feature_name, v) in d
        if startswith(feature_name, "node_") && sum(isnothing.(values(v))) < length(v)
            for (node_key, node_value) in v
                if !isnothing(node_value)
                    node_data[node_key][Symbol(feature_name[6:end])] = maybesqueeze(node_value)
                end
            end
        end
    end
    node_data = Dict(k => v for (k, v) in node_data if !isempty(v))
    edge_data = Dict(k => v for (k, v) in edge_data if !isempty(v))
    # node_data = isempty(node_data) ? nothing : (; node_data...)
    # edge_data = map(x -> isempty(x) ? nothing : (; x...), edge_data)

    return HeteroGraph(; num_nodes, edge_indices, edge_data, node_data)
end

Base.length(data::OGBDataset) = length(data.graphs)
function Base.getindex(data::OGBDataset{Nothing}, ::Colon)
    length(data.graphs) == 1 ? data.graphs[1] : data.graphs
end
Base.getindex(data::OGBDataset, ::Colon) = (; data.graphs, data.graph_data.labels)
Base.getindex(data::OGBDataset{Nothing}, i) = getobs(data.graphs, i)
Base.getindex(data::OGBDataset, i) = getobs((; data.graphs, data.graph_data.labels), i)

# dataset OGBDaataset looks odd
function Base.show(io::IO, ::MIME"text/plain", d::OGBDataset)
    recur_io = IOContext(io, :compact => false)

    print(io, "OGBDataset $(d.name):")  # if the type is parameterized don't print the parameters

    for f in fieldnames(OGBDataset)
        if !startswith(string(f), "_") && f != :name
            fstring = leftalign(string(f), 10)
            print(recur_io, "\n  $fstring  =>    ")
            # show(recur_io, MIME"text/plain"(), getfield(d, f))
            # println(recur_io)
            print(recur_io, "$(_summary(getfield(d, f)))")
        end
    end
end

[.\src\datasets\graphs\organicmaterialsdb.jl]
function __init__omdb()
    DEPNAME = "OrganicMaterialsDB"
    LINK = "https://omdb.mathub.io/dataset"
    FILE = "https://omdb.mathub.io/dataset/download/gap1_v1.1"

    register(DataDep(DEPNAME,
                     """
                     Dataset : The Organic Materials Database (OMDB)
                     Website : $LINK
                     """,
                     []
                     # post_fetch_method = unpack
                     ))
end

"""
    OrganicMaterialsDB(; split=:train, dir=nothing)

The OMDB-GAP1 v1.1 dataset from the Organic Materials Database (OMDB) of bulk organic crystals.

The dataset has to be manually downloaded from https://omdb.mathub.io/dataset, 
then unzipped and  its file content placed in the `OrganicMaterialsDB` folder.

The dataset contains the following files:

| Filename       | Description                                                                                              |
|----------------|----------------------------------------------------------------------------------------------------------|
| structures.xyz | 12500 crystal structures. Use the first 10000 as training examples and the remaining 2500 as test set.   |
| bandgaps.csv   | 12500 DFT band gaps corresponding to structures.xyz                                                      |
| CODids.csv     | 12500 COD ids cross referencing the Crystallographic Open Database (in the same order as structures.xyz) |

Please cite the paper introducing this dataset: https://arxiv.org/abs/1810.12814
"""
struct OrganicMaterialsDB <: AbstractDataset
    metadata::Dict{String, Any}
    graphs::Vector{Graph}
    graph_data::NamedTuple
end

function OrganicMaterialsDB(; split = :train, dir = nothing)
    @assert split in [:train, :test]
    procfiles = process_data_if_needed(OrganicMaterialsDB; dir)
    if split == :train
        return FileIO.load(procfiles[1], "dataset")
    else
        return FileIO.load(procfiles[2], "dataset")
    end
end

processed_files(::Type{<:OrganicMaterialsDB}) = ["train.jld2", "test.jld2"]
raw_files(::Type{<:OrganicMaterialsDB}) = ["structures.xyz", "bandgaps.csv"]

function process_data_if_needed(::Type{<:OrganicMaterialsDB}; dir = nothing)
    DEPNAME = "OrganicMaterialsDB"
    ddir = datadir(DEPNAME, dir)

    if !(isdir(joinpath(ddir, "processed")))
        mkpath(joinpath(ddir, "processed"))
    end

    procfiles = [joinpath(ddir, "processed", f)
                 for f in processed_files(OrganicMaterialsDB)]
    rawfiles = [joinpath(ddir, f) for f in raw_files(OrganicMaterialsDB)]

    for f in rawfiles
        if !isfile(f)
            error("Please download the OMDB dataset from https://omdb.mathub.io/dataset, 
            then unzip it and place its content in the $ddir folder.")
        end
    end

    for f in procfiles
        if !isfile(f)
            process_data(OrganicMaterialsDB, rawfiles, procfiles)
        end
    end
    return procfiles
end

function process_data(::Type{<:OrganicMaterialsDB}, rawfiles, procfiles)
    structures = read_chemfile(rawfiles[1])
    bandgaps = read_csv(rawfiles[2], Matrix) |> vec
    graphs = Graph[]
    for frame in structures
        pos = Float32.(Chemfiles.positions(frame))
        z = Int.(Chemfiles.atomic_number.(frame))
        g = Graph(num_nodes = length(z), node_data = (; pos, z))
        push!(graphs, g)
    end

    train_graphs = graphs[1:10000]
    train_bandgaps = bandgaps[1:10000]
    train_metadata = Dict{String, Any}("split" => :train,
                                       "size" => length(graphs))
    train_dataset = OrganicMaterialsDB(train_metadata, train_graphs,
                                       (; bandgaps = train_bandgaps))
    FileIO.save(procfiles[1], Dict("dataset" => train_dataset))

    test_graphs = graphs[10001:end]
    test_bandgaps = bandgaps[10001:end]
    test_metadata = Dict{String, Any}("split" => :test,
                                      "size" => length(graphs))
    test_dataset = OrganicMaterialsDB(test_metadata, test_graphs,
                                      (; bandgaps = test_bandgaps))
    FileIO.save(procfiles[2], Dict("dataset" => test_dataset))
end

Base.length(data::OrganicMaterialsDB) = length(data.graphs)

function Base.getindex(data::OrganicMaterialsDB, ::Colon)
    return (; data.graphs, data.graph_data.bandgaps)
end

function Base.getindex(data::OrganicMaterialsDB, i)
    return getobs((; data.graphs, data.graph_data...), i)
end

[.\src\datasets\graphs\pemsbay.jl]
function __init__pemsbay()
    DEPNAME = "PEMSBAY"
    LINK = "http://www-sop.inria.fr/members/Aurora.Rossi/index.html"
    register(ManualDataDep(DEPNAME,
                           """
                           Dataset: $DEPNAME
                           Website : $LINK
                           """))
end

"""
    PEMSBAY(; num_timesteps_in::Int = 12, num_timesteps_out::Int=12, dir=nothing, normalize = true)

The PEMS-BAY dataset described in the [Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting](https://arxiv.org/abs/1707.01926) paper.
It is collected by California Transportation Agencies (Cal-
Trans) Performance Measurement System (PeMS).

`PEMSBAY` is a graph with 325 nodes representing traffic sensors in the Bay Area. 

The edge weights `w` are contained as a feature array in `edge_data` and represent the distance between the sensors. 

The node features are the traffic speed and the time of the measurements collected by the sensors, divided into `num_timesteps_in` time steps. 

The target values are the traffic speed of the measurements collected by the sensors, divided into `num_timesteps_out` time steps.

The `normalize` flag indicates whether the data are normalized using Z-score normalization.
"""
struct PEMSBAY <: AbstractDataset
    graphs::Vector{Graph}
end

function PEMSBAY(;num_timesteps::Int = 12, dir = nothing, normalize = true)
    s, t, w, x, y = processed_traffic("PEMSBAY", num_timesteps, dir, normalize)

    g = Graph(; num_nodes = 325,
              edge_index = (s, t),
              edge_data = w,
            node_data = (features = x, targets = y))
            
    return PEMSBAY([g])
end

Base.length(d::PEMSBAY) = length(d.graphs)
Base.getindex(d::PEMSBAY, ::Colon) = d.graphs[1]
Base.getindex(d::PEMSBAY, i) = getindex(d.graphs, i)

[.\src\datasets\graphs\planetoid.jl]
"""
Read any of the citation network datasets “Cora”, “CiteSeer” and “PubMed” 
from the “Revisiting Semi-Supervised Learning with Graph Embeddings” paper. 
Nodes represent documents and edges represent citation links. 

Data collected from 
https://github.com/kimiyoung/planetoid/raw/master/data
"""
function read_planetoid_data(DEPNAME; dir = nothing, reverse_edges = true)
    name = lowercase(DEPNAME)

    x = read_planetoid_file(DEPNAME, "ind.$(name).x", dir)
    y = read_planetoid_file(DEPNAME, "ind.$(name).y", dir)
    allx = read_planetoid_file(DEPNAME, "ind.$(name).allx", dir)
    ally = read_planetoid_file(DEPNAME, "ind.$(name).ally", dir)
    tx = read_planetoid_file(DEPNAME, "ind.$(name).tx", dir)
    ty = read_planetoid_file(DEPNAME, "ind.$(name).ty", dir)
    graph = read_planetoid_file(DEPNAME, "ind.$(name).graph", dir)
    test_indices = read_planetoid_file(DEPNAME, "ind.$(name).test.index", dir)

    ntrain = size(x, 2)
    train_indices = 1:ntrain
    val_indices = (ntrain + 1):(ntrain + 500)
    sorted_test_index = sort(test_indices)

    if name == "citeseer"
        # There are some isolated nodes in the Citeseer graph, resulting in
        # not consecutive test indices. We need to identify them and add them
        # as zero vectors to `tx` and `ty`.
        len_test_indices = (maximum(test_indices) - minimum(test_indices)) + 1

        tx_ext = zeros(size(tx, 1), len_test_indices)
        tx_ext[:, sorted_test_index .- minimum(test_indices) .+ 1] .= tx
        ty_ext = zeros(len_test_indices)
        ty_ext[sorted_test_index .- minimum(test_indices) .+ 1] = ty

        tx, ty = tx_ext, ty_ext
    end
    x = hcat(allx, tx)
    y = vcat(ally, ty)
    x[:, test_indices] = x[:, sorted_test_index]
    y[test_indices] = y[sorted_test_index]
    test_indices = (size(allx, 2) + 1):size(x, 2)
    num_nodes = size(x, 2)

    adj_list = [Int[] for i in 1:num_nodes]
    for (i, neigs) in pairs(graph) # graph is dictionay representing the adjacency list
        neigs = unique(neigs) # remove duplicated edges
        neigs = filter(x -> x != i, neigs)# remove self-loops
        append!(adj_list[i + 1], neigs .+ 1) # convert to 1-indexed
    end
    if reverse_edges
        for (i, neigs) in enumerate(adj_list)
            for j in neigs
                i ∉ adj_list[j] && push!(adj_list[j], i)
            end
        end
    end

    node_data = (features = x, targets = y,
                 train_indices,
                 val_indices,
                 test_indices)

    node_data = (features = x, targets = y,
                 train_mask = indexes2mask(train_indices, num_nodes),
                 val_mask = indexes2mask(val_indices, num_nodes),
                 test_mask = indexes2mask(test_indices, num_nodes))

    metadata = Dict{String, Any}("name" => name,
                                 "num_classes" => length(unique(y)),
                                 "classes" => sort(unique(y)))

    edge_index = adjlist2edgeindex(adj_list)

    g = Graph(; num_nodes,
              edge_index,
              node_data)

    return metadata, g
end

function read_planetoid_file(DEPNAME, name, dir)
    filename = datafile(DEPNAME, name, dir)
    if endswith(name, "test.index")
        out = 1 .+ vec(readdlm(filename, Int))
    else
        out = read_pickle(filename)
        if out isa SparseMatrixCSC
            out = Matrix(out)
        end
        if out isa Matrix
            out = collect(out')
        end
    end
    if endswith(name, "y")
        out = map(y -> y[1], argmax(out, dims = 1)) |> vec
    end
    return out
end

[.\src\datasets\graphs\polblogs.jl]
function __init__polblogs()
    LINK = "https://netset.telecom-paris.fr/datasets/polblogs.tar.gz"
    DEPNAME = "PolBlogs"

    register(DataDep(DEPNAME,
                     """
                     Dataset : The $DEPNAME dataset
                     Website : $LINK
                     """,
                     LINK,
                     post_fetch_method = unpack))
end

"""
    PolBlogs(; dir=nothing)
 
The Political Blogs dataset from the [The Political Blogosphere and
the 2004 US Election: Divided they Blog](https://dl.acm.org/doi/10.1145/1134271.1134277) paper.

`PolBlogs` is a graph with 1,490 vertices (representing political blogs) and 19,025 edges (links between blogs).

The links are automatically extracted from a crawl of the front page of the blog. 

Each vertex receives a label indicating the political leaning of the blog: liberal or conservative.
"""
struct PolBlogs <: AbstractDataset
    metadata::Dict{String, Any}
    graphs::Vector{Graph}
end

function PolBlogs(; dir = nothing)
    DEPNAME = "PolBlogs"
    DATA = ["adjacency.tsv", "labels.tsv", "names.tsv"]

    path = datafile(DEPNAME, DATA[1], dir)
    adj = 1 .+ Matrix{Int64}(readdlm(path, '\t')[:, 1:2])
    s, t = adj[:, 1], adj[:, 2]

    path = datafile(DEPNAME, DATA[2], dir)
    labels = Matrix{Int}(readdlm(path, '\t')) |> vec

    path = datafile(DEPNAME, DATA[3], dir)
    names = Matrix{String}(readdlm(path, '\t')) |> vec

    metadata = Dict{String, Any}()
    g = Graph(; num_nodes = 1490,
              edge_index = (s, t),
              node_data = (; labels, names))

    return PolBlogs(metadata, [g])
end

Base.length(d::PolBlogs) = length(d.graphs)
Base.getindex(d::PolBlogs, ::Colon) = d.graphs[1]
Base.getindex(d::PolBlogs, i) = getindex(d.graphs, i)

[.\src\datasets\graphs\pubmed.jl]
function __init__pubmed()
    DEPNAME = "PubMed"
    LINK = "https://github.com/kimiyoung/planetoid/raw/master/data"
    DOCS = "https://github.com/kimiyoung/planetoid"
    DATA = "ind.pubmed." .* ["x", "y", "tx", "allx", "ty", "ally", "graph", "test.index"]

    register(DataDep(DEPNAME,
                     """
                     Dataset: The $DEPNAME dataset.
                     Website: $DOCS
                     """,
                     map(x -> "$LINK/$x", DATA),
                     "0b8bf8e80564611b540655e9cbb8c5900dd3728d4ababe0b990b6f27144bd76c"  # if checksum omitted, will be generated by DataDeps
                     # post_fetch_method = unpack
                     ))
end

"""
    PubMed(; dir=nothing, reverse_edges=true)

The PubMed citation network dataset from Ref. [1].
Nodes represent documents and edges represent citation links.
The dataset is designed for the node classification task. 
The task is to predict the category of certain paper.
The dataset is retrieved from Ref. [2].

# References

[1]: [Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking](https://arxiv.org/abs/1707.03815)

[2]: [Planetoid](https://github.com/kimiyoung/planetoid)
"""
struct PubMed <: AbstractDataset
    metadata::Dict{String, Any}
    graphs::Vector{Graph}
end

function PubMed(; dir = nothing, reverse_edges = true)
    metadata, g = read_planetoid_data("PubMed", dir = dir, reverse_edges = reverse_edges)
    return PubMed(metadata, [g])
end

Base.length(d::PubMed) = length(d.graphs)
Base.getindex(d::PubMed, ::Colon) = d.graphs[1]
Base.getindex(d::PubMed, i) = getindex(d.graphs, i)

# DEPRECATED in v0.6.0
function Base.getproperty(::Type{PubMed}, s::Symbol)
    if s === :dataset
        @warn "PubMed.dataset() is deprecated, use `PubMed()` instead."
        function dataset(; dir = nothing)
            d = PubMed(; dir)
            g = d[1]
            adjacency_list = edgeindex2adjlist(g.edge_index..., g.num_nodes)
            return (; g.num_nodes, g.num_edges,
                    node_features = g.node_data.features,
                    node_labels = g.node_data.targets,
                    train_indices = mask2indexes(g.node_data.train_mask),
                    val_indices = mask2indexes(g.node_data.val_mask),
                    test_indices = mask2indexes(g.node_data.test_mask),
                    adjacency_list)
        end
        return dataset
    else
        return getfield(PubMed, s)
    end
end

[.\src\datasets\graphs\reddit.jl]
function __init__reddit()
    DEPNAME = "Reddit"
    LINK = "http://snap.stanford.edu/graphsage/reddit.zip"
    DOCS = "http://snap.stanford.edu/graphsage/"

    register(DataDep(DEPNAME,
                     """
                     Dataset: The $DEPNAME Dataset
                     Website: $DOCS
                     Download Link: $LINK
                     """,
                     LINK,
                     "25337a21540cd373e4cee3751e6600324ab6a7377ef3966bb49f57412a17ed02",
                     post_fetch_method = unpack))
end

"""
    Reddit(; full=true, dir=nothing)

The Reddit dataset was introduced in Ref [1].
It is a graph dataset of Reddit posts made in the month of September, 2014.
The dataset contains a single post-to-post graph, connecting posts if the same user comments on both. 
The node label in this case is one of the 41 communities, or “subreddit”s, that a post belongs to.  
This dataset contains 232,965 posts.
The first 20 days are used for training and the remaining days for testing (with 30% used for validation).
Each node is represented by a 602 word vector.

Use `full=false` to load only a subsample of the dataset.


# References
[1]: [Inductive Representation Learning on Large Graphs](https://arxiv.org/abs/1706.02216)

[2]: [Benchmarks on the Reddit Dataset](https://paperswithcode.com/dataset/reddit)
"""
struct Reddit <: AbstractDataset
    metadata::Dict{String, Any}
    graphs::Vector{Graph}
end

function Reddit(; full = true, dir = nothing)
    DATAFILES = [
        "reddit-G.json", "reddit-G_full.json", "reddit-adjlist.txt",
        "reddit-class_map.json", "reddit-feats.npy", "reddit-id_map.json",
    ]
    DATA = joinpath.("reddit", DATAFILES)
    DEPNAME = "Reddit"

    if full
        graph_json = datafile(DEPNAME, DATA[2], dir)
    else
        graph_json = datafile(DEPNAME, DATA[1], dir)
    end

    class_map_json = datafile(DEPNAME, DATA[4], dir)
    id_map_json = datafile(DEPNAME, DATA[6], dir)
    feat_path = datafile(DEPNAME, DATA[5], dir)

    # Read the json files
    graph = read_json(graph_json)
    class_map = read_json(class_map_json)
    id_map = read_json(id_map_json)

    # Metadata
    directed = graph["directed"]
    multigraph = graph["multigraph"]
    links = graph["links"]
    nodes = graph["nodes"]
    num_edges = directed ? length(links) : length(links) * 2
    num_nodes = length(nodes)
    @assert length(graph["graph"]) == 0 # should be zero

    # edges
    s = get.(links, "source", nothing) .+ 1
    t = get.(links, "target", nothing) .+ 1
    if !directed
        s, t = [s; t], [t; s]
    end

    # labels
    node_keys = get.(nodes, "id", nothing)
    node_idx = [id_map[key] for key in node_keys] .+ 1

    sort_order = sortperm(node_idx)
    node_idx = node_idx[sort_order]
    labels = [class_map[key] for key in node_keys][sort_order]
    @assert length(node_idx) == length(labels)

    # features
    features = read_npz(feat_path)[node_idx, :]
    features = permutedims(features, (2, 1))

    # split
    test_mask = get.(nodes, "test", nothing)[sort_order]
    val_mask = get.(nodes, "val", nothing)[sort_order]
    # A node should not be both test and validation
    @assert sum(val_mask .& test_mask) == 0
    train_mask = nor.(test_mask, val_mask)

    metadata = Dict{String, Any}("directed" => directed, "multigraph" => multigraph,
                                 "num_edges" => num_edges, "num_nodes" => num_nodes)
    g = Graph(; num_nodes,
              edge_index = (s, t),
              node_data = (; labels, features, train_mask, val_mask, test_mask))
    return Reddit(metadata, [g])
end

Base.length(d::Reddit) = length(d.graphs)
Base.getindex(d::Reddit, ::Colon) = d.graphs
Base.getindex(d::Reddit, i) = getindex(d.graphs, i)

[.\src\datasets\graphs\temporalbrains.jl]
function __init__temporalbrains()
    DEPNAME = "TemporalBrains"
    LINK = "http://www-sop.inria.fr/members/Aurora.Rossi/index.html"
    register(ManualDataDep(DEPNAME,
                           """
                           Dataset: $DEPNAME
                           Website : $LINK
                           """))
end


function tb_datadir(dir = nothing)
    dir = isnothing(dir) ? datadep"TemporalBrains" : dir
    LINK = "http://www-sop.inria.fr/members/Aurora.Rossi/data/LabelledTBN.zip"
    if length(readdir((dir))) == 0
        DataDeps.fetch_default(LINK, dir)
        currdir = pwd()
        cd(dir) # Needed since `unpack` extracts in working dir
        DataDeps.unpack(joinpath(dir, "LabelledTBN.zip"))
        # conditions when unzipped folder is our required data dir
        cd(currdir)
    end
    @assert isdir(dir)
    return dir
end


function create_tbdataset(dir, thre)
    name_filelabels = joinpath(dir, "LabelledTBN", "labels.txt")
    filelabels = open(name_filelabels, "r")
    temporalgraphs = Vector{MLDatasets.TemporalSnapshotsGraph}(undef, 1000)

    for (i,line) in enumerate(eachline(filelabels))
        id, gender, age = split(line)
        name_network_file = joinpath(dir, "LabelledTBN", "networks", id * "_ws60_wo30_tuk0_pearson_schaefer_100.txt")
        
        data = readdlm(name_network_file,',',Float32; skipstart = 1) 
    
        data_thre = view(data,view(data,:,4) .> thre,:)
        data_thre_int = Int.(view(data_thre,:,1:3))

        activation = [zeros(Float32, 102) for _ in 1:27]
        for t in 1:27
             for n in 1:102
                rows = ((view(data_thre_int,:,1).==n) .& (view(data_thre_int,:,3).==t))
                activation[t][n] = mean(view(data_thre,rows,4))
             end
        end

        temporalgraphs[i] = TemporalSnapshotsGraph(num_nodes=ones(Int,27)*102, edge_index = (data_thre_int[:,1], data_thre_int[:,2], data_thre_int[:,3]), node_data= activation, graph_data= (g = gender, a = age))
    end
    return temporalgraphs
end

"""
    TemporalBrains(; dir = nothing, threshold_value = 0.6)

The TemporalBrains dataset contains a collection of temporal brain networks (as `TemporalSnapshotsGraph`s) of 1000 subjects obtained from resting-state fMRI data from the [Human Connectome Project (HCP)](https://www.humanconnectome.org/study/hcp-young-adult/document/extensively-processed-fmri-data-documentation).

The number of nodes is fixed for each of the 27 snapshots at 102, while the edges change over time.
    
For each `Graph` snapshot, the feature of a node represents the average activation of the node during that snapshot and it is contained in `Graphs.node_data`.

Each `TemporalSnapshotsGraph` has a label representing their gender ("M" for male and "F" for female) and age range (22-25, 26-30, 31-35 and 36+) contained as a named tuple in `graph_data`.

The `threshold_value` is used to binarize the edge weights and is set to 0.6 by default.
"""
struct TemporalBrains <: AbstractDataset
    graphs::Vector{MLDatasets.TemporalSnapshotsGraph}
end

function TemporalBrains(;threshold_value = 0.6, dir=nothing)
    create_default_dir("TemporalBrains")
    dir = tb_datadir(dir)
    graphs = create_tbdataset(dir, threshold_value)
    return TemporalBrains(graphs)
end

Base.length(d::TemporalBrains) = length(d.graphs)
Base.getindex(d::TemporalBrains, ::Colon) = d.graphs[1]
Base.getindex(d::TemporalBrains, i) = getindex(d.graphs, i)

[.\src\datasets\graphs\traffic.jl]
function traffic_datadir(dname ::String, dir = nothing)
    if dname == "PEMSBAY"
        dir = isnothing(dir) ? datadep"PEMSBAY" : dir
    elseif dname == "METRLA"
        dir = isnothing(dir) ? datadep"METRLA" : dir
    end
    LINK = "http://www-sop.inria.fr/members/Aurora.Rossi/data/$dname.zip"
    if length(readdir((dir))) == 0
        DataDeps.fetch_default(LINK, dir)
        currdir = pwd()
        cd(dir) # Needed since `unpack` extracts in working dir
        DataDeps.unpack(joinpath(dir, "$dname.zip"))
        # conditions when unzipped folder is our required data dir
        cd(currdir)
    end
    @assert isdir(dir)
    return dir
end

function read_traffic(d::String, dname::String)
    adj_matrix = load(joinpath(d, "$(dname).jld2"), "adj_matrix")
    node_features = load(joinpath(d, "$(dname).jld2"), "node_features")
    node_features = permutedims(node_features,(2,3,1))
    return adj_matrix, node_features
end
    
function traffic_generate_task(node_values::AbstractArray, num_timesteps::Int)
    features = []
    targets = []
    for i in 1:size(node_values,3)-num_timesteps
        push!(features, node_values[:,:,i:i+num_timesteps-1])
        push!(targets, reshape(node_values[1,:,i+1:i+num_timesteps], (1, size(node_values, 2),num_timesteps)))
    end
    return features, targets
end

function processed_traffic(dname::String, num_timesteps::Int, dir = nothing, normalize = true)
    create_default_dir(dname)
    d = traffic_datadir(dname, dir)
    adj_matrix, node_values = read_traffic(d, dname)

    if normalize
        node_values = (node_values .- Statistics.mean(node_values, dims=(3,1))) ./ Statistics.std(node_values, dims=(3,1)) #Z-score normalization
    end
    
    s, t, w = adjmatrix2edgeindex(adj_matrix; weighted = true)
    x, y = traffic_generate_task(node_values, num_timesteps )
    
    return s, t, w, x, y
end
[.\src\datasets\graphs\tudataset.jl]
function __init__tudataset()
    DEPNAME = "TUDataset"
    DOCS = "https://chrsmrrs.github.io/datasets/docs/home/"

    register(ManualDataDep(DEPNAME,
                           """
                           Datahub: $DEPNAME.
                           Website: $DOCS
                           """))
end

"""
    TUDataset(name; dir=nothing)

A variety of graph benchmark datasets, *.e.g.* "QM9", "IMDB-BINARY",
"REDDIT-BINARY" or "PROTEINS", collected from the [TU Dortmund University](https://chrsmrrs.github.io/datasets/).
Retrieve from the TUDataset collection the dataset `name`, where `name`
is any of the datasets available [here](https://chrsmrrs.github.io/datasets/docs/datasets/). 

A `TUDataset` object can be indexed to retrieve a specific graph or a subset of graphs.

See [here](https://chrsmrrs.github.io/datasets/docs/format/) for an in-depth 
description of the format. 

# Usage Example

```julia-repl
julia> data = TUDataset("PROTEINS")
dataset TUDataset:
  name        =>    PROTEINS
  metadata    =>    Dict{String, Any} with 1 entry
  graphs      =>    1113-element Vector{MLDatasets.Graph}
  graph_data  =>    (targets = "1113-element Vector{Int64}",)
  num_nodes   =>    43471
  num_edges   =>    162088
  num_graphs  =>    1113

julia> data[1]
(graphs = Graph(42, 162), targets = 1)
```
"""
struct TUDataset <: AbstractDataset
    name::String
    metadata::Dict{String, Any}
    graphs::Vector{Graph}
    graph_data::Union{Nothing, NamedTuple}
    num_nodes::Int
    num_edges::Int
    num_graphs::Int
end

function TUDataset(name; dir = nothing)
    create_default_dir("TUDataset")
    d = tudataset_datadir(name, dir)
    # See here for the file format: https://chrsmrrs.github.io/datasets/docs/format/

    st = readdlm(joinpath(d, "$(name)_A.txt"), ',', Int)
    # Check that the first node is labeled 1.
    # TODO this will fail if the first node is isolated
    @assert minimum(st) == 1
    source, target = st[:, 1], st[:, 2]

    graph_indicator = readdlm(joinpath(d, "$(name)_graph_indicator.txt"), Int) |> vec
    if !all(sort(unique(graph_indicator)) .== 1:length(unique(graph_indicator)))
        @warn "Some graph indicators are not present in graph_indicator.txt. Ordering of graph and graph labels may not be consistent. Base.getindex might produce unexpected behavior for unaltered data."
    end

    num_nodes = length(graph_indicator)
    num_edges = length(source)
    num_graphs = length(unique(graph_indicator))

    # LOAD OPTIONAL FILES IF EXIST

    node_labels = isfile(joinpath(d, "$(name)_node_labels.txt")) ?
                  readdlm(joinpath(d, "$(name)_node_labels.txt"), ',', Int)' |> collect |>
                  maybesqueeze :
                  nothing
    edge_labels = isfile(joinpath(d, "$(name)_edge_labels.txt")) ?
                  readdlm(joinpath(d, "$(name)_edge_labels.txt"), ',', Int)' |> collect |>
                  maybesqueeze :
                  nothing
    graph_labels = isfile(joinpath(d, "$(name)_graph_labels.txt")) ?
                   readdlm(joinpath(d, "$(name)_graph_labels.txt"), ',', Int)' |> collect |>
                   maybesqueeze :
                   nothing

    node_attributes = isfile(joinpath(d, "$(name)_node_attributes.txt")) ?
                      readdlm(joinpath(d, "$(name)_node_attributes.txt"), ',', Float32)' |>
                      collect :
                      nothing
    edge_attributes = isfile(joinpath(d, "$(name)_edge_attributes.txt")) ?
                      readdlm(joinpath(d, "$(name)_edge_attributes.txt"), ',', Float32)' |>
                      collect :
                      nothing
    graph_attributes = isfile(joinpath(d, "$(name)_graph_attributes.txt")) ?
                       readdlm(joinpath(d, "$(name)_graph_attributes.txt"), ',',
                               Float32)' |> collect :
                       nothing

    # TODO: maybe introduce consistency in graph labels and attributes if possible

    # We need this two vectors sorted for efficiency in tudataset_getgraph(full_dataset, i)
    @assert issorted(graph_indicator)
    if !issorted(source)
        p = sortperm(source)
        source, target = source[p], target[p]
        if edge_labels !== nothing
            edge_labels = edge_labels[p]
        end
        if edge_attributes !== nothing
            edge_attributes = edge_attributes[:, p]
        end
    end

    full_dataset = (; num_nodes, num_edges, num_graphs,
                    source, target,
                    graph_indicator,
                    node_labels,
                    edge_labels,
                    graph_labels,
                    node_attributes,
                    edge_attributes,
                    graph_attributes)

    graphs = [tudataset_getgraph(full_dataset, i) for i in sort(unique(graph_indicator))]
    graph_data = (; features = graph_attributes, targets = graph_labels) |> clean_nt
    metadata = Dict{String, Any}("name" => name)
    return TUDataset(name, metadata, graphs, graph_data, num_nodes, num_edges, num_graphs)
end

function tudataset_datadir(name, dir = nothing)
    dir = isnothing(dir) ? datadep"TUDataset" : dir
    LINK = "https://www.chrsmrrs.com/graphkerneldatasets/$name.zip"
    d = joinpath(dir, name)
    if !isdir(d)
        DataDeps.fetch_default(LINK, dir)
        currdir = pwd()
        cd(dir) # Needed since `unpack` extracts in working dir
        DataDeps.unpack(joinpath(dir, "$name.zip"))
        cd(currdir)
    end
    @assert isdir(d)
    return d
end

function tudataset_getgraph(data::NamedTuple, i::Int)
    vmin = searchsortedfirst(data.graph_indicator, i)
    vmax = searchsortedlast(data.graph_indicator, i)
    nodes = vmin:vmax
    node_labels = isnothing(data.node_labels) ? nothing : getobs(data.node_labels, nodes)
    node_attributes = isnothing(data.node_attributes) ? nothing :
                      getobs(data.node_attributes, nodes)

    emin = searchsortedfirst(data.source, vmin)
    emax = searchsortedlast(data.source, vmax)
    edges = emin:emax
    source = data.source[edges] .- vmin .+ 1
    target = data.target[edges] .- vmin .+ 1
    edge_labels = isnothing(data.edge_labels) ? nothing : getobs(data.edge_labels, edges)
    edge_attributes = isnothing(data.edge_attributes) ? nothing :
                      getobs(data.edge_attributes, edges)

    num_nodes = length(nodes)
    num_edges = length(source)
    node_data = (features = node_attributes, targets = node_labels)
    edge_data = (features = edge_attributes, targets = edge_labels)

    return Graph(; num_nodes,
                 edge_index = (source, target),
                 node_data = node_data |> clean_nt,
                 edge_data = edge_data |> clean_nt)
end

Base.length(data::TUDataset) = length(data.graphs)

function Base.getindex(data::TUDataset, ::Colon)
    if data.graph_data === nothing
        return data.graphs
    else
        return (; data.graphs, data.graph_data...)
    end
end

function Base.getindex(data::TUDataset, i)
    if data.graph_data === nothing
        return getobs(data.graphs, i)
    else
        return getobs((; data.graphs, data.graph_data...), i)
    end
end

[.\src\datasets\graphs\windmillenergy.jl]
function __init__windmillenergy()
    DEPNAME = "WindMillEnergy"
    LINK = "https://graphmining.ai/temporal_datasets/"
    register(ManualDataDep(DEPNAME,
                           """
                           Dataset: $DEPNAME
                           Website : $LINK
                           """))
end

function windmillenergy_datadir(size::String, dir = nothing)
    dir = isnothing(dir) ? datadep"WindMillEnergy" : dir
    if size == "small" || size == "medium" || size == "large"
        LINK = "http://www-sop.inria.fr/members/Aurora.Rossi/data/windmill_output_$(size).json"
    else
        print("Please choose a valid size: small, medium or large")
    end 
    if isfile(joinpath(dir, "windmill_output_$(size).json")) == false
        DataDeps.fetch_default(LINK, dir)
    end
    @assert isdir(dir)
    return dir
end

function generate_task_windmillenergy(data::AbstractArray, num_timesteps_in::Int, num_timesteps_out::Int)
    features = []
    targets = []
    for i in 1:(size(data,3)-num_timesteps_in-num_timesteps_out)
        push!(features, data[:,:,i:i+num_timesteps_in-1])
        push!(targets, data[:,:,i+num_timesteps_in:i+num_timesteps_in+num_timesteps_out-1])
    end
    return features, targets
end

function create_windmillenergy_dataset(s::String, normalize::Bool, num_timesteps_in::Int, num_timesteps_out::Int, dir)
    name_file = joinpath(dir, "windmill_output_$(s).json")
    data = read_json(name_file)
    src = zeros(Int, length(data["edges"]))
    dst = zeros(Int, length(data["edges"]))
    for (i, edge) in enumerate(data["edges"])
        src[i] = edge[1] + 1
        dst[i] = edge[2] + 1
    end
    weights = Float32.(data["weights"])
    f = Float32.(stack(data["block"]))
    f = reshape(f, 1, size(f, 1), size(f, 2))

    if normalize
        f = (f .- Statistics.mean(f, dims=(2))) ./ Statistics.std(f, dims=(2)) #Z-score normalization
    end

    x, y = generate_task_windmillenergy(f, num_timesteps_in, num_timesteps_out)

    g = Graph(; edge_index = (src, dst),
                edge_data = weights,
                node_data = (features = x, targets = y))
    return g
end

"""
    WindMillEnergy(; size, normalize=true, num_timesteps_in=8, num_timesteps_out=8, dir=nothing)

The WindMillEnergy dataset contains a collection hourly energy output of windmills from a European country for more than 2 years. 

`WindMillEnergy` is a graph with nodes representing windmills. The edge weights represent the strength of the relationship between the windmills. The number of nodes is fixed and depends on the size of the dataset, 11 for `small`, 26 for `medium`, and 319 for `large`.

The node features and targets are the number of hourly energy output of the windmills. They are represented as an array of arrays of size `(1, num_nodes, num_timesteps_in)`. In both cases, two consecutive arrays are shifted by one-time step.

# Keyword Arguments

- `size::String`: The size of the dataset, can be `small`, `medium`, or `large`.
- `normalize::Bool`: Whether to normalize the data using Z-score normalization. Default is `true`.
- `num_timesteps_in::Int`: The number of time steps, in this case, the number of hours, for the input features. Default is `8`.
- `num_timesteps_out::Int`: The number of time steps, in this case, the number of hours, for the target values. Default is `8`.
- `dir::String`: The directory to save the dataset. Default is `nothing`.

# Examples

```julia-repl
julia> using JSON3

julia> dataset = WindMillEnergy(;size= "small");

julia> dataset.graphs[1]
Graph:
  num_nodes   =>    11
  num_edges   =>    121
  edge_index  =>    ("121-element Vector{Int64}", "121-element Vector{Int64}")
  node_data   =>    (features = "17456-element Vector{Any}", targets = "17456-element Vector{Any}")
  edge_data   =>    121-element Vector{Float32}

julia> size(dataset.graphs[1].node_data.features[1])
(1, 11, 8)
```
"""
struct WindMillEnergy <: AbstractDataset
    graphs::Vector{Graph}
end

function WindMillEnergy(;size::String, normalize::Bool = true, num_timesteps_in::Int = 8 , num_timesteps_out::Int = 8, dir = nothing)
    create_default_dir("WindMillEnergy")
    dir = windmillenergy_datadir(size, dir)
    g = create_windmillenergy_dataset(size, normalize, num_timesteps_in, num_timesteps_out, dir)
    return WindMillEnergy([g])
end

Base.length(d::WindMillEnergy) = length(d.graphs)
Base.getindex(d::WindMillEnergy, ::Colon) = d.graphs[1]
Base.getindex(d::WindMillEnergy, i) = getindex(d.graphs, i)
[.\src\datasets\meshes\faust.jl]
function __init__faust()
    DEPNAME = "MPI-FAUST"
    DOCS = "http://faust.is.tue.mpg.de/"

    register(ManualDataDep(DEPNAME,
                           """
                           Dataset: $DEPNAME.
                           Website: $DOCS
                           """))
end

"""
    FAUST(split=:train; dir=nothing)

The MPI FAUST dataset (2014).

FAUST contains 300 real, high-resolution human scans of 10 different subjects in 30 different poses,
with automatically computed ground-truth correspondences.

Each scan is a high-resolution, triangulated, non-watertight mesh acquired with a 3D multi-stereo system.

FAUST is subdivided into a training and a test set. The training set includes 100 scans (10 per subject)
with their corresponding ground-truth alignments. The test set includes 200 scans. The FAUST benchmark defines
100 preselected scan pairs, partitioned into two classes – 60 requiring intra-subject matching,
40 requiring inter-subject matching.

The dataset required to be downloaded manually from the [website](http://faust.is.tue.mpg.de/)
and extracted in the correct location. For information about where to place the dataset, refer to the example section.


# Dataset Variables

- `scans`: Vector of non-watertight scans in the form of `Mesh`.
- `registrations`: Vector of registrations corresponding to each scan in `scans`. `registrations` like `scans` are also in the form of `Mesh`.
- `labels`: For each scan in the training set, we provide the boolean Vector of length equal to the number of vertices in the corresponding scan. It represents which vertices were reliably registered by the corresponding registration.
- `metadata`: A dictionary containing additional information on the dataset. Currently only `:test` split has metadata containing information about the registrations required for the inter and intra challenge proposed by the author.

# Examples

## Loading the dataset

```julia-repl
julia> using MLDatasets

julia> dataset = FAUST()
[ Info: This program requested access to the data dependency MPI-FAUST
[ Info: It could not be found on your system. It requires manual installation.
┌ Info: Please install it to one of the directories in the DataDeps load path: /home/user/.julia/packages/DataDeps/EDWdQ/deps/data/MPI-FAUST,
│ /home/user/.julia/datadeps/MPI-FAUST,
│ /home/user/.julia/juliaup/julia-1.7.3+0.x86/local/share/julia/datadeps/MPI-FAUST,
│ /home/user/.julia/juliaup/julia-1.7.3+0.x86/share/julia/datadeps/MPI-FAUST,
│ /home/user/datadeps/MPI-FAUST,
│ /scratch/datadeps/MPI-FAUST,
│ /staging/datadeps/MPI-FAUST,
│ /usr/share/datadeps/MPI-FAUST,
└ or /usr/local/share/datadeps/MPI-FAUST
[ Info: by following the instructions:
┌ Info: Dataset: MPI-FAUST.
└ Website: http://faust.is.tue.mpg.de/
Once installed please enter 'y' reattempt loading, or 'a' to abort
[y/a]
```
Now download and extract the dataset into one of the given locations. For unix link systems, an example command can be
```bash
unzip -q <path-to-filename</filename.zip ~/.julia/datadeps
```
The corresponding folder tree should look like
```
├── test
│   ├── challenge_pairs
│   └── scans
└── training
    ├── ground_truth_vertices
    ├── registrations
    └── scans
```
Press `y` to re-attept loading.
```julia-repl
dataset FAUST:
  scans          =>    100-element Vector{Any}
  registrations  =>    100-element Vector{Any}
  labels         =>    100-element Vector{Vector{Bool}}
  metadata       =>    Dict{String, Any} with 0 entries
```

## Load train and test split

```julia-repl
julia> train_faust = FAUST(:train)
dataset FAUST:
  scans          =>    100-element Vector{Any}
  registrations  =>    100-element Vector{Any}
  labels         =>    100-element Vector{Vector{Bool}}
  metadata       =>    Dict{String, Any} with 0 entries

julia> test_faust = FAUST(:test)
dataset FAUST:
  scans          =>    200-element Vector{Any}
  registrations  =>    0-element Vector{Any}
  labels         =>    0-element Vector{Vector{Bool}}
  metadata       =>    Dict{String, Any} with 2 entries
```

## Scan, registrations and ground-truth

```julia-repl
julia> dataset = FAUST(); # defaults to train split

julia> scan = dataset.scans[1] # pick one scan
Mesh{3, Float32, Triangle}:
 Triangle(Float32[-0.0045452323, 0.08537669, 0.22134435], Float32[-0.0030340434, 0.08542955, 0.22206494],
Float32[-0.0042151767, 0.08697654, 0.22171047])
 Triangle(Float32[-0.05358432, 0.08490027, 0.17748278], Float32[-0.05379858, 0.083174236, 0.17670263],
Float32[-0.052645437, 0.08346437, 0.17816517])
.
.
.
 Triangle(Float32[-0.07851, -1.0956081, 0.07093428], Float32[-0.06905176, -1.0986279, 0.07775441],
Float32[-0.069199145, -1.0928112, 0.06812464])

julia> registration = dataset.registrations[1] # The corresponding registration
Mesh{3, Float32, Triangle}:
 Triangle(Float32[0.12491254, 0.51199615, 0.29041073], Float32[0.11376736, 0.5156298, 0.3007352],
Float32[0.119374536, 0.50043654, 0.29687837])
 Triangle(Float32[0.119374536, 0.50043654, 0.29687837], Float32[0.11376736, 0.5156298, 0.3007352],
Float32[0.10888693, 0.5008964, 0.30557302])
.
.
.
 Triangle(Float32[0.033744745, 0.030968456, 0.2359996], Float32[0.058017172, 0.044458304, 0.23422624],
Float32[0.03615713, 0.04858183, 0.23596591])

julia> label = dataset.labels[1] # The ground-truth/labels for each vertices in scan
176387-element Vector{Bool}:
 1
 1
 1
 .
 .
 .
 0
 0
 0
```

# References

1. [MPI Faust Website](http://faust.is.tue.mpg.de/)

2. Bogo, Federica & Romero, Javier & Loper, Matthew & Black, Michael. (2014). FAUST: Dataset
and evaluation for 3D mesh registration. Proceedings of the IEEE Computer Society Conference
on Computer Vision and Pattern Recognition. 10.1109/CVPR.2014.491.
"""
struct FAUST <: AbstractDataset
    scans::Vector
    registrations::Vector
    labels::Vector{Vector{Bool}}
    metadata::Dict{String, Any}
end

function FAUST(split = :train; dir = nothing)
    isnothing(dir) && (dir = datadep"MPI-FAUST")

    @assert split ∈ [:train, :test] "Only train and test splits are present in the dataset."

    registrations = []
    scans = []
    labels = []
    if split == :train
        trainig_dir = joinpath(dir, "training")
        reg_dir = joinpath(trainig_dir, "registrations")
        scan_dir = joinpath(trainig_dir, "scans")
        gt_dir = joinpath(trainig_dir, "ground_truth_vertices")
        for i in range(0, 99)
            reg_file = @sprintf("tr_reg_%03d.ply", i)
            scan_file = @sprintf("tr_scan_%03d.ply", i)
            gt_file = @sprintf("tr_gt_%03d.txt", i)
            scan = load(joinpath(scan_dir, scan_file))
            registration = load(joinpath(reg_dir, reg_file))
            gt = open(joinpath(gt_dir, gt_file)) do file
                s = readlines(file)
                map(x -> x == "1", s)
            end
            push!(scans, scan)
            push!(registrations, registration)
            push!(labels, gt)
        end
        return FAUST(scans, registrations, labels, Dict())
    else
        scan_dir = joinpath(dir, "test", "scans")
        for i in range(0, 199)
            scan_file = @sprintf("test_scan_%03d.ply", i)
            scan = load(joinpath(scan_dir, scan_file))
            push!(scans, scan)
        end
        interfile = joinpath(dir, "test", "challenge_pairs", "inter_challenge.txt")
        intrafile = joinpath(dir, "test", "challenge_pairs", "intra_challenge.txt")
        inter_pairs = read_challenge_file(interfile)
        intra_pairs = read_challenge_file(intrafile)
        metadata = Dict("Inter_Pairs" => inter_pairs, "Intra_Pairs" => intra_pairs)
        return FAUST(scans, registrations, labels, metadata)
    end
end

function read_challenge_file(filename::String)::Vector{Tuple{Int, Int}}
    pairs = open(filename) do file
        s = readlines(file)
        map(x -> Tuple(parse.(Int, (split(x, "_")))), s)
    end
    return pairs
end

[.\src\datasets\misc\boston_housing.jl]
"""
    BostonHousing(; as_df = true, dir = nothing)

The classical Boston Housing tabular dataset.

Sources:
   (a) Origin:  This dataset was taken from the StatLib library which is
                maintained at Carnegie Mellon University.
   (b) Creator:  Harrison, D. and Rubinfeld, D.L. 'Hedonic prices and the 
                 demand for clean air', J. Environ. Economics & Management,
                 vol.5, 81-102, 1978.
   (c) Date: July 7, 1993

Number of Instances: 506

Number of Attributes: 13 continuous attributes (including target
                            attribute "MEDV"), 1 binary-valued attribute.

# Arguments

$ARGUMENTS_SUPERVISED_TABLE

# Fields

$FIELDS_SUPERVISED_TABLE

# Methods

$METHODS_SUPERVISED_TABLE

# Examples
    
```julia-repl
julia> using MLDatasets: BostonHousing

julia> dataset = BostonHousing()
BostonHousing:
  metadata => Dict{String, Any} with 5 entries
  features => 506×13 DataFrame
  targets => 506×1 DataFrame
  dataframe => 506×14 DataFrame


julia> dataset[1:5][1]
5×13 DataFrame
 Row │ CRIM     ZN       INDUS    CHAS   NOX      RM       AGE      DIS      RAD    TAX    PTRATIO  B        LSTAT   
     │ Float64  Float64  Float64  Int64  Float64  Float64  Float64  Float64  Int64  Int64  Float64  Float64  Float64 
─────┼───────────────────────────────────────────────────────────────────────────────────────────────────────────────
   1 │ 0.00632     18.0     2.31      0    0.538    6.575     65.2   4.09        1    296     15.3   396.9      4.98
   2 │ 0.02731      0.0     7.07      0    0.469    6.421     78.9   4.9671      2    242     17.8   396.9      9.14
   3 │ 0.02729      0.0     7.07      0    0.469    7.185     61.1   4.9671      2    242     17.8   392.83     4.03
   4 │ 0.03237      0.0     2.18      0    0.458    6.998     45.8   6.0622      3    222     18.7   394.63     2.94
   5 │ 0.06905      0.0     2.18      0    0.458    7.147     54.2   6.0622      3    222     18.7   396.9      5.33

julia> dataset[1:5][2]
5×1 DataFrame
Row │ MEDV    
    │ Float64 
────┼─────────
  1 │    24.0
  2 │    21.6
  3 │    34.7
  4 │    33.4
  5 │    36.2  

julia> X, y = BostonHousing(as_df=false)[:]
([0.00632 0.02731 … 0.10959 0.04741; 18.0 0.0 … 0.0 0.0; … ; 396.9 396.9 … 393.45 396.9; 4.98 9.14 … 6.48 7.88], [24.0 21.6 … 22.0 11.9])
```
"""
struct BostonHousing <: SupervisedDataset
    metadata::Dict{String, Any}
    features::Any
    targets::Any
    dataframe::Any
end

function BostonHousing(; as_df = true, dir = nothing)
    @assert dir===nothing "custom `dir` is not supported at the moment."
    path = joinpath(@__DIR__, "..", "..", "..", "data", "boston_housing.csv")
    df = read_csv(path)
    features = df[!, DataFrames.Not(:MEDV)]
    targets = df[!, [:MEDV]]

    metadata = Dict{String, Any}()
    metadata["path"] = path
    metadata["feature_names"] = names(features)
    metadata["target_names"] = names(targets)
    metadata["n_observations"] = size(targets, 1)
    metadata["description"] = BOSTONHOUSING_DESCR

    if !as_df
        features = df_to_matrix(features)
        targets = df_to_matrix(targets)
        df = nothing
    end

    return BostonHousing(metadata, features, targets, df)
end

const BOSTONHOUSING_DESCR = """
The Boston Housing Dataset.

Sources:
   (a) Origin:  This dataset was taken from the StatLib library which is
                maintained at Carnegie Mellon University.
   (b) Creator:  Harrison, D. and Rubinfeld, D.L. 'Hedonic prices and the 
                 demand for clean air', J. Environ. Economics & Management,
                 vol.5, 81-102, 1978.
   (c) Date: July 7, 1993

Number of Instances: 506

Number of Attributes: 13 continuous attributes (including target
                            attribute "MEDV"), 1 binary-valued attribute.

Features:

    1. CRIM      per capita crime rate by town
    2. ZN        proportion of residential land zoned for lots over 25,000 sq.ft.
    3. INDUS     proportion of non-retail business acres per town
    4. CHAS      Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
    5. NOX       nitric oxides concentration (parts per 10 million)
    6. RM        average number of rooms per dwelling
    7. AGE       proportion of owner-occupied units built prior to 1940
    8. DIS       weighted distances to five Boston employment centres
    9. RAD       index of accessibility to radial highways
    10. TAX      full-value property-tax rate per 10,000 dollars
    11. PTRATIO  pupil-teacher ratio by town
    12. B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
    13. LSTAT    % lower status of the population
        
Target:
    
    14. MEDV     Median value of owner-occupied homes in 1000's of dollars   

Note: Variable #14 seems to be censored at 50.00 (corresponding to a median price of \\\$50,000); 
Censoring is suggested by the fact that the highest median price of exactly \\\$50,000 is reported in 16 cases, 
while 15 cases have prices between \\\$40,000 and \\\$50,000, with prices rounded to the nearest hundred. 
Harrison and Rubinfeld do not mention any censoring.

The data file stored in this repo is a copy of the UCI ML housing dataset. 
https://archive.ics.uci.edu/ml/machine-learning-databases/housing/
"""

# Deprecated in v0.6,
function Base.getproperty(::Type{BostonHousing}, s::Symbol)
    if s == :features
        @warn "BostonHousing.features() is deprecated, use `BostonHousing().features` instead."
        return () -> BostonHousing(as_df = false).features
    elseif s == :targets
        @warn "BostonHousing.targets() is deprecated, use `BostonHousing().targets` instead."
        return () -> BostonHousing(as_df = false).targets
    elseif s == :feature_names
        @warn "BostonHousing.feature_names() is deprecated, use `BostonHousing().feature_names` instead."
        return () -> lowercase.(BostonHousing().metadata["feature_names"])
    else
        return getfield(BostonHousing, s)
    end
end

[.\src\datasets\misc\iris.jl]
function __init__iris()
    DEPNAME = "Iris"
    LINK = "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/"
    DOCS = "https://archive.ics.uci.edu/ml/datasets/Iris"
    DATA = "iris.data"

    register(DataDep(DEPNAME,
                     """
                     Dataset: The Iris dataset
                     Website: $DOCS
                     """,
                     LINK .* [DATA],
                     "6f608b71a7317216319b4d27b4d9bc84e6abd734eda7872b71a458569e2656c0"))
end

"""
    Iris(; as_df = true, dir = nothing)

Fisher's classic iris dataset. 

Measurements from 3 different species of iris: setosa, versicolor and
virginica. There are 50 examples of each species.

There are 4 measurements for each example: sepal length, sepal width, petal
length and petal width.  The measurements are in centimeters.

The module retrieves the data from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/iris).

NOTE: no pre-defined train-test split for this dataset. 

# Arguments

$ARGUMENTS_SUPERVISED_TABLE

# Fields

$FIELDS_SUPERVISED_TABLE

# Methods

$METHODS_SUPERVISED_TABLE

# Examples

```julia-repl
julia> dataset = Iris()
Iris:
  metadata => Dict{String, Any} with 4 entries
  features => 150×4 DataFrame
  targets => 150×1 DataFrame
  dataframe => 150×5 DataFrame


julia> dataset[1:2]
(2×4 DataFrame
 Row │ sepallength  sepalwidth  petallength  petalwidth 
     │ Float64      Float64     Float64      Float64    
─────┼──────────────────────────────────────────────────
   1 │         5.1         3.5          1.4         0.2
   2 │         4.9         3.0          1.4         0.2, 2×1 DataFrame
 Row │ class       
     │ String15    
─────┼─────────────
   1 │ Iris-setosa
   2 │ Iris-setosa)

julia> X, y = Iris(as_df=false)[:]
([5.1 4.9 … 6.2 5.9; 3.5 3.0 … 3.4 3.0; 1.4 1.4 … 5.4 5.1; 0.2 0.2 … 2.3 1.8], InlineStrings.String15["Iris-setosa" "Iris-setosa" … "Iris-virginica" "Iris-virginica"])
```
"""
struct Iris <: SupervisedDataset
    metadata::Dict{String, Any}
    features::Any
    targets::Any
    dataframe::Any
end

function Iris(; dir = nothing, as_df = true)
    path = datafile("Iris", "iris.data", dir)
    df = read_csv(path, header = 0)
    DataFrames.rename!(df,
                       ["sepallength", "sepalwidth", "petallength", "petalwidth", "class"])

    features = df[!, DataFrames.Not(:class)]
    targets = df[!, [:class]]

    metadata = Dict{String, Any}()
    metadata["path"] = path
    metadata["n_observations"] = size(df, 1)
    metadata["feature_names"] = names(features)
    metadata["target_names"] = names(targets)

    if !as_df
        features = df_to_matrix(features)
        targets = df_to_matrix(targets)
        df = nothing
    end

    return Iris(metadata, features, targets, df)
end

# deprecated in v0.6
function Base.getproperty(::Type{Iris}, s::Symbol)
    if s == :features
        @warn "Iris.features() is deprecated, use `Iris().features` instead."
        return () -> Iris(as_df = false).features
    elseif s == :labels
        @warn "Iris.labels() is deprecated, use `Iris().targets` instead."
        return () -> Iris(as_df = false).targets |> vec
    else
        return getfield(Iris, s)
    end
end

[.\src\datasets\misc\mutagenesis.jl]
function __init__mutagenesis()
    ORIGINAL_LINK = "https://relational.fit.cvut.cz/dataset/Mutagenesis"
    DATA_LINK = "https://raw.githubusercontent.com/CTUAvastLab/datasets/main/mutagenesis"
    DEPNAME = "Mutagenesis"
    DATA = "data.json"
    METADATA = "meta.json"

    register(DataDep(DEPNAME,
                     """
                     Dataset: The $DEPNAME dataset.
                     Website: $ORIGINAL_LINK
                     License: CC0
                     """,
                     "$DATA_LINK/" .* [DATA, METADATA],
                     "80ec1716217135e1f2e0b5a61876c65184e2014e64551103c41e174775ca207c"))
end

"""
    Mutagenesis(; split=:train, dir=nothing)
    Mutagenesis(split; dir=nothing)

The `Mutagenesis` dataset comprises 188 molecules trialed for mutagenicity on Salmonella typhimurium, available from
 [relational.fit.cvut.cz](https://relational.fit.cvut.cz/dataset/Mutagenesis) and
 [CTUAvastLab/datasets](https://github.com/CTUAvastLab/datasets/tree/main/mutagenesis).

Set `split` to `:train`, `:val`, `:test`, or `:all`, to select the training, 
validation, test partition respectively or the whole dataset.
The `indexes` field in the result contains the indexes of the partition in the
full dataset.

Website: https://relational.fit.cvut.cz/dataset/Mutagenesis
License: CC0

```julia-repl
julia> using MLDatasets: Mutagenesis

julia> dataset = Mutagenesis(:train)
Mutagenesis dataset:
  split : train
  indexes : 100-element Vector{Int64}
  features : 100-element Vector{Dict{Symbol, Any}}
  targets : 100-element Vector{Int64}

julia> dataset[1].features
Dict{Symbol, Any} with 5 entries:
  :lumo  => -1.246
  :inda  => 0
  :logp  => 4.23
  :ind1  => 1
  :atoms => Dict{Symbol, Any}[Dict(:element=>"c", :bonds=>Dict{Symbol, Any}[Dict(:element=>"c", :bond_type=>7, :charge=>-0.117, :atom_type=>22), Dict(:element=>"h", :bond_type=>1, :charge=>0.142, :atom_type=>3)…

julia> dataset[1].targets
1

julia> dataset = Mutagenesis(:all)
Mutagenesis dataset:
  split : all
  indexes : 188-element Vector{Int64}
  features : 188-element Vector{Dict{Symbol, Any}}
  targets : 188-element Vector{Int64}
```
"""
struct Mutagenesis <: SupervisedDataset
    metadata::Dict{Symbol, Any}
    split::Symbol
    indexes::Vector{Int}    # indexes of the split in the full dataset
    features::Vector{Dict{Symbol, Any}}
    targets::Vector{Int}
end

Mutagenesis(; split = :train, dir = nothing) = Mutagenesis(split; dir)

function Mutagenesis(split::Symbol; dir = nothing)
    DEPNAME = "Mutagenesis"
    DATA = "data.json"
    METADATA = "meta.json"

    @assert split ∈ [:train, :val, :test, :all]

    data_path = datafile(DEPNAME, DATA, dir)
    metadata_path = datafile(DEPNAME, METADATA, dir)
    samples = read_json(data_path)
    metadata = read_json(metadata_path)
    labelkey = metadata["label"]
    targets = map(i -> i[labelkey], samples)
    features = map(x -> delete!(copy(x), Symbol(labelkey)), samples)
    val_num = metadata["val_samples"]
    test_num = metadata["test_samples"]
    train_idxs = 1:(length(samples) - val_num - test_num)
    val_idxs = (length(samples) - val_num - test_num + 1):(length(samples) - test_num)
    test_idxs = (length(samples) - test_num + 1):length(samples)
    indexes = split == :train ? train_idxs :
              split == :val ? val_idxs :
              split == :test ? test_idxs : 1:length(samples)
    metadata = Dict(metadata)
    metadata[:data_path] = data_path
    metadata[:metadata_path] = metadata_path

    Mutagenesis(metadata, split, indexes, features[indexes], targets[indexes])
end

Base.length(d::Mutagenesis, ::Colon) = numobs((; d.features, d.targets))
Base.getindex(d::Mutagenesis, ::Colon) = getobs((; d.features, d.targets))
Base.getindex(d::Mutagenesis, i) = getobs((; d.features, d.targets), i)

# deprecated in v0.6
function Base.getproperty(::Type{Mutagenesis}, s::Symbol)
    if s == :traindata
        @warn "Mutagenesis.traindata() is deprecated, use `Mutagenesis(split=:train)` instead."
        return (; dir = nothing) -> begin
            d = Mutagenesis(; split = :train, dir)
            d.features, d.targets
        end
    elseif s == :valdata
        @warn "Mutagenesis.valdata() is deprecated, use `Mutagenesis(split=:val)` instead."
        return (; dir = nothing) -> begin
            d = Mutagenesis(; split = :val, dir)
            d.features, d.targets
        end
    elseif s == :testdata
        @warn "Mutagenesis.testdata() is deprecated, use `Mutagenesis(split=:test)` instead."
        return (; dir = nothing) -> begin
            d = Mutagenesis(; split = :test, dir)
            d.features, d.targets
        end
    else
        return getfield(Mutagenesis, s)
    end
end

[.\src\datasets\misc\titanic.jl]
export Titanic

"""
    Titanic(; as_df = true, dir = nothing)

The Titanic dataset, describing the survival of passengers on the Titanic ship.

# Arguments

$ARGUMENTS_SUPERVISED_TABLE

# Fields

$FIELDS_SUPERVISED_TABLE

# Methods

$METHODS_SUPERVISED_TABLE

# Examples

```julia-repl
julia> using MLDatasets: Titanic

julia> using DataFrames

julia> dataset = Titanic()
Titanic:
  metadata => Dict{String, Any} with 5 entries
  features => 891×11 DataFrame
  targets => 891×1 DataFrame
  dataframe => 891×12 DataFrame


julia> describe(dataset.dataframe)
12×7 DataFrame
 Row │ variable     mean      min                  median   max                          nmissing  eltype                   
     │ Symbol       Union…    Any                  Union…   Any                          Int64     Type                     
─────┼──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
   1 │ PassengerId  446.0     1                    446.0    891                                 0  Int64
   2 │ Survived     0.383838  0                    0.0      1                                   0  Int64
   3 │ Pclass       2.30864   1                    3.0      3                                   0  Int64
   4 │ Name                   Abbing, Mr. Anthony           van Melkebeke, Mr. Philemon         0  String
   5 │ Sex                    female                        male                                0  String7
   6 │ Age          29.6991   0.42                 28.0     80.0                              177  Union{Missing, Float64}
   7 │ SibSp        0.523008  0                    0.0      8                                   0  Int64
   8 │ Parch        0.381594  0                    0.0      6                                   0  Int64
   9 │ Ticket                 110152                        WE/P 5735                           0  String31
  10 │ Fare         32.2042   0.0                  14.4542  512.329                             0  Float64
  11 │ Cabin                  A10                           T                                 687  Union{Missing, String15}
  12 │ Embarked               C                             S                                   2  Union{Missing, String1}
```
"""
struct Titanic <: SupervisedDataset
    metadata::Dict{String, Any}
    features::Any
    targets::Any
    dataframe::Any
end

function Titanic(; as_df = true, dir = nothing)
    @assert dir===nothing "custom `dir` is not supported at the moment."
    path = joinpath(@__DIR__, "..", "..", "..", "data", "titanic.csv")
    df = read_csv(path)

    features = df[!, DataFrames.Not(:Survived)]
    targets = df[!, [:Survived]]

    metadata = Dict{String, Any}()
    metadata["path"] = path
    metadata["feature_names"] = names(features)
    metadata["target_names"] = names(targets)
    metadata["n_observations"] = size(df, 1)
    metadata["description"] = TITANIC_DESCR

    if !as_df
        features = df_to_matrix(features)
        targets = df_to_matrix(targets)
        df = nothing
    end

    return Titanic(metadata, features, targets, df)
end

const TITANIC_DESCR = """
The titanic and titanic2 data frames describe the survival status of individual passengers on the Titanic. 

The titanic data frame does not contain information from the crew, but it does contain actual ages of half of the passengers. 
The principal source for data about Titanic passengers is the Encyclopedia Titanica. 
The datasets used here were begun by a variety of researchers. 
One of the original sources is Eaton & Haas (1994) 
Titanic: Triumph and Tragedy, Patrick Stephens Ltd, which includes a passenger list created by many researchers and edited by Michael A. Findlay.

The variables on our extracted dataset are pclass, survived, name, age, embarked, home.dest, room, ticket, boat, and sex. 
pclass refers to passenger class (1st, 2nd, 3rd), and is a proxy for socio-economic class. 
Age is in years, and some infants had fractional values. 
The titanic2 data frame has no missing data and includes records for the crew, but age is dichotomized at adult vs. child. 
These data were obtained from Robert Dawson, Saint Mary's University, E-mail. 
The variables are pclass, age, sex, survived. 
These data frames are useful for demonstrating many of the functions in Hmisc as well as 
demonstrating binary logistic regression analysis using the Design library. 
For more details and references see Simonoff, Jeffrey S (1997): The "unusual episode" and a second statistics course. 
J Statistics Education, Vol. 5 No. 1. Thomas Cason of UVa has greatly updated and improved the titanic data frame 
using the Encyclopedia Titanica and created a new dataset called titanic3. 
These datasets reflects the state of data available as of 2 August 1999. 
Some duplicate passengers have been dropped, many errors corrected, many missing ages filled in, and new variables created.

DATASET specs

NAME:	titanic3
TYPE:	Census
SIZE:	1309 Passengers, 14 Variables

DESCRIPTIVE ABSTRACT:

The titanic3 data frame describes the survival status of individual passengers on the Titanic.
The titanic3 data frame does not contain information for the crew, but it does contain actual and estimated ages for almost 80% of the passengers.

SOURCES:

Hind, Philip. Encyclopedia Titanica. Online-only resource. Retrieved 01Feb2012 from http://www.encyclopedia-titanica.org/

VARIABLE DESCRIPTIONS

Pclass	Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)
survival	Survival (0 = No; 1 = Yes)
name	Name
sex	Sex
age	Age
sibsp	Number of Siblings/Spouses Aboard
parch	Number of Parents/Children Aboard
ticket	Ticket Number
fare	Passenger Fare (British pound)
cabin	Cabin
embarked	Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)
boat	Lifeboat
body	Body Identification Number
home.dest	Home/Destination


SPECIAL NOTES

Pclass is a proxy for socio-economic status (SES) 1st ~ Upper; 2nd ~ Middle; 3rd ~ Lower

Age is in Years; Fractional if Age less than One (1) If the Age is estimated, it is in the form xx.5

Fare is in Pre-1970 British Pounds ()
Conversion Factors: 1 = 12s = 240d and 1s = 20d


With respect to the family relation variables (i.e. sibsp and parch) some relations were ignored. 
The following are the definitions used for sibsp and parch.

Sibling:	Brother, Sister, Stepbrother, or Stepsister of Passenger Aboard Titanic
Spouse:	Husband or Wife of Passenger Aboard Titanic (Mistresses and Fiances Ignored)
Parent:	Mother or Father of Passenger Aboard Titanic
Child:	Son, Daughter, Stepson, or Stepdaughter of Passenger Aboard Titanic

Other family relatives excluded from this study include cousins, nephews/nieces, aunts/uncles, and in-laws. 
Some children travelled only with a nanny, therefore parch=0 for them. 
    As well, some travelled with very close friends or neighbors in a village, however, the definitions do not support such relations.


An interesting result may be obtained using functions from the Hmisc library.

attach	(titanic3)
plsmo	(age, survived, group=sex, datadensity=T) 
# or group=pclass plot	(naclus	(titanic3)) # study patterns of missing values summary	(survived ~ age + sex + pclass + sibsp + parch, data=titanic3)
"""

function Base.getproperty(::Type{Titanic}, s::Symbol)
    if s == :features
        @warn "Titanic.features() is deprecated, use `Titanic().features` instead."
        return () -> Titanic(as_df = false).features
    elseif s == :targets
        @warn "Titanic.targets() is deprecated, use `Titanic().targets` instead."
        return () -> Titanic(as_df = false).targets
    elseif s == :feature_names
        @warn "Titanic.feature_names() is deprecated, use `Titanic().feature_names` instead."
        return () -> Titanic().metadata["feature_names"]
    else
        return getfield(Titanic, s)
    end
end

[.\src\datasets\misc\wine.jl]
export Wine

"""
    Wine(; as_df = true, dir = nothing)

The UCI Wine dataset.

These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars.
The analysis determined the quantities of 13 constituents found in each of the three types of wines.

Data source is the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/wine) where further details can be retrieved.

# Arguments

$ARGUMENTS_SUPERVISED_TABLE

# Fields

$FIELDS_SUPERVISED_TABLE

# Methods

$METHODS_SUPERVISED_TABLE

# Examples

```julia-repl
julia> using MLDatasets: Wine

julia> using DataFrames

julia> dataset = Wine()
dataset Wine:
  metadata   =>    Dict{String, Any} with 5 entries
  features   =>    178×13 DataFrame
  targets    =>    178×1 DataFrame
  dataframe  =>    178×14 DataFrame


julia> describe(dataset.dataframe)
14×7 DataFrame
 Row │ variable              mean        min     median   max      nmissing  eltype   
     │ Symbol                Float64     Real    Float64  Real     Int64     DataType 
─────┼────────────────────────────────────────────────────────────────────────────────
   1 │ Wine                    1.9382      1       2.0       3            0  Int64
   2 │ Alcohol                13.0006     11.03   13.05     14.83         0  Float64
   3 │ Malic.acid              2.33635     0.74    1.865     5.8          0  Float64
   4 │ Ash                     2.36652     1.36    2.36      3.23         0  Float64
   5 │ Acl                    19.4949     10.6    19.5      30.0          0  Float64
   6 │ Mg                     99.7416     70      98.0     162            0  Int64
   7 │ Phenols                 2.29511     0.98    2.355     3.88         0  Float64
   8 │ Flavanoids              2.02927     0.34    2.135     5.08         0  Float64
   9 │ Nonflavanoid.phenols    0.361854    0.13    0.34      0.66         0  Float64
  10 │ Proanth                 1.5909      0.41    1.555     3.58         0  Float64
  11 │ Color.int               5.05809     1.28    4.69     13.0          0  Float64
  12 │ Hue                     0.957449    0.48    0.965     1.71         0  Float64
  13 │ OD                      2.61169     1.27    2.78      4.0          0  Float64
  14 │ Proline               746.893     278     673.5    1680            0  Int64
```
"""
struct Wine <: SupervisedDataset
    metadata::Dict{String, Any}
    features::Any
    targets::Any
    dataframe::Any
end

function Wine(; as_df = true, dir = nothing)
    @assert dir===nothing "custom `dir` is not supported at the moment."
    path = joinpath(@__DIR__, "..", "..", "..", "data", "wine.csv")
    df = read_csv(path)

    features = df[!, DataFrames.Not(:Wine)]
    targets = df[!, [:Wine]]

    metadata = Dict{String, Any}()
    metadata["path"] = path
    metadata["feature_names"] = names(features)
    metadata["target_names"] = names(targets)
    metadata["n_observations"] = size(df, 1)

    if !as_df
        features = df_to_matrix(features)
        targets = df_to_matrix(targets)
        df = nothing
    end

    return Wine(metadata, features, targets, df)
end

[.\src\datasets\text\ptblm.jl]

function __init__ptblm()
    DEPNAME = "PTBLM"
    TRAINFILE = "ptb.train.txt"
    TESTFILE = "ptb.test.txt"

    register(DataDep(DEPNAME,
                     """
                     Dataset: Penn Treebank sentences for language modeling
                     Website: https://github.com/tomsercu/lstm

                     The files are available for download at the github
                     repository linked above. Note that using the data
                     responsibly and respecting copyright remains your
                     responsibility.
                     """,
                     "https://raw.githubusercontent.com/tomsercu/lstm/master/data/" .*
                     [TRAINFILE, TESTFILE],
                     "218f4e6c7288bb5efeb03cc4cb8ae9c04ecd8462ebfba8e13e3549fab69dc25f"))
end

"""
    PTBLM(; split=:train, dir=nothing)
    PTBLM(split; [dir])

The PTBLM dataset consists of Penn Treebank sentences
for language modeling, available from https://github.com/tomsercu/lstm.
The unknown words are replaced with <unk> so that the
total vocabulary size becomes 10000.
"""
struct PTBLM <: SupervisedDataset
    metadata::Dict{String, Any}
    split::Symbol
    features::Vector{Vector{String}}
    targets::Vector{Vector{String}}
end

PTBLM(; split = :train, dir = nothing) = PTBLM(split; dir)

function PTBLM(split::Symbol; dir = nothing)
    DEPNAME = "PTBLM"
    @assert split ∈ [:train, :test]
    FILE = split == :train ? "ptb.train.txt" : "ptb.test.txt"
    path = datafile(DEPNAME, FILE, dir)

    lines = open(readlines, path)
    @assert all(x -> x isa String, lines)
    features = map(l -> Vector{String}(Base.split(chomp(l))), lines)

    targets = map(features) do x
        y = copy(x)
        popfirst!(y)
        push!(y, "<eos>")
    end

    metadata = Dict{String, Any}("n_observations" => length(features))
    return PTBLM(metadata, split, features, targets)
end

# DEPRECATED INTERFACE, REMOVE IN v0.7 (or 0.6.x)
function Base.getproperty(::Type{PTBLM}, s::Symbol)
    if s === :traindata
        @warn "PTBLM.traindata() is deprecated, use `PTBLM(:train)[:]` instead."
        function traindata(; dir = nothing)
            PTBLM(; split = :train, dir)[:]
        end
        return traindata
    elseif s === :testdata
        @warn "PTBLM.testdata() is deprecated, use `PTBLM(:test)[:]` instead."
        function testdata(; dir = nothing)
            PTBLM(; split = :test, dir)[:]
        end
        return testdata
    else
        return getfield(PTBLM, s)
    end
end

[.\src\datasets\text\smsspamcollection.jl]
function __init__smsspam()
    DEPNAME = "SMSSpamCollection"
    LINK = "https://raw.githubusercontent.com/mohitgupta-omg/Kaggle-SMS-Spam-Collection-Dataset-/master/"
    LINK = "https://archive.ics.uci.edu/ml/machine-learning-databases/00228/"
    DOCS = "https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection#"
    DATA = "smsspamcollection.zip"

    register(DataDep(DEPNAME,
                     """
                     Dataset: The SMS Spam Collection v.1
                     Website: $DOCS
                     """,
                     LINK .* [DATA],
                     "1587ea43e58e82b14ff1f5425c88e17f8496bfcdb67a583dbff9eefaf9963ce3",
                     post_fetch_method = unpack))
end

"""
    SMSSpamCollection(; dir=nothing)


The SMS Spam Collection v.1 (hereafter the corpus) is a set of SMS tagged messages 
that have been collected for SMS Spam research. It contains one set of SMS messages
in English of 5,574 messages, tagged according being ham (legitimate) or spam.
The corpus has a total of 4,827 SMS legitimate messages (86.6%) and a total of 747 (13.4%) spam messages.

The corpus has been collected by Tiago Agostinho de Almeida (http://www.dt.fee.unicamp.br/~tiago) 
and José María Gómez Hidalgo (http://www.esp.uem.es/jmgomez).

```julia-repl
julia> using MLDatasets: SMSSpamCollection

julia> targets = SMSSpamCollection.targets();

julia> summary(targets)
"5574-element Vector{Any}"

julia> targets[1]
"ham"

julia> summary(features)
"5574-element Vector{Any}"
"""
struct SMSSpamCollection <: SupervisedDataset
    metadata::Dict{String, Any}
    features::Vector{String}
    targets::Vector{String}
end

function SMSSpamCollection(; dir = nothing)
    DEPNAME = "SMSSpamCollection"
    path = datafile(DEPNAME, "SMSSpamCollection", dir)
    spam_data = open(readlines, path)
    spam_data = [split(str, "\t") for str in spam_data]
    @assert all(x -> length(x) == 2, spam_data)
    targets = [s[1] for s in spam_data]
    features = [s[2] for s in spam_data]

    metadata = Dict{String, Any}()
    metadata["n_observations"] = length(features)
    SMSSpamCollection(metadata, features, targets)
end

# DEPRECATED in V0.6
function Base.getproperty(::Type{SMSSpamCollection}, s::Symbol)
    if s == :features
        @warn "SMSSpamCollection.features() is deprecated, use `SMSSpamCollection().features` instead."
        return () -> SMSSpamCollection().features
    elseif s == :targets
        @warn "SMSSpamCollection.targets() is deprecated, use `SMSSpamCollection().targets` instead."
        return () -> SMSSpamCollection().targets
    else
        return getfield(SMSSpamCollection, s)
    end
end

[.\src\datasets\text\udenglish.jl]
function __init__udenglish()
    DEPNAME = "UD_English"
    TRAINFILE = "en_ewt-ud-train.conllu"
    DEVFILE = "en_ewt-ud-dev.conllu"
    TESTFILE = "en_ewt-ud-test.conllu"

    register(DataDep(DEPNAME,
                     """
                     Dataset: Universal Dependencies - English Dependency Treebank Universal Dependencies English Web Treebank
                     Authors: Natalia Silveira and Timothy Dozat and
                              Marie-Catherine de Marneffe and Samuel
                              Bowman and Miriam Connor and John Bauer and
                              Christopher D. Manning
                     Website: https://github.com/UniversalDependencies/UD_English-EWT


                     The files are available for download at the github
                     repository linked above. Note that using the data
                     responsibly and respecting copyright remains your
                     responsibility. Copyright and License is discussed in
                     detail on the Website.
                     """,
                     "https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/" .*
                     [TRAINFILE, DEVFILE, TESTFILE],
                     "43d082e0b277c4758f7ad5d3f7c508078d43e82d4d6fcaa58d0056a6d5576a49"))
end

"""
    UD_English(; split=:train, dir=nothing)
    UD_English(split=; [dir])

A Gold Standard Universal Dependencies Corpus for
English, built over the source material of the
English Web Treebank LDC2012T13
(https://catalog.ldc.upenn.edu/LDC2012T13).

The corpus comprises 254,825 words and 16,621 sentences, 
taken from five genres of web media: weblogs, newsgroups, emails, reviews, and Yahoo! answers. 
See the LDC2012T13 documentation for more details on the sources of the sentences. 
The trees were automatically converted into Stanford Dependencies and then hand-corrected to Universal Dependencies. 
All the basic dependency annotations have been single-annotated, a limited portion of them have been double-annotated, 
and subsequent correction has been done to improve consistency. Other aspects of the treebank, such as Universal POS, 
features and enhanced dependencies, has mainly been done automatically, with very limited hand-correction.


Authors: Natalia Silveira and Timothy Dozat and
            Marie-Catherine de Marneffe and Samuel
            Bowman and Miriam Connor and John Bauer and
            Christopher D. Manning
Website: https://github.com/UniversalDependencies/UD_English-EWT
"""
struct UD_English <: UnsupervisedDataset
    metadata::Dict{String, Any}
    split::Symbol
    features::Vector{Vector{Vector{String}}}
end

UD_English(; split = :train, dir = nothing) = UD_English(split; dir)

function UD_English(split::Symbol; dir = nothing)
    DEPNAME = "UD_English"
    TRAINFILE = "en_ewt-ud-train.conllu"
    DEVFILE = "en_ewt-ud-dev.conllu"
    TESTFILE = "en_ewt-ud-test.conllu"

    @assert split ∈ [:train, :test, :dev]

    FILE = split == :train ? TRAINFILE :
           split == :test ? TESTFILE :
           split === :dev ? DEVFILE : error()

    path = datafile(DEPNAME, FILE, dir)

    doc = []
    sent = []
    lines = open(readlines, path)
    for line in lines
        line = chomp(line)
        if length(line) == 0
            length(sent) > 0 && push!(doc, sent)
            sent = []
        elseif line[1] == '#' # comment line
            continue
        else
            items = Vector{String}(Base.split(line, '\t'))
            push!(sent, items)
        end
    end
    length(sent) > 0 && push!(doc, sent)
    T = typeof(doc[1][1])
    features = Vector{Vector{T}}(doc)

    metadata = Dict{String, Any}("n_observations" => length(features))
    return UD_English(metadata, split, features)
end

[.\src\datasets\vision\cifar10.jl]
function __init__cifar10()
    DEPNAME = "CIFAR10"

    register(DataDep(DEPNAME,
                     """
                     Dataset: The CIFAR-10 dataset
                     Authors: Alex Krizhevsky, Vinod Nair, Geoffrey Hinton
                     Website: https://www.cs.toronto.edu/~kriz/cifar.html
                     Reference: https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf

                     [Krizhevsky, 2009]
                         Alex Krizhevsky.
                         "Learning Multiple Layers of Features from Tiny Images",
                         Tech Report, 2009.

                     The CIFAR-10 dataset is a labeled subsets of the 80
                     million tiny images dataset. It consists of 60000
                     32x32 colour images in 10 classes, with 6000 images
                     per class.

                     The compressed archive file that contains the
                     complete dataset is available for download at the
                     offical website linked above; specifically the binary
                     version for C programs. Note that using the data
                     responsibly and respecting copyright remains your
                     responsibility. The authors of CIFAR-10 aren't really
                     explicit about any terms of use, so please read the
                     website to make sure you want to download the
                     dataset.
                     """,
                     "https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz",
                     "c4a38c50a1bc5f3a1c5537f2155ab9d68f9f25eb1ed8d9ddda3db29a59bca1dd",
                     post_fetch_method = DataDeps.unpack))
end

"""
    CIFAR10(; Tx=Float32, split=:train, dir=nothing)
    CIFAR10([Tx, split])

The CIFAR10 dataset is a labeled subsets of the 80
million tiny images dataset. It consists of 60000
32x32 colour images in 10 classes, with 6000 images
per class.

# Arguments

$ARGUMENTS_SUPERVISED_ARRAY
- `split`: selects the data partition. Can take the values `:train` or `:test`. 

# Fields

$FIELDS_SUPERVISED_ARRAY
- `split`.

# Methods

$METHODS_SUPERVISED_ARRAY
- [`convert2image`](@ref) converts features to `RGB` images.

# Examples

```julia-repl
julia> using MLDatasets: CIFAR10

julia> dataset = CIFAR10()
CIFAR10:
  metadata    =>    Dict{String, Any} with 2 entries
  split       =>    :train
  features    =>    32×32×3×50000 Array{Float32, 4}
  targets     =>    50000-element Vector{Int64}

julia> dataset[1:5].targets
5-element Vector{Int64}:
 6
 9
 9
 4
 1

julia> X, y = dataset[:];

julia> dataset = CIFAR10(Tx=Float64, split=:test)
CIFAR10:
  metadata    =>    Dict{String, Any} with 2 entries
  split       =>    :test
  features    =>    32×32×3×10000 Array{Float64, 4}
  targets     =>    10000-element Vector{Int64}

julia> dataset.metadata
Dict{String, Any} with 2 entries:
  "n_observations" => 10000
  "class_names"    => ["airplane", "automobile", "bird", "cat", "deer", "dog", "frog", "horse", "ship", "truck"]
```
"""
struct CIFAR10 <: SupervisedDataset
    metadata::Dict{String, Any}
    split::Symbol
    features::Array{<:Any, 4}
    targets::Vector{Int}
end

CIFAR10(; split = :train, Tx = Float32, dir = nothing) = CIFAR10(Tx, split; dir)
CIFAR10(split::Symbol; kws...) = CIFAR10(; split, kws...)
CIFAR10(Tx::Type; kws...) = CIFAR10(; Tx, kws...)

function CIFAR10(Tx::Type, split::Symbol; dir = nothing)
    DEPNAME = "CIFAR10"
    NCHUNKS = 5
    TESTSET_FILENAME = joinpath("cifar-10-batches-bin", "test_batch.bin")

    function filename_for_chunk(file_index::Int)
        joinpath("cifar-10-batches-bin", "data_batch_$(file_index).bin")
    end

    @assert split ∈ (:train, :test)

    if split == :train
        # placeholders for the chunks
        Xs = Vector{Array{UInt8, 4}}(undef, NCHUNKS)
        Ys = Vector{Vector{Int}}(undef, NCHUNKS)
        # loop over all 5 trainingset files (i.e. chunks)
        for file_index in 1:NCHUNKS
            file_name = filename_for_chunk(file_index)
            file_path = datafile(DEPNAME, file_name, dir)
            # load all the data from each file and append it to
            # the placeholders X and Y
            X, Y = CIFAR10Reader.readdata(file_path)
            Xs[file_index] = X
            Ys[file_index] = Y

            #TODO define a lazy version that reads a signle image only when asked
            # file_index = ceil(Int, index / Reader.CHUNK_SIZE)
            # file_name = filename_for_chunk(file_index)
            # file_path = datafile(DEPNAME, file_name, dir)
            ## once we know the file we just need to compute the approriate
            ## offset of the image realtive to that file.
            # sub_index = ((index - 1) % Reader.CHUNK_SIZE) + 1
            # image, label = CIFAR10Reader.readdata(file_path, sub_index)
        end
        # cat all the placeholders into one image array
        # and one label array. (good enough)
        images = cat(Xs..., dims = 4)::Array{UInt8, 4}
        labels = vcat(Ys...)::Vector{Int}
        # optionally transform the image array before returning
        features, targets = bytes_to_type(Tx, images), labels
    else
        file_path = datafile(DEPNAME, TESTSET_FILENAME, dir)
        # simply read the complete content of the testset file
        images, labels = CIFAR10Reader.readdata(file_path)
        # optionally transform the image array before returning
        features, targets = bytes_to_type(Tx, images), labels
    end

    metadata = Dict{String, Any}()
    metadata["class_names"] = [
        "airplane",
        "automobile",
        "bird",
        "cat",
        "deer",
        "dog",
        "frog",
        "horse",
        "ship",
        "truck",
    ]
    metadata["n_observations"] = size(features)[end]
    return CIFAR10(metadata, split, features, targets)
end

function convert2image(::Type{<:CIFAR10}, x::AbstractArray{<:Integer})
    convert2image(CIFAR10, reinterpret(N0f8, convert(Array{UInt8}, x)))
end

function convert2image(::Type{<:CIFAR10}, x::AbstractArray{T, N}) where {T, N}
    @assert N == 3 || N == 4
    x = permutedims(x, (3, 2, 1, 4:N...))
    ImageCore = ImageShow.ImageCore
    return ImageCore.colorview(ImageCore.RGB, x)
end

# DEPRECATED INTERFACE, REMOVE IN v0.7 (or 0.6.x)
function Base.getproperty(::Type{CIFAR10}, s::Symbol)
    if s == :traintensor
        @warn "CIFAR10.traintensor() is deprecated, use `CIFAR10(split=:train).features` instead."
        traintensor(T::Type = N0f8; kws...) = traintensor(T, :; kws...)
        traintensor(i; kws...) = traintensor(N0f8, i; kws...)
        function traintensor(T::Type, i; dir = nothing)
            CIFAR10(; split = :train, Tx = T, dir)[i][1]
        end
        return traintensor
    elseif s == :testtensor
        @warn "CIFAR10.testtensor() is deprecated, use `CIFAR10(split=:test).features` instead."
        testtensor(T::Type = N0f8; kws...) = testtensor(T, :; kws...)
        testtensor(i; kws...) = testtensor(N0f8, i; kws...)
        function testtensor(T::Type, i; dir = nothing)
            CIFAR10(; split = :test, Tx = T, dir)[i][1]
        end
        return testtensor
    elseif s == :trainlabels
        @warn "CIFAR10.trainlabels() is deprecated, use `CIFAR10(split=:train).targets` instead."
        trainlabels(; kws...) = trainlabels(:; kws...)
        function trainlabels(i; dir = nothing)
            CIFAR10(; split = :train, dir)[i][2]
        end
        return trainlabels
    elseif s == :testlabels
        @warn "CIFAR10.testlabels() is deprecated, use `CIFAR10(split=:test).targets` instead."
        testlabels(; kws...) = testlabels(:; kws...)
        function testlabels(i; dir = nothing)
            CIFAR10(; split = :test, dir)[i][2]
        end
        return testlabels
    elseif s == :traindata
        @warn "CIFAR10.traindata() is deprecated, use `CIFAR10(split=:train)[:]` instead."
        traindata(T::Type = N0f8; kws...) = traindata(T, :; kws...)
        traindata(i; kws...) = traindata(N0f8, i; kws...)
        function traindata(T::Type, i; dir = nothing)
            CIFAR10(; split = :train, Tx = T, dir)[i]
        end
        return traindata
    elseif s == :testdata
        @warn "CIFAR10.testdata() is deprecated, use `CIFAR10(split=:test)[:]` instead."
        testdata(T::Type = N0f8; kws...) = testdata(T, :; kws...)
        testdata(i; kws...) = testdata(N0f8, i; kws...)
        function testdata(T::Type, i; dir = nothing)
            CIFAR10(; split = :test, Tx = T, dir)[i]
        end
        return testdata
    elseif s == :convert2image
        @warn "CIFAR10.convert2image(x) is deprecated, use `convert2image(CIFAR10, x)` instead"
        return x -> convert2image(CIFAR10, x)
    else
        return getfield(CIFAR10, s)
    end
end

[.\src\datasets\vision\cifar100.jl]
function __init__cifar100()
    DEPNAME = "CIFAR100"

    register(DataDep(DEPNAME,
                     """
                     Dataset: The CIFAR-100 dataset
                     Authors: Alex Krizhevsky, Vinod Nair, Geoffrey Hinton
                     Website: https://www.cs.toronto.edu/~kriz/cifar.html
                     Reference: https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf

                     [Krizhevsky, 2009]
                         Alex Krizhevsky.
                         "Learning Multiple Layers of Features from Tiny Images",
                         Tech Report, 2009.

                     The CIFAR-100 dataset is a labeled subsets of the 80
                     million tiny images dataset. It consists of 60000
                     32x32 colour images in 100 classes. Specifically, it
                     has 100 classes containing 600 images each. There are
                     500 training images and 100 testing images per class.
                     The 100 classes in the CIFAR-100 are grouped into 20
                     superclasses. Each image comes with a "fine" label
                     (the class to which it belongs) and a "coarse" label
                     (the superclass to which it belongs).

                     The compressed archive file that contains the
                     complete dataset is available for download at the
                     offical website linked above; specifically the binary
                     version for C programs. Note that using the data
                     responsibly and respecting copyright remains your
                     responsibility. The authors of CIFAR-10 aren't really
                     explicit about any terms of use, so please read the
                     website to make sure you want to download the
                     dataset.
                     """,
                     "https://www.cs.toronto.edu/~kriz/cifar-100-binary.tar.gz",
                     "58a81ae192c23a4be8b1804d68e518ed807d710a4eb253b1f2a199162a40d8ec",
                     post_fetch_method = DataDeps.unpack))
end

"""
    CIFAR100(; Tx=Float32, split=:train, dir=nothing)
    CIFAR100([Tx, split])

The CIFAR100 dataset is a labeled subsets of the 80
million tiny images dataset. It consists of 60000
32x32 colour images in 100 classes and 20 superclasses,
with 600 images per class.

Return the CIFAR-100 **trainset** labels (coarse and fine)
corresponding to the given `indices` as a tuple of two `Int` or
two `Vector{Int}`. The variables returned are the coarse label(s)
(`Yc`) and the fine label(s) (`Yf`) respectively.

# Arguments

$ARGUMENTS_SUPERVISED_ARRAY
- `split`: selects the data partition. Can take the values `:train` or `:test`. 

# Fields

$FIELDS_SUPERVISED_ARRAY
- `split`.

# Methods

$METHODS_SUPERVISED_ARRAY
- [`convert2image`](@ref) converts features to `RGB` images.

# Examples

```julia-repl
julia> dataset = CIFAR100()
CIFAR100:
  metadata    =>    Dict{String, Any} with 3 entries
  split       =>    :train
  features    =>    32×32×3×50000 Array{Float32, 4}
  targets     =>    (coarse = "50000-element Vector{Int64}", fine = "50000-element Vector{Int64}")

julia> dataset[1:5].targets
(coarse = [11, 15, 4, 14, 1], fine = [19, 29, 0, 11, 1])

julia> X, y = dataset[:];

julia> dataset.metadata
Dict{String, Any} with 3 entries:
  "n_observations"     => 50000
  "class_names_coarse" => ["aquatic_mammals", "fish", "flowers", "food_containers", "fruit_and_vegetables", "household_electrical_devices", "household_furniture", "insects", "large_carnivores", "large_man-made_…
  "class_names_fine"   => ["apple", "aquarium_fish", "baby", "bear", "beaver", "bed", "bee", "beetle", "bicycle", "bottle"  …  "train", "trout", "tulip", "turtle", "wardrobe", "whale", "willow_tree", "wolf", "w…
```
"""
struct CIFAR100 <: SupervisedDataset
    metadata::Dict{String, Any}
    split::Symbol
    features::Array{<:Any, 4}
    targets::NamedTuple{(:coarse, :fine), Tuple{Vector{Int}, Vector{Int}}}
end

CIFAR100(; split = :train, Tx = Float32, dir = nothing) = CIFAR100(Tx, split; dir)
CIFAR100(split::Symbol; kws...) = CIFAR100(; split, kws...)
CIFAR100(Tx::Type; kws...) = CIFAR100(; Tx, kws...)

function CIFAR100(Tx::Type, split::Symbol; dir = nothing)
    DEPNAME = "CIFAR100"
    TRAINSET_FILENAME = joinpath("cifar-100-binary", "train.bin")
    TESTSET_FILENAME = joinpath("cifar-100-binary", "test.bin")
    COARSE_FILENAME = joinpath("cifar-100-binary", "coarse_label_names.txt")
    FINE_FILENAME = joinpath("cifar-100-binary", "fine_label_names.txt")
    TRAINSET_SIZE = 50_000
    TESTSET_SIZE = 10_000

    @assert split ∈ (:train, :test)

    if split == :train
        file_path = datafile(DEPNAME, TRAINSET_FILENAME, dir)
        images, labels_c, labels_f = CIFAR100Reader.readdata(file_path, TRAINSET_SIZE)
    else
        file_path = datafile(DEPNAME, TESTSET_FILENAME, dir)
        images, labels_c, labels_f = CIFAR100Reader.readdata(file_path, TESTSET_SIZE)
    end
    features = bytes_to_type(Tx, images)
    targets = (coarse = labels_c, fine = labels_f)

    metadata = Dict{String, Any}()
    metadata["class_names_coarse"] = readlines(datafile(DEPNAME, COARSE_FILENAME, dir))
    metadata["class_names_fine"] = readlines(datafile(DEPNAME, FINE_FILENAME, dir))
    metadata["n_observations"] = size(features)[end]

    return CIFAR100(metadata, split, features, targets)
end

convert2image(::Type{<:CIFAR100}, x) = convert2image(CIFAR10, x)

# DEPRECATED INTERFACE, REMOVE IN v0.7 (or 0.6.x)
function Base.getproperty(::Type{CIFAR100}, s::Symbol)
    if s == :traintensor
        @warn "CIFAR100.traintensor() is deprecated, use `CIFAR100(split=:train).features` instead."
        traintensor(T::Type = N0f8; kws...) = traintensor(T, :; kws...)
        traintensor(i; kws...) = traintensor(N0f8, i; kws...)
        function traintensor(T::Type, i; dir = nothing)
            CIFAR100(; split = :train, Tx = T, dir)[i][1]
        end
        return traintensor
    elseif s == :testtensor
        @warn "CIFAR100.testtensor() is deprecated, use `CIFAR100(split=:test).features` instead."
        testtensor(T::Type = N0f8; kws...) = testtensor(T, :; kws...)
        testtensor(i; kws...) = testtensor(N0f8, i; kws...)
        function testtensor(T::Type, i; dir = nothing)
            CIFAR100(; split = :test, Tx = T, dir)[i][1]
        end
        return testtensor
    elseif s == :trainlabels
        @warn "CIFAR100.trainlabels() is deprecated, use `CIFAR100(split=:train).targets` instead."
        trainlabels(; kws...) = trainlabels(:; kws...)
        function trainlabels(i; dir = nothing)
            yc, yf = CIFAR100(; split = :train, dir)[i][2]
            yc, yf
        end
        return trainlabels
    elseif s == :testlabels
        @warn "CIFAR100.testlabels() is deprecated, use `CIFAR100(split=:test).targets` instead."
        testlabels(; kws...) = testlabels(:; kws...)
        function testlabels(i; dir = nothing)
            yc, yf = CIFAR100(; split = :test, dir)[i][2]
            yc, yf
        end
        return testlabels
    elseif s == :traindata
        @warn "CIFAR100.traindata() is deprecated, use `CIFAR100(split=:train)[:]` instead."
        traindata(T::Type = N0f8; kws...) = traindata(T, :; kws...)
        traindata(i; kws...) = traindata(N0f8, i; kws...)
        function traindata(T::Type, i; dir = nothing)
            x, (yc, yf) = CIFAR100(; split = :train, Tx = T, dir)[i]
            x, yc, yf
        end
        return traindata
    elseif s == :testdata
        @warn "CIFAR100.testdata() is deprecated, use `CIFAR100(split=:test)[:]` instead."
        testdata(T::Type = N0f8; kws...) = testdata(T, :; kws...)
        testdata(i; kws...) = testdata(N0f8, i; kws...)
        function testdata(T::Type, i; dir = nothing)
            x, (yc, yf) = CIFAR100(; split = :test, Tx = T, dir)[i]
            x, yc, yf
        end
        return testdata
    elseif s == :convert2image
        @warn "CIFAR100.convert2image(x) is deprecated, use `convert2image(CIFAR100, x)` instead"
        return x -> convert2image(CIFAR100, x)
    elseif s == :classnames_fine
        @warn "CIFAR100.classnames_fine() is deprecated, use `CIFAR100().metadata[\"class_names_fine\"]` instead"
        return () -> CIFAR100().metadata["class_names_fine"]
    elseif s == :classnames_coarse
        @warn "CIFAR100.classnames_coarse() is deprecated, use `CIFAR100().metadata[\"class_names_coarse\"]` instead"
        return () -> CIFAR100().metadata["class_names_coarse"]
    else
        return getfield(CIFAR100, s)
    end
end

[.\src\datasets\vision\emnist.jl]

function __init__emnist()
    DEPNAME = "EMNIST"

    register(DataDep(DEPNAME,
                     """
                     Dataset: The EMNIST Dataset
                     Authors: Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre van Schaik
                     Website: https://www.nist.gov/itl/products-and-services/emnist-dataset

                     [Cohen et al., 2017]
                         Cohen, G., Afshar, S., Tapson, J., & van Schaik, A. (2017).
                         EMNIST: an extension of MNIST to handwritten letters.
                         Retrieved from http://arxiv.org/abs/1702.05373

                     The EMNIST dataset is a set of handwritten character digits derived from the
                     NIST Special Database 19 (https://www.nist.gov/srd/nist-special-database-19)
                     and converted to a 28x28 pixel image format and dataset structure that directly
                     matches the MNIST dataset (http://yann.lecun.com/exdb/mnist/). Further information
                     on the dataset contents and conversion process can be found in the paper available
                     at https://arxiv.org/abs/1702.05373v1.

                     The files are available for download at the official
                     website linked above. Note that using the data
                     responsibly and respecting copyright remains your
                     responsibility. For example the website mentions that
                     the data is for non-commercial use only. Please read
                     the website to make sure you want to download the
                     dataset.
                     """,
                     "http://www.itl.nist.gov/iaui/vip/cs_links/EMNIST/matlab.zip",
                     "e1fa805cdeae699a52da0b77c2db17f6feb77eed125f9b45c022e7990444df95",
                     post_fetch_method = DataDeps.unpack))
end

"""
    EMNIST(name; Tx=Float32, split=:train, dir=nothing)
    EMNIST(name, [Tx, split])

The EMNIST dataset is a set of handwritten character digits derived from the
NIST Special Database 19 (https://www.nist.gov/srd/nist-special-database-19)
and converted to a 28x28 pixel image format and dataset structure that directly
matches the MNIST dataset (http://yann.lecun.com/exdb/mnist/). Further information
on the dataset contents and conversion process can be found in the paper available
at https://arxiv.org/abs/1702.05373v1.

# Arguments

- `name`: name of the EMNIST dataset. Possible values are: `:balanced, :byclass, :bymerge, :digits, :letters, :mnist`.
- `split`: selects the data partition. Can take the values `:train` or `:test`. 
$ARGUMENTS_SUPERVISED_ARRAY

# Fields

- `name`.
- `split`.
$FIELDS_SUPERVISED_ARRAY

# Methods

$METHODS_SUPERVISED_ARRAY
- [`convert2image`](@ref) converts features to `Gray` images.

# Examples

The images are loaded as a multi-dimensional array of eltype `Tx`.
If `Tx <: Integer`, then all values will be within `0` and `255`, 
otherwise the values are scaled to be between `0` and `1`.
`EMNIST().features` is a 3D array (i.e. a `Array{Tx,3}`), in
WHN format (width, height, num_images). Labels are stored as
a vector of integers in `EMNIST().targets`. 

```julia-repl
julia> using MLDatasets: EMNIST

julia> dataset = EMNIST(:letters, split=:train)
EMNIST:
  metadata    =>    Dict{String, Any} with 3 entries
  split       =>    :train
  features    =>    28×28×60000 Array{Float32, 3}
  targets     =>    60000-element Vector{Int64}

julia> dataset[1:5].targets
5-element Vector{Int64}:
7
2
1
0
4

julia> X, y = dataset[:];

julia> dataset = EMNIST(:balanced, Tx=UInt8, split=:test)
EMNIST:
  metadata    =>    Dict{String, Any} with 3 entries
  split       =>    :test
  features    =>    28×28×10000 Array{UInt8, 3}
  targets     =>    10000-element Vector{Int64}
```
"""
struct EMNIST <: SupervisedDataset
    metadata::Dict{String, Any}
    name::Symbol
    split::Symbol
    features::Array{<:Any, 3}
    targets::Vector{Int}
end

EMNIST(name; split = :train, Tx = Float32, dir = nothing) = EMNIST(name, Tx, split; dir)
EMNIST(name, split::Symbol; kws...) = EMNIST(name; split, kws...)
EMNIST(name, Tx::Type; kws...) = EMNIST(name; Tx, kws...)

function EMNIST(name, Tx::Type, split::Symbol; dir = nothing)
    @assert split ∈ [:train, :test]
    @assert name ∈ [:balanced, :byclass, :bymerge, :digits, :letters, :mnist]
    path = "matlab/emnist-$name.mat"

    path = datafile("EMNIST", path, dir)
    vars = read_mat(path)
    features = reshape(vars["dataset"]["$split"]["images"], :, 28, 28)
    features = permutedims(features, (3, 2, 1))
    targets = Int.(vars["dataset"]["$split"]["labels"] |> vec)
    features = bytes_to_type(Tx, features)

    metadata = Dict{String, Any}()
    metadata["n_observations"] = size(features)[end]

    return EMNIST(metadata, name, split, features, targets)
end

convert2image(::Type{<:EMNIST}, x::AbstractArray) = convert2image(MNIST, x)

[.\src\datasets\vision\fashion_mnist.jl]
function __init__fashionmnist()
    DEPNAME = "FashionMNIST"
    TRAINIMAGES = "train-images-idx3-ubyte.gz"
    TRAINLABELS = "train-labels-idx1-ubyte.gz"
    TESTIMAGES = "t10k-images-idx3-ubyte.gz"
    TESTLABELS = "t10k-labels-idx1-ubyte.gz"

    register(DataDep(DEPNAME,
                     """
                     Dataset: FashionMNIST
                     Authors: Han Xiao, Kashif Rasul, Roland Vollgraf
                     Website: https://github.com/zalandoresearch/fashion-mnist
                     License: MIT

                     [Han Xiao et al. 2017]
                         Han Xiao, Kashif Rasul, and Roland Vollgraf.
                         "Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms."
                         arXiv:1708.07747

                     The files are available for download at the offical
                     website linked above. Note that using the data
                     responsibly and respecting copyright remains your
                     responsibility.
                     """,
                     "http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/" .*
                     [TRAINIMAGES, TRAINLABELS, TESTIMAGES, TESTLABELS],
                     "c916b6e00d3083643332b70f3c5c3543d3941334b802e252976893969ee6af67"))
end

"""
    FashionMNIST(; Tx=Float32, split=:train, dir=nothing)
    FashionMNIST([Tx, split])

FashionMNIST is a dataset of Zalando's article images consisting
of a training set of 60000 examples and a test set of 10000
examples. Each example is a 28x28 grayscale image, associated
with a label from 10 classes. It can serve as a drop-in
replacement for MNIST.

- Authors: Han Xiao, Kashif Rasul, Roland Vollgraf
- Website: https://github.com/zalandoresearch/fashion-mnist

See [`MNIST`](@ref) for details of the interface.
"""
struct FashionMNIST <: SupervisedDataset
    metadata::Dict{String, Any}
    split::Symbol
    features::Array{<:Any, 3}
    targets::Vector{Int}
end

FashionMNIST(; split = :train, Tx = Float32, dir = nothing) = FashionMNIST(Tx, split; dir)
FashionMNIST(split::Symbol; kws...) = FashionMNIST(; split, kws...)
FashionMNIST(Tx::Type; kws...) = FashionMNIST(; Tx, kws...)

function FashionMNIST(Tx, split::Symbol; dir = nothing)
    @assert split in [:train, :test]
    if split == :train
        IMAGESPATH = "train-images-idx3-ubyte.gz"
        LABELSPATH = "train-labels-idx1-ubyte.gz"
    else
        IMAGESPATH = "t10k-images-idx3-ubyte.gz"
        LABELSPATH = "t10k-labels-idx1-ubyte.gz"
    end

    features_path = datafile("FashionMNIST", IMAGESPATH, dir)
    features = bytes_to_type(Tx, MNISTReader.readimages(features_path))

    targets_path = datafile("FashionMNIST", LABELSPATH, dir)
    targets = Vector{Int}(MNISTReader.readlabels(targets_path))
    # targets = reshape(targets, 1, :) 

    metadata = Dict{String, Any}()
    metadata["n_observations"] = size(features)[end]
    metadata["features_path"] = features_path
    metadata["targets_path"] = targets_path
    metadata["class_names"] = [
        "T-Shirt",
        "Trouser",
        "Pullover",
        "Dress",
        "Coat",
        "Sandal",
        "Shirt",
        "Sneaker",
        "Bag",
        "Ankle boot",
    ]

    return FashionMNIST(metadata, split, features, targets)
end

convert2image(::Type{<:FashionMNIST}, x::AbstractArray) = convert2image(MNIST, x)

# DEPRECATED INTERFACE, REMOVE IN v0.7 (or 0.6.x)
function Base.getproperty(::Type{FashionMNIST}, s::Symbol)
    if s == :traintensor
        @warn "FashionMNIST.traintensor() is deprecated, use `FashionMNIST(split=:train).features` instead."
        traintensor(T::Type = N0f8; kws...) = traintensor(T, :; kws...)
        traintensor(i; kws...) = traintensor(N0f8, i; kws...)
        function traintensor(T::Type, i; dir = nothing)
            FashionMNIST(; split = :train, Tx = T, dir)[i][1]
        end
        return traintensor
    elseif s == :testtensor
        @warn "FashionMNIST.testtensor() is deprecated, use `FashionMNIST(split=:test).features` instead."
        testtensor(T::Type = N0f8; kws...) = testtensor(T, :; kws...)
        testtensor(i; kws...) = testtensor(N0f8, i; kws...)
        function testtensor(T::Type, i; dir = nothing)
            FashionMNIST(; split = :test, Tx = T, dir)[i][1]
        end
        return testtensor
    elseif s == :trainlabels
        @warn "FashionMNIST.trainlabels() is deprecated, use `FashionMNIST(split=:train).targets` instead."
        trainlabels(; kws...) = trainlabels(:; kws...)
        function trainlabels(i; dir = nothing)
            FashionMNIST(; split = :train, dir)[i][2]
        end
        return trainlabels
    elseif s == :testlabels
        @warn "FashionMNIST.testlabels() is deprecated, use `FashionMNIST(split=:test).targets` instead."
        testlabels(; kws...) = testlabels(:; kws...)
        function testlabels(i; dir = nothing)
            FashionMNIST(; split = :test, dir)[i][2]
        end
        return testlabels
    elseif s == :traindata
        @warn "FashionMNIST.traindata() is deprecated, use `FashionMNIST(split=:train)[:]` instead."
        traindata(T::Type = N0f8; kws...) = traindata(T, :; kws...)
        traindata(i; kws...) = traindata(N0f8, i; kws...)
        function traindata(T::Type, i; dir = nothing)
            FashionMNIST(; split = :train, Tx = T, dir)[i]
        end
        return traindata
    elseif s == :testdata
        @warn "FashionMNIST.testdata() is deprecated, use `FashionMNIST(split=:test)[:]` instead."
        testdata(T::Type = N0f8; kws...) = testdata(T, :; kws...)
        testdata(i; kws...) = testdata(N0f8, i; kws...)
        function testdata(T::Type, i; dir = nothing)
            FashionMNIST(; split = :test, Tx = T, dir)[i]
        end
        return testdata
    elseif s == :convert2image
        @warn "FashionMNIST.convert2image(x) is deprecated, use `convert2image(FashionMNIST, x)` instead."
        return x -> convert2image(FashionMNIST, x)
    elseif s == :classnames
        @warn "FashionMNIST.classnames() is deprecated, use `FashionMNIST().metadata[\"class_names\"]` instead."
        return () -> [
            "T-Shirt",
            "Trouser",
            "Pullover",
            "Dress",
            "Coat",
            "Sandal",
            "Shirt",
            "Sneaker",
            "Bag",
            "Ankle boot",
        ]
    else
        return getfield(FashionMNIST, s)
    end
end

[.\src\datasets\vision\mnist.jl]
function __init__mnist()
    DEPNAME = "MNIST"
    TRAINIMAGES = "train-images-idx3-ubyte.gz"
    TRAINLABELS = "train-labels-idx1-ubyte.gz"
    TESTIMAGES = "t10k-images-idx3-ubyte.gz"
    TESTLABELS = "t10k-labels-idx1-ubyte.gz"

    register(DataDep(DEPNAME,
                     """
                     Dataset: THE MNIST DATABASE of handwritten digits
                     Authors: Yann LeCun, Corinna Cortes, Christopher J.C. Burges
                     Website: http://yann.lecun.com/exdb/mnist/

                     [LeCun et al., 1998a]
                         Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner.
                         "Gradient-based learning applied to document recognition."
                         Proceedings of the IEEE, 86(11):2278-2324, November 1998

                     The files are available for download at the offical
                     website linked above. Note that using the data
                     responsibly and respecting copyright remains your
                     responsibility. The authors of MNIST aren't really
                     explicit about any terms of use, so please read the
                     website to make sure you want to download the
                     dataset.
                     """,
                     "https://ossci-datasets.s3.amazonaws.com/mnist/" .*
                     [TRAINIMAGES, TRAINLABELS, TESTIMAGES, TESTLABELS],
                     "0bb1d5775d852fc5bb32c76ca15a7eb4e9a3b1514a2493f7edfcf49b639d7975"
                     # post_fetch_method = DataDeps.unpack  # TODO should we unzip instead of reading directly from the zip?
                     ))
end

"""
    MNIST(; Tx=Float32, split=:train, dir=nothing)
    MNIST([Tx, split])

The MNIST database of handwritten digits.

- Authors: Yann LeCun, Corinna Cortes, Christopher J.C. Burges
- Website: http://yann.lecun.com/exdb/mnist/

MNIST is a classic image-classification dataset that is often
used in small-scale machine learning experiments. It contains
70,000 images of handwritten digits. Each observation is a 28x28
pixel gray-scale image that depicts a handwritten version of 1 of
the 10 possible digits (0-9).

# Arguments

$ARGUMENTS_SUPERVISED_ARRAY
- `split`: selects the data partition. Can take the values `:train` or `:test`. 

# Fields

$FIELDS_SUPERVISED_ARRAY
- `split`.

# Methods

$METHODS_SUPERVISED_ARRAY
- [`convert2image`](@ref) converts features to `Gray` images.

# Examples

The images are loaded as a multi-dimensional array of eltype `Tx`.
If `Tx <: Integer`, then all values will be within `0` and `255`, 
otherwise the values are scaled to be between `0` and `1`.
`MNIST().features` is a 3D array (i.e. a `Array{Tx,3}`), in
WHN format (width, height, num_images). Labels are stored as
a vector of integers in `MNIST().targets`. 

```julia-repl
julia> using MLDatasets: MNIST

julia> dataset = MNIST(:train)
MNIST:
  metadata    =>    Dict{String, Any} with 3 entries
  split       =>    :train
  features    =>    28×28×60000 Array{Float32, 3}
  targets     =>    60000-element Vector{Int64}

julia> dataset[1:5].targets
5-element Vector{Int64}:
7
2
1
0
4

julia> X, y = dataset[:];

julia> dataset = MNIST(UInt8, :test)
MNIST:
  metadata    =>    Dict{String, Any} with 3 entries
  split       =>    :test
  features    =>    28×28×10000 Array{UInt8, 3}
  targets     =>    10000-element Vector{Int64}
```
"""
struct MNIST <: SupervisedDataset
    metadata::Dict{String, Any}
    split::Symbol
    features::Array{<:Any, 3}
    targets::Vector{Int}
end

MNIST(; split = :train, Tx = Float32, dir = nothing) = MNIST(Tx, split; dir)
MNIST(split::Symbol; kws...) = MNIST(; split, kws...)
MNIST(Tx::Type; kws...) = MNIST(; Tx, kws...)

function MNIST(Tx::Type, split::Symbol; dir = nothing)
    @assert split in [:train, :test]
    if split === :train
        IMAGESPATH = "train-images-idx3-ubyte.gz"
        LABELSPATH = "train-labels-idx1-ubyte.gz"
    else
        IMAGESPATH = "t10k-images-idx3-ubyte.gz"
        LABELSPATH = "t10k-labels-idx1-ubyte.gz"
    end

    features_path = datafile("MNIST", IMAGESPATH, dir)
    features = bytes_to_type(Tx, MNISTReader.readimages(features_path))

    targets_path = datafile("MNIST", LABELSPATH, dir)
    targets = Vector{Int}(MNISTReader.readlabels(targets_path))
    # targets = reshape(targets, 1, :) 

    metadata = Dict{String, Any}()
    metadata["n_observations"] = size(features)[end]
    metadata["features_path"] = features_path
    metadata["targets_path"] = targets_path

    return MNIST(metadata, split, features, targets)
end

function convert2image(::Type{<:MNIST}, x::AbstractArray{<:Integer})
    convert2image(MNIST, reinterpret(N0f8, convert(Array{UInt8}, x)))
end

function convert2image(::Type{<:MNIST}, x::AbstractArray{T, N}) where {T, N}
    @assert N == 2 || N == 3
    x = permutedims(x, (2, 1, 3:N...))
    ImageCore = ImageShow.ImageCore
    return ImageCore.colorview(ImageCore.Gray, x)
end

# DEPRECATED INTERFACE, REMOVE IN v0.7 (or 0.6.x)
# This is has an hack to deprecate the old datasets-are-modules interface
# in favor of a datasets-are-types interface.
# Unfortunately overloading getproperty for a a type doesn't play well for
# parameterized types (e.g. typeof(dataset) hangs). 
# After we remove this deprecation path, we can parameterize the type like this:
# MNIST{Tx, A <: AbstractArray{Tx, 3}}
function Base.getproperty(::Type{MNIST}, s::Symbol)
    if s === :traintensor
        @warn "MNIST.traintensor() is deprecated, use `MNIST(split=:train).features` instead."
        traintensor(T::Type = N0f8; kws...) = traintensor(T, :; kws...)
        traintensor(i; kws...) = traintensor(N0f8, i; kws...)
        function traintensor(T::Type, i; dir = nothing)
            MNIST(; split = :train, Tx = T, dir)[i][1]
        end
        return traintensor
    elseif s === :testtensor
        @warn "MNIST.testtensor() is deprecated, use `MNIST(split=:test).features` instead."
        testtensor(T::Type = N0f8; kws...) = testtensor(T, :; kws...)
        testtensor(i; kws...) = testtensor(N0f8, i; kws...)
        function testtensor(T::Type, i; dir = nothing)
            MNIST(; split = :test, Tx = T, dir)[i][1]
        end
        return testtensor
    elseif s === :trainlabels
        @warn "MNIST.trainlabels() is deprecated, use `MNIST(split=:train).targets` instead."
        trainlabels(; kws...) = trainlabels(:; kws...)
        function trainlabels(i; dir = nothing)
            MNIST(; split = :train, dir)[i][2]
        end
        return trainlabels
    elseif s === :testlabels
        @warn "MNIST.testlabels() is deprecated, use `MNIST(split=:test).targets` instead."
        testlabels(; kws...) = testlabels(:; kws...)
        function testlabels(i; dir = nothing)
            MNIST(; split = :test, dir)[i][2]
        end
        return testlabels
    elseif s === :traindata
        @warn "MNIST.traindata() is deprecated, use `MNIST(split=:train)[:]` instead."
        traindata(T::Type = N0f8; kws...) = traindata(T, :; kws...)
        traindata(i; kws...) = traindata(N0f8, i; kws...)
        function traindata(T::Type, i; dir = nothing)
            MNIST(; split = :train, Tx = T, dir)[i]
        end
        return traindata
    elseif s === :testdata
        @warn "MNIST.testdata() is deprecated, use `MNIST(split=:test)[:]` instead."
        testdata(T::Type = N0f8; kws...) = testdata(T, :; kws...)
        testdata(i; kws...) = testdata(N0f8, i; kws...)
        function testdata(T::Type, i; dir = nothing)
            MNIST(; split = :test, Tx = T, dir)[i]
        end
        return testdata
    elseif s === :convert2image
        @warn "MNIST.convert2image(x) is deprecated, use `convert2image(MNIST, x)` instead"
        return x -> convert2image(MNIST, x)
    else
        return getfield(MNIST, s)
    end
end

[.\src\datasets\vision\omniglot.jl]
function __init__omniglot()
    DEPNAME = "Omniglot"
    TRAIN = "data_background.mat"
    TEST = "data_evaluation.mat"
    SMALL1 = "data_background_small1.mat"
    SMALL2 = "data_background_small2.mat"

    register(DataDep(DEPNAME,
                     """
                     Dataset: Omniglot data set for one-shot learning
                     Authors: Brenden M. Lake, Ruslan Salakhutdinov, Joshua B. Tenenbaum
                     Website: https://github.com/brendenlake/omniglot

                     [Lake et al., 2015]
                         Lake, B. M., Salakhutdinov, R., and Tenenbaum, J. B. (2015).
                         Human-level concept learning through probabilistic program induction.
                         Science, 350(6266), 1332-1338.

                     The files are available for download at the official
                     github repository linked above. Note that using the data
                     responsibly and respecting copyright remains your
                     responsibility. The authors of Omniglot aren't really
                     explicit about any terms of use, so please read the
                     website to make sure you want to download the dataset.
                     """,
                     "https://github.com/brendenlake/omniglot/raw/master/matlab/" .*
                     [TRAIN, TEST, SMALL1, SMALL2],
                     "1cfb52d931d794a3dd2717da6c80ddb8f48b0a6f559916c60fcdcd908b65d3d8"))
end

"""
    Omniglot(; Tx=Float32, split=:train, dir=nothing)
    Omniglot([Tx, split])

Omniglot data set for one-shot learning

- Authors: Brenden M. Lake, Ruslan Salakhutdinov, Joshua B. Tenenbaum
- Website: https://github.com/brendenlake/omniglot

The Omniglot data set is designed for developing more human-like learning
algorithms. It contains 1623 different handwritten characters from 50 different
alphabets. Each of the 1623 characters was drawn online via Amazon's
Mechanical Turk by 20 different people. Each image is paired with stroke data, a
sequences of [x,y,t] coordinates with time (t) in milliseconds.

# Arguments

$ARGUMENTS_SUPERVISED_ARRAY
- `split`: selects the data partition. Can take the values `:train`, `:test`, `:small1`, or `:small2`. 

# Fields

$FIELDS_SUPERVISED_ARRAY
- `split`.

# Methods

$METHODS_SUPERVISED_ARRAY
- [`convert2image`](@ref) converts features to `Gray` images.

# Examples

The images are loaded as a multi-dimensional array of eltype `Tx`.
All values will be `0` or `1`. `Omniglot().features` is a 3D array
(i.e. a `Array{Tx,3}`), in WHN format (width, height, num_images).
Labels are stored as a vector of strings in `Omniglot().targets`. 

```julia-repl
julia> using MLDatasets: Omniglot

julia> dataset = Omniglot(:train)
Omniglot:
  metadata    =>    Dict{String, Any} with 3 entries
  split       =>    :train
  features    =>    105×105×19280 Array{Float32, 3}
  targets     =>    19280-element Vector{Int64}

julia> dataset[1:5].targets
5-element Vector{String}:
 "Arcadian"
 "Arcadian"
 "Arcadian"
 "Arcadian"
 "Arcadian"

julia> X, y = dataset[:];

julia> dataset = Omniglot(UInt8, :test)
Omniglot:
  metadata    =>    Dict{String, Any} with 3 entries
  split       =>    :test
  features    =>    105×105×13180 Array{UInt8, 3}
  targets     =>    13180-element Vector{Int64}
```
"""
struct Omniglot <: SupervisedDataset
    metadata::Dict{String, Any}
    split::Symbol
    features::Array{<:Any, 3}
    targets::Vector{String}
end

Omniglot(; split = :train, Tx = Float32, dir = nothing) = Omniglot(Tx, split; dir)
Omniglot(split::Symbol; kws...) = Omniglot(; split, kws...)
Omniglot(Tx::Type; kws...) = Omniglot(; Tx, kws...)

function Omniglot(Tx::Type, split::Symbol; dir = nothing)
    @assert split ∈ [:train, :test, :small1, :small2]
    if split === :train
        IMAGESPATH = "data_background.mat"
    elseif split === :test
        IMAGESPATH = "data_evaluation.mat"
    elseif split === :small1
        IMAGESPATH = "data_background_small1.mat"
    elseif split === :small2
        IMAGESPATH = "data_background_small2.mat"
    end

    filename = datafile("Omniglot", IMAGESPATH, dir)

    file = MAT.matopen(filename)
    images = MAT.read(file, "images")
    names = MAT.read(file, "names")
    MAT.close(file)

    image_count = 0
    for alphabet in images
        for character in alphabet
            image_count += length(character)
        end
    end

    features = Array{Tx}(undef, 105, 105, image_count)
    targets = Vector{String}(undef, image_count)

    curr_idx = 1
    for i in range(1, length(images))
        alphabet = images[i]
        for character in alphabet
            for variation in character
                targets[curr_idx] = names[i]
                features[:, :, curr_idx] = variation
                curr_idx += 1
            end
        end
    end

    metadata = Dict{String, Any}()
    metadata["n_observations"] = size(features)[end]
    metadata["features_path"] = IMAGESPATH
    metadata["targets_path"] = IMAGESPATH

    return Omniglot(metadata, split, features, targets)
end

function convert2image(::Type{<:Omniglot}, x::AbstractArray{<:Integer})
    convert2image(Omniglot, reinterpret(N0f8, convert(Array{UInt8}, x)))
end

function convert2image(::Type{<:Omniglot}, x::AbstractArray{T, N}) where {T, N}
    @assert N == 2 || N == 3
    x = permutedims(x, (2, 1, 3:N...))
    ImageCore = ImageShow.ImageCore
    return ImageCore.colorview(ImageCore.Gray, x)
end

[.\src\datasets\vision\svhn2.jl]

function __init__svhn2()
    DEPNAME = "SVHN2"
    TRAINDATA = "train_32x32.mat"
    TESTDATA = "test_32x32.mat"
    EXTRADATA = "extra_32x32.mat"

    register(DataDep(DEPNAME,
                     """
                     Dataset: The Street View House Numbers (SVHN) Dataset
                     Authors: Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, Andrew Y. Ng
                     Website: http://ufldl.stanford.edu/housenumbers
                     Format: Cropped Digits (Format 2 on the website)
                     Note: for non-commercial use only

                     [Netzer et al., 2011]
                         Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, Andrew Y. Ng
                         "Reading Digits in Natural Images with Unsupervised Feature Learning"
                         NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011

                     The dataset is split up into three subsets: 73257
                     digits for training, 26032 digits for testing, and
                     531131 additional to use as extra training data.

                     The files are available for download at the official
                     website linked above. Note that using the data
                     responsibly and respecting copyright remains your
                     responsibility. For example the website mentions that
                     the data is for non-commercial use only. Please read
                     the website to make sure you want to download the
                     dataset.
                     """,
                     "http://ufldl.stanford.edu/housenumbers/" .*
                     [TRAINDATA, TESTDATA, EXTRADATA],
                     "2fa3b0b79baf39de36ed7579e6947760e6241f4c52b6b406cabc44d654c13a50"))
end

"""
    SVHN2(; Tx=Float32, split=:train, dir=nothing)
    SVHN2([Tx, split])

The Street View House Numbers (SVHN) Dataset.

- Authors: Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, Andrew Y. Ng
- Website: http://ufldl.stanford.edu/housenumbers

SVHN was obtained from house numbers in Google Street View
images. As such they are quite diverse in terms of orientation
and image background. Similar to MNIST, SVHN has 10 classes (the
digits 0-9), but unlike MNIST there is more data and the images
are a little bigger (32x32 instead of 28x28) with an additional
RGB color channel. The dataset is split up into three subsets:
73257 digits for training, 26032 digits for testing, and 531131
additional to use as extra training data.

# Arguments

$ARGUMENTS_SUPERVISED_ARRAY
- `split`: selects the data partition. Can take the values `:train`, `:test` or `:extra`. 

# Fields

$FIELDS_SUPERVISED_ARRAY
- `split`.

# Methods

$METHODS_SUPERVISED_ARRAY
- [`convert2image`](@ref) converts features to `RGB` images.

# Examples

```julia-repl
julia> using MLDatasets: SVHN2

julia> using MLDatasets: SVHN2

julia> dataset = SVHN2()
SVHN2:
  metadata    =>    Dict{String, Any} with 2 entries
  split       =>    :train
  features    =>    32×32×3×73257 Array{Float32, 4}
  targets     =>    73257-element Vector{Int64}

julia> dataset[1:5].targets
5-element Vector{Int64}:
 1
 9
 2
 3
 2

julia> dataset.metadata
Dict{String, Any} with 2 entries:
  "n_observations" => 73257
  "class_names"    => ["1", "2", "3", "4", "5", "6", "7", "8", "9", "0"]
```
"""
struct SVHN2 <: SupervisedDataset
    metadata::Dict{String, Any}
    split::Symbol
    features::Array{<:Any, 4}
    targets::Vector{Int}
end

SVHN2(; split = :train, Tx = Float32, dir = nothing) = SVHN2(Tx, split; dir)
SVHN2(split::Symbol; kws...) = SVHN2(; split, kws...)
SVHN2(Tx::Type; kws...) = SVHN2(; Tx, kws...)

function SVHN2(Tx::Type, split::Symbol; dir = nothing)
    DEPNAME = "SVHN2"
    TRAINDATA = "train_32x32.mat"
    TESTDATA = "test_32x32.mat"
    EXTRADATA = "extra_32x32.mat"
    CLASSES = [1, 2, 3, 4, 5, 6, 7, 8, 9, 0]
    @assert split ∈ [:train, :test, :extra]
    if split == :train
        PATH = TRAINDATA
    elseif split == :test
        PATH = TESTDATA
    else
        PATH = EXTRADATA
    end

    path = datafile(DEPNAME, PATH, dir)
    vars = read_mat(path)
    images = vars["X"]::Array{UInt8, 4}
    labels = vars["y"]
    images = permutedims(images, (2, 1, 3, 4))
    features = bytes_to_type(Tx, images)
    targets = Vector{Int}(vec(labels))

    metadata = Dict{String, Any}()
    metadata["n_observations"] = size(features)[end]
    metadata["class_names"] = string.(CLASSES)

    return SVHN2(metadata, split, features, targets)
end

convert2image(::Type{<:SVHN2}, x) = convert2image(CIFAR10, x)

# DEPRECATED INTERFACE, REMOVE IN v0.7 (or 0.6.x)
function Base.getproperty(::Type{SVHN2}, s::Symbol)
    if s == :traintensor
        @warn "SVHN2.traintensor() is deprecated, use `SVHN2(split=:train).features` instead."
        traintensor(T::Type = N0f8; kws...) = traintensor(T, :; kws...)
        traintensor(i; kws...) = traintensor(N0f8, i; kws...)
        function traintensor(T::Type, i; dir = nothing)
            SVHN2(; split = :train, Tx = T, dir)[i][1]
        end
        return traintensor
    elseif s == :testtensor
        @warn "SVHN2.testtensor() is deprecated, use `SVHN2(split=:test).features` instead."
        testtensor(T::Type = N0f8; kws...) = testtensor(T, :; kws...)
        testtensor(i; kws...) = testtensor(N0f8, i; kws...)
        function testtensor(T::Type, i; dir = nothing)
            SVHN2(; split = :test, Tx = T, dir)[i][1]
        end
        return testtensor
    elseif s == :extratensor
        @warn "SVHN2.extratensor() is deprecated, use `SVHN2(split=:test)[:]` instead."
        extratensor(T::Type = N0f8; kws...) = extratensor(T, :; kws...)
        extratensor(i; kws...) = extratensor(N0f8, i; kws...)
        function extratensor(T::Type, i; dir = nothing)
            SVHN2(; split = :extra, Tx = T, dir)[i][1]
        end
        return extratensor
    elseif s == :trainlabels
        @warn "SVHN2.trainlabels() is deprecated, use `SVHN2(split=:train).targets` instead."
        trainlabels(; kws...) = trainlabels(:; kws...)
        function trainlabels(i; dir = nothing)
            SVHN2(; split = :train, dir)[i][2]
        end
        return trainlabels
    elseif s == :testlabels
        @warn "SVHN2.testlabels() is deprecated, use `SVHN2(split=:test).targets` instead."
        testlabels(; kws...) = testlabels(:; kws...)
        function testlabels(i; dir = nothing)
            SVHN2(; split = :test, dir)[i][2]
        end
        return testlabels
    elseif s == :extralabels
        @warn "SVHN2.testdata() is deprecated, use `SVHN2(split=:test)[:]` instead."
        extralabels(T::Type = N0f8; kws...) = extralabels(T, :; kws...)
        extralabels(i; kws...) = extralabels(N0f8, i; kws...)
        function extralabels(T::Type, i; dir = nothing)
            SVHN2(; split = :extra, Tx = T, dir)[i][2]
        end
        return extralabels
    elseif s == :traindata
        @warn "SVHN2.traindata() is deprecated, use `SVHN2(split=:train)[:]` instead."
        traindata(T::Type = N0f8; kws...) = traindata(T, :; kws...)
        traindata(i; kws...) = traindata(N0f8, i; kws...)
        function traindata(T::Type, i; dir = nothing)
            SVHN2(; split = :train, Tx = T, dir)[i]
        end
        return traindata
    elseif s == :testdata
        @warn "SVHN2.testdata() is deprecated, use `SVHN2(split=:test)[:]` instead."
        testdata(T::Type = N0f8; kws...) = testdata(T, :; kws...)
        testdata(i; kws...) = testdata(N0f8, i; kws...)
        function testdata(T::Type, i; dir = nothing)
            SVHN2(; split = :test, Tx = T, dir)[i]
        end
        return testdata
    elseif s == :extradata
        @warn "SVHN2.testdata() is deprecated, use `SVHN2(split=:test)[:]` instead."
        extradata(T::Type = N0f8; kws...) = extradata(T, :; kws...)
        extradata(i; kws...) = extradata(N0f8, i; kws...)
        function extradata(T::Type, i; dir = nothing)
            SVHN2(; split = :extra, Tx = T, dir)[i]
        end
        return extradata
    elseif s == :convert2image
        @warn "SVHN2.convert2image(x) is deprecated, use `convert2image(SVHN2, x)` instead"
        return x -> convert2image(SVHN2, x)
    elseif s == :classnames
        @warn "SVHN2.classnames() is deprecated, use `SVHN2().metadata[\"class_names\"]` instead"
        return () -> parse.(Int, SVHN2().metadata["class_names"])

    else
        return getfield(SVHN2, s)
    end
end

[.\src\datasets\vision\cifar100_reader\CIFAR100Reader.jl]
module CIFAR100Reader

export
       readdata!,
       readdata

const NROW = 32
const NCOL = 32
const NCHAN = 3
const NBYTE = NROW * NCOL * NCHAN + 2 # "+ 2" for label

function readnext!(buffer::Array{UInt8}, io::IO)
    c = Int(read(io, UInt8))
    f = Int(read(io, UInt8))
    read!(io, buffer)
    buffer, c, f
end

function readdata!(buffer::Array{UInt8}, io::IO, index::Integer)
    seek(io, (index - 1) * NBYTE)
    readnext!(buffer, io)
end

function readdata(io::IO, nobs::Int, index::Integer)
    buffer = Array{UInt8}(undef, NROW, NCOL, NCHAN)
    readdata!(buffer, io, index)
end

function readdata(io::IO, nobs::Int)
    X = Array{UInt8}(undef, NROW, NCOL, NCHAN, nobs)
    C = Array{Int}(undef, nobs)
    F = Array{Int}(undef, nobs)
    buffer = Array{UInt8}(undef, NROW, NCOL, NCHAN)
    @inbounds for index in 1:nobs
        _, tc, tf = readnext!(buffer, io)
        copyto!(view(X, :, :, :, index), buffer)
        C[index] = tc
        F[index] = tf
    end
    X, C, F
end

function readdata(file::AbstractString, nobs::Int, index::Integer)
    open(file, "r") do io
        readdata(io, nobs, index)
    end::Tuple{Array{UInt8, 3}, Int, Int}
end

function readdata(file::AbstractString, nobs::Int)
    open(file, "r") do io
        readdata(io, nobs)
    end::Tuple{Array{UInt8, 4}, Vector{Int}, Vector{Int}}
end

end

[.\src\datasets\vision\cifar10_reader\CIFAR10Reader.jl]
module CIFAR10Reader

export
       readdata!,
       readdata

const NROW = 32
const NCOL = 32
const NCHAN = 3
const NBYTE = NROW * NCOL * NCHAN + 1 # "+ 1" for label
const CHUNK_SIZE = 10_000

function readnext!(buffer::Array{UInt8}, io::IO)
    y = Int(read(io, UInt8))
    read!(io, buffer)
    buffer, y
end

function readdata!(buffer::Array{UInt8}, io::IO, index::Integer)
    seek(io, (index - 1) * NBYTE)
    readnext!(buffer, io)
end

function readdata(io::IO, index::Integer)
    buffer = Array{UInt8}(undef, NROW, NCOL, NCHAN)
    readdata!(buffer, io, index)
end

function readdata(io::IO)
    X = Array{UInt8}(undef, NROW, NCOL, NCHAN, CHUNK_SIZE)
    Y = Array{Int}(undef, CHUNK_SIZE)
    buffer = Array{UInt8}(undef, NROW, NCOL, NCHAN)
    @inbounds for index in 1:CHUNK_SIZE
        _, ty = readnext!(buffer, io)
        copyto!(view(X, :, :, :, index), buffer)
        Y[index] = ty
    end
    X, Y
end

function readdata(file::AbstractString, index::Integer)
    open(file, "r") do io
        readdata(io, index)
    end::Tuple{Array{UInt8, 3}, Int}
end

function readdata(file::AbstractString)
    open(file, "r") do io
        readdata(io)
    end::Tuple{Array{UInt8, 4}, Vector{Int}}
end

end

[.\src\datasets\vision\mnist_reader\MNISTReader.jl]
module MNISTReader
using GZip

export
       readimages,
       readlabels

# Constants

const IMAGEOFFSET = 16
const LABELOFFSET = 8

# Includes

include("readheader.jl")
include("readimages.jl")
include("readlabels.jl")
end

[.\src\datasets\vision\mnist_reader\readheader.jl]
# """
#     readimageheader(io::IO)

# Reads four 32 bit integers at the current position of `io` and
# interprets them as a MNIST-image-file header, which is described
# in detail in the table below

#             ║     First    │  Second  │  Third  │   Fourth
#     ════════╬══════════════╪══════════╪═════════╪════════════
#     offset  ║         0000 │     0004 │    0008 │       0012
#     descr   ║ magic number │ # images │  # rows │  # columns

# These four numbers are returned as a Tuple in the same storage order
# """
function readimageheader(io::IO)
    magic_number = bswap(read(io, UInt32))
    total_items = bswap(read(io, UInt32))
    nrows = bswap(read(io, UInt32))
    ncols = bswap(read(io, UInt32))
    UInt32(magic_number), Int(total_items), Int(nrows), Int(ncols)
end

# """
#     readimageheader(file::AbstractString)

# Opens and reads the first four 32 bits values of `file` and
# returns them interpreted as an MNIST-image-file header
# """
function readimageheader(file::AbstractString)
    gzopen(readimageheader, file, "r")::Tuple{UInt32, Int, Int, Int}
end

# """
#     readlabelheader(io::IO)

# Reads two 32 bit integers at the current position of `io` and
# interprets them as a MNIST-label-file header, which consists of a
# *magic number* and the *total number of labels* stored in the
# file. These two numbers are returned as a Tuple in the same
# storage order.
# """
function readlabelheader(io::IO)
    magic_number = bswap(read(io, UInt32))
    total_items = bswap(read(io, UInt32))
    UInt32(magic_number), Int(total_items)
end

# """
#     readlabelheader(file::AbstractString)

# Opens and reads the first two 32 bits values of `file` and
# returns them interpreted as an MNIST-label-file header
# """
function readlabelheader(file::AbstractString)
    gzopen(readlabelheader, file, "r")::Tuple{UInt32, Int}
end

[.\src\datasets\vision\mnist_reader\readimages.jl]
function readimages!(buffer::Matrix{UInt8}, io::IO, index::Integer, nrows::Integer,
                     ncols::Integer)
    seek(io, IMAGEOFFSET + nrows * ncols * (index - 1))
    read!(io, buffer)
end

# """
#     readimages(io::IO, index::Integer, nrows::Integer, ncols::Integer)

# Jumps to the position of `io` where the bytes for the `index`'th
# image are located and reads the next `nrows` * `ncols` bytes. The
# read bytes are returned as a `Matrix{UInt8}` of size `(nrows, ncols)`.
# """
function readimages(io::IO, index::Integer, nrows::Integer, ncols::Integer)
    buffer = Array{UInt8}(undef, nrows, ncols)
    readimages!(buffer, io, index, nrows, ncols)
end

# """
#     readimages(io::IO, indices::AbstractVector, nrows::Integer, ncols::Integer)

# Reads the first `nrows` * `ncols` bytes for each image index in
# `indices` and stores them in a `Array{UInt8,3}` of size `(nrows,
# ncols, length(indices))` in the same order as denoted by
# `indices`.
# """
function readimages(io::IO, indices::AbstractVector, nrows::Integer, ncols::Integer)
    images = Array{UInt8}(undef, nrows, ncols, length(indices))
    buffer = Array{UInt8}(undef, nrows, ncols)
    dst_index = 1
    for src_index in indices
        readimages!(buffer, io, src_index, nrows, ncols)
        copyto!(images, 1 + nrows * ncols * (dst_index - 1), buffer, 1, nrows * ncols)
        dst_index += 1
    end
    images
end

# """
#     readimages(file, [indices])

# Reads the images denoted by `indices` from `file`. The given
# `file` can either be specified using an IO-stream or a string
# that denotes the fully qualified path. The conent of `file` is
# assumed to be in the MNIST image-file format, as it is described
# on the official homepage at http://yann.lecun.com/exdb/mnist/

# - if `indices` is an `Integer`, the single image is returned as
#   `Matrix{UInt8}` in horizontal major layout, which means that
#   the first dimension denotes the pixel *rows* (x), and the
#   second dimension denotes the pixel *columns* (y) of the image.

# - if `indices` is a `AbstractVector`, the images are returned as
#   a 3D array (i.e. a `Array{UInt8,3}`), in which the first
#   dimension corresponds to the pixel *rows* (x) of the image, the
#   second dimension to the pixel *columns* (y) of the image, and
#   the third dimension denotes the index of the image.

# - if `indices` is ommited all images are returned
#   (as 3D array described above)
# """
function readimages(io::IO, indices)
    _, nimages, nrows, ncols = readimageheader(io)
    @assert minimum(indices) >= 1 && maximum(indices) <= nimages
    readimages(io, indices, nrows, ncols)
end

function readimages(file::AbstractString, index::Integer)
    gzopen(file, "r") do io
        readimages(io, index)
    end::Matrix{UInt8}
end

function readimages(file::AbstractString, indices::AbstractVector)
    gzopen(file, "r") do io
        readimages(io, indices)
    end::Array{UInt8, 3}
end

function readimages(file::AbstractString)
    gzopen(file, "r") do io
        _, nimages, nrows, ncols = readimageheader(io)
        readimages(io, 1:nimages, nrows, ncols)
    end::Array{UInt8, 3}
end

[.\src\datasets\vision\mnist_reader\readlabels.jl]
# """
#     readlabels(io::IO, index::Integer)

# Jumps to the position of `io` where the byte for the `index`'th
# label is located and returns the byte at that position as `UInt8`
# """
function readlabels(io::IO, index::Integer)
    seek(io, LABELOFFSET + (index - 1))
    read(io, UInt8)::UInt8
end

# """
#     readlabels(io::IO, indices::AbstractVector)

# Reads the byte for each label-index in `indices` and stores them
# in a `Vector{UInt8}` of length `length(indices)` in the same order
# as denoted by `indices`.
# """
function readlabels(io::IO, indices::AbstractVector)
    labels = Array{UInt8}(undef, length(indices))
    dst_index = 1
    for src_index in indices
        labels[dst_index] = readlabels(io, src_index)
        dst_index += 1
    end
    labels::Vector{UInt8}
end

# """
#     readlabels(file::AbstractString, [indices])

# Reads the label denoted by `indices` from `file`. The given `file`
# is assumed to be in the MNIST label-file format, as it is described
# on the official homepage at http://yann.lecun.com/exdb/mnist/

# - if `indices` is an `Integer`, the single label is returned as `UInt8`.

# - if `indices` is a `AbstractVector`, the labels are returned as
#   a `Vector{UInt8}`, length `length(indices)` in the same order as
#   denoted by `indices`.

# - if `indices` is ommited all all are returned
#   (as `Vector{UInt8}` as described above)
# """
function readlabels(file::AbstractString, index::Integer)
    gzopen(file, "r") do io
        _, nlabels = readlabelheader(io)
        @assert minimum(index) >= 1 && maximum(index) <= nlabels
        readlabels(io, index)
    end::UInt8
end

function readlabels(file::AbstractString, indices::AbstractVector)
    gzopen(file, "r") do io
        _, nlabels = readlabelheader(io)
        @assert minimum(indices) >= 1 && maximum(indices) <= nlabels
        readlabels(io, indices)
    end::Vector{UInt8}
end

function readlabels(file::AbstractString)
    gzopen(file, "r") do io
        _, nlabels = readlabelheader(io)
        readlabels(io, 1:nlabels)
    end::Vector{UInt8}
end

[.\test\runtests.jl]
using Test
using MLDatasets
using MLDatasets: SupervisedDataset, UnsupervisedDataset, AbstractDataset
using MLDatasets: Graph
using DataFrames, Tables, CSV
using ImageShow
using ColorTypes
using FixedPointNumbers
using JSON3
using Pickle

ENV["DATADEPS_ALWAYS_ACCEPT"] = true

include("test_utils.jl")

dataset_tests = [
    "datasets/graphs.jl",
    "datasets/misc.jl",
    "datasets/text.jl",
    "datasets/vision/fashion_mnist.jl",
    "datasets/vision/mnist.jl",
]

no_ci_dataset_tests = [
    "datasets/graphs_no_ci.jl",
    "datasets/text_no_ci.jl",
    "datasets/vision/cifar10.jl",
    "datasets/vision/cifar100.jl",
    "datasets/vision/emnist.jl",
    "datasets/vision/omniglot.jl",
    "datasets/vision/svhn2.jl",
    "datasets/meshes.jl",
]

@assert isempty(intersect(dataset_tests, no_ci_dataset_tests))

container_tests = [
    "containers/filedataset.jl",
    # "containers/tabledataset.jl",
    # "containers/hdf5dataset.jl",
    # "containers/jld2dataset.jl",
    "containers/cacheddataset.jl",
]

@testset "Datasets" begin
    @testset "$(split(t,"/")[end])" for t in dataset_tests
        include(t)
    end

    if !parse(Bool, get(ENV, "CI", "false"))
        @info "Testing larger datasets"
        @testset "$(split(t,"/")[end])" for t in no_ci_dataset_tests
            include(t)
        end
    else
        @info "CI detected: skipping tests on large datasets"
    end
end

@testset "Containers" begin for t in container_tests
    include(t)
end end

nothing

[.\test\test_utils.jl]

### TRAITS CHECKS ###########
# see https://github.com/JuliaML/MLUtils.jl/issues/67
numobs_df(x) = numobs(x)
getobs_df(x, i) = getobs(x, i)
getobs_df(x::NamedTuple, i) = map(y -> getobs_df(y, i), x)
getobs_df(x::DataFrame, i) = x[i, :]
numobs_df(x::DataFrame) = size(x, 1)

function test_inmemory_supervised_table_dataset(d::D;
                                                n_obs, n_features, n_targets,
                                                feature_names = nothing,
                                                target_names = nothing,
                                                Tx = Any,
                                                Ty = Any) where {D <: SupervisedDataset}
    @test hasfield(D, :metadata)
    @test hasfield(D, :features)
    @test hasfield(D, :targets)
    @test hasfield(D, :dataframe)
    @test d.metadata isa Dict{String, Any}

    as_df = d.features isa DataFrame

    if as_df
        @test d.features isa DataFrame
        @test d.targets isa DataFrame
        @test d.dataframe isa DataFrame
        @test isempty(intersect(names(d.features), names(d.targets)))
        @test size(d.dataframe) == (n_obs, n_features + n_targets)
        @test size(d.features) == (n_obs, n_features)
        @test size(d.targets) == (n_obs, n_targets)

        # check that dataframe shares the same storage of features and targets
        for c in names(d.dataframe)
            if c in names(d.targets)
                @test d.dataframe[!, c] === d.targets[!, c]
            else
                @test d.dataframe[!, c] === d.features[!, c]
            end
        end

        if feature_names !== nothing
            @test names(d.features) == feature_names
        end
        if target_names !== nothing
            @test names(d.targets) == target_names
        end
    else
        @test d.features isa Matrix{<:Tx}
        @test d.targets isa Matrix{<:Ty}
        @test d.dataframe === nothing
        @test size(d.features) == (n_features, n_obs)
        @test size(d.targets) == (n_targets, n_obs)

        if feature_names !== nothing
            @test d.metadata["feature_names"] == feature_names
        end
        if target_names !== nothing
            @test d.metadata["target_names"] == target_names
        end
    end

    @test d[:] === (; d.features, d.targets)
    @test length(d) == n_obs
    @test numobs(d) == n_obs
    idx = rand(1:n_obs)
    @test isequal(d[idx], getobs_df((; d.features, d.targets), idx))
    @test isequal(d[idx], getobs(d, idx))
    idxs = rand(1:n_obs, 2)
    @test isequal(d[idxs], getobs_df((; d.features, d.targets), idxs))
    @test isequal(d[idxs], getobs(d, idxs))
end

function test_supervised_array_dataset(d::D;
                                       n_obs, n_features, n_targets,
                                       Tx = Any, Ty = Any,
                                       conv2img = false) where {D <: SupervisedDataset}
    if n_features isa Int
        @assert n_features!=0 "use n_features=() if you don't want features dimensions"
        Nx == 2
    else # tuple
        Nx = length(n_features) + 1
    end
    Ny = map(x -> x == 1 ? 1 : 2, n_targets)

    @test d.features isa Array{Tx, Nx}
    @test size(d.features) == (n_features..., n_obs)

    if Ny isa NamedTuple
        for k in keys(Ny)
            ny = Ny[k]
            @test d.targets[k] isa Array{Ty, ny}
            if ny == 1
                @test size(d.targets[k]) == (n_obs,)
            else
                @test size(d.targets[k]) == (ny, n_obs)
            end
        end
    else
        @assert Ny isa Int
        if Ny == 1
            @test size(d.targets) == (n_obs,)
        else
            @test size(d.targets) == (Ny, n_obs)
        end
    end

    @test length(d) == n_obs
    @test numobs(d) == n_obs
    X, y = d[:]
    @test X === d.features
    @test y === d.targets

    idx = rand(1:n_obs)
    @test isequal(d[idx], getobs((; d.features, d.targets), idx))
    @test isequal(d[idx], getobs(d, idx))
    idxs = rand(1:n_obs, 2)
    @test isequal(d[idxs], getobs((; d.features, d.targets), idxs))
    @test isequal(d[idxs], getobs(d, idxs))

    if conv2img
        img = convert2image(d, 1)
        @test img isa AbstractArray{<:Color}
        x = d[1].features
        @test convert2image(D, x) == img
        @test convert2image(d, x) == img
    end
end

function test_unsupervised_array_dataset(d::D;
                                         n_obs, n_features,
                                         Tx = Any,
                                         conv2img = false) where {D <: UnsupervisedDataset}
    n_features = n_features === nothing ? () : n_features
    if n_features isa Int
        @assert n_features!=0 "use n_features = () if you don't want features dimensions"
        Nx == 2
    else # tuple
        Nx = length(n_features) + 1
    end

    @test d.features isa Array{Tx, Nx}
    @test size(d.features) == (n_features..., n_obs)

    @test length(d) == n_obs
    @test numobs(d) == n_obs
    X = d[:]
    @test X === d.features

    idx = rand(1:n_obs)
    @test isequal(d[idx], getobs(d.features, idx))
    @test isequal(d[idx], getobs(d, idx))
    idxs = rand(1:n_obs, 2)
    @test isequal(d[idxs], getobs(d.features, idxs))
    @test isequal(d[idxs], getobs(d, idxs))

    if conv2img
        img = convert2image(d, 1)
        @test img isa AbstractArray{<:Color}
        x = d[1].features
        @test convert2image(D, x) == img
        @test convert2image(d, x) == img
    end
end

[.\test\containers\cacheddataset.jl]
@testset "CachedDataset" begin
    @testset "CachedDataset(::FileDataset)" begin
        files = setup_filedataset_test()
        fdataset = FileDataset(f -> CSV.read(f, DataFrame), "root", "*.csv")
        cdataset = CachedDataset(fdataset)

        @test numobs(cdataset) == numobs(fdataset)
        @test cdataset.cache == getobs(fdataset, 1:numobs(fdataset))
        @test all(getobs(cdataset, i) == getobs(fdataset, i) for i in 1:numobs(fdataset))

        cleanup_filedataset_test()
    end

    # TODO add back when reinstating HDF5Dataset and JLD2Dataset
    # @testset "CachedDataset(::HDF5Dataset)" begin
    #     paths, datas = setup_hdf5dataset_test()
    #     hdataset = HDF5Dataset("test.h5", "d1")
    #     cdataset = CachedDataset(hdataset, 5)

    #     @test numobs(cdataset) == numobs(hdataset)
    #     @test cdataset.cache isa Matrix{Float64}
    #     @test cdataset.cache == getobs(hdataset, 1:5)
    #     @test all(getobs(cdataset, i) == getobs(hdataset, i) for i in 1:10)

    #     close(hdataset)
    #     cleanup_hdf5dataset_test()
    # end

    # @testset "CachedDataset(::JLD2Dataset)" begin
    #     paths, datas = setup_jld2dataset_test()
    #     jdataset = JLD2Dataset("test.jld2", "d1")
    #     cdataset = CachedDataset(jdataset, 5)

    #     @test numobs(cdataset) == numobs(jdataset)
    #     @test cdataset.cache isa Matrix{Float64}
    #     @test cdataset.cache == getobs(jdataset, 1:5)
    #     @test all(@inferred(getobs(cdataset, i)) == getobs(jdataset, i) for i in 1:10)

    #     close(jdataset)
    #     cleanup_jld2dataset_test()
    # end
end

[.\test\containers\filedataset.jl]
function setup_filedataset_test()
    files = [
        "root/p1/f1.csv",
        "root/p2/f2.csv",
        "root/p2/p2p1/f2.csv",
        "root/p3/p3p1/f1.csv",
    ]

    for (i, file) in enumerate(files)
        paths = splitpath(file)[1:(end - 1)]
        root = ""
        for p in paths
            fullp = joinpath(root, p)
            isdir(fullp) || mkdir(fullp)
            root = fullp
        end

        open(file, "w") do io
            write(io, "a,b,c\n")
            write(io, join(i .* [1, 2, 3], ","))
        end
    end

    return files
end
cleanup_filedataset_test() = rm("root"; recursive = true)

@testset "FileDataset" begin
    files = setup_filedataset_test()
    dataset = FileDataset(f -> CSV.read(f, DataFrame), "root", "*.csv")
    @test numobs(dataset) == length(files)
    for (i, file) in enumerate(files)
        true_obs = CSV.read(file, DataFrame)
        @test getobs(dataset, i) == true_obs
    end
    cleanup_filedataset_test()
end

[.\test\containers\hdf5dataset.jl]
function setup_hdf5dataset_test()
    datasets = [
        ("d1", rand(2, 10)),
        ("g1/d1", rand(10)),
        ("g1/d2", string.('a':'j')),
        ("g2/g1/d1", "test"),
    ]

    fid = h5open("test.h5", "w")
    for (path, data) in datasets
        fid[path] = data
    end
    close(fid)

    return first.(datasets), last.(datasets)
end
cleanup_hdf5dataset_test() = rm("test.h5")

@testset "HDF5Dataset" begin
    paths, datas = setup_hdf5dataset_test()
    @testset "Single path" begin
        dataset = HDF5Dataset("test.h5", "d1")
        for i in 1:10
            @test getobs(dataset, i) == getobs(datas[1], i)
        end
        @test numobs(dataset) == 10
        close(dataset)
    end
    @testset "Multiple paths" begin
        dataset = HDF5Dataset("test.h5", paths)
        for i in 1:10
            data = Tuple(map(x -> (x isa String) ? x : getobs(x, i), datas))
            @test @inferred(Tuple, getobs(dataset, i)) == data
        end
        @test numobs(dataset) == 10
        close(dataset)
    end
    cleanup_hdf5dataset_test()
end

[.\test\containers\jld2dataset.jl]
function setup_jld2dataset_test()
    datasets = [
        ("d1", rand(2, 10)),
        ("g1/d1", rand(10)),
        ("g1/d2", string.('a':'j')),
        ("g2/g1/d1", rand(Bool, 3, 3, 10)),
    ]

    fid = jldopen("test.jld2", "w")
    for (path, data) in datasets
        fid[path] = data
    end
    close(fid)

    return first.(datasets), Tuple(last.(datasets))
end
cleanup_jld2dataset_test() = rm("test.jld2")

@testset "JLD2Dataset" begin
    paths, datas = setup_jld2dataset_test()
    @testset "Single path" begin
        dataset = JLD2Dataset("test.jld2", "d1")
        for i in 1:10
            @test @inferred(getobs(dataset, i)) == getobs(datas[1], i)
        end
        @test numobs(dataset) == 10
        close(dataset)
    end
    @testset "Multiple paths" begin
        dataset = JLD2Dataset("test.jld2", paths)
        for i in 1:10
            @test @inferred(getobs(dataset, i)) == getobs(datas, i)
        end
        @test numobs(dataset) == 10
        close(dataset)
    end
    cleanup_jld2dataset_test()
end

[.\test\containers\tabledataset.jl]
@testset "TableDataset" begin
    @testset "TableDataset from rowaccess table" begin
        Tables.columnaccess(::Type{<:Tables.MatrixTable}) = false
        Tables.rowaccess(::Type{<:Tables.MatrixTable}) = true

        testtable = Tables.table([1 4.0 "7"; 2 5.0 "8"; 3 6.0 "9"])
        td = TableDataset(testtable)

        @test collect(@inferred(getobs(td, 1))) == [1, 4.0, "7"]
        @test numobs(td) == 3
    end

    @testset "TableDataset from columnaccess table" begin
        Tables.columnaccess(::Type{<:Tables.MatrixTable}) = true
        Tables.rowaccess(::Type{<:Tables.MatrixTable}) = false

        testtable = Tables.table([1 4.0 "7"; 2 5.0 "8"; 3 6.0 "9"])
        td = TableDataset(testtable)

        @test collect(@inferred(NamedTuple, getobs(td, 2))) == [2, 5.0, "8"]
        @test numobs(td) == 3

        @test getobs(td, 1) isa NamedTuple
    end

    @testset "TableDataset from DataFrames" begin
        testtable = DataFrame(col1 = [1, 2, 3, 4, 5],
                              col2 = ["a", "b", "c", "d", "e"],
                              col3 = [10, 20, 30, 40, 50],
                              col4 = ["A", "B", "C", "D", "E"],
                              col5 = [100.0, 200.0, 300.0, 400.0, 500.0],
                              split = ["train", "train", "train", "valid", "valid"])
        td = TableDataset(testtable)
        @test td isa TableDataset{<:DataFrame}

        @test collect(@inferred(getobs(td, 1))) == [1, "a", 10, "A", 100.0, "train"]
        @test numobs(td) == 5
    end

    @testset "TableDataset from CSV" begin
        open("test.csv", "w") do io
            write(io, "col1,col2,col3,col4,col5, split\n1,a,10,A,100.,train")
        end
        testtable = CSV.File("test.csv")
        td = TableDataset(testtable)
        @test td isa TableDataset{<:CSV.File}
        @test collect(@inferred(getobs(td, 1))) == [1, "a", 10, "A", 100.0, "train"]
        @test numobs(td) == 1
        rm("test.csv")
    end

    @testset "TableDataset is a table" begin
        testtable = DataFrame(col1 = [1, 2, 3, 4, 5],
                              col2 = ["a", "b", "c", "d", "e"],
                              col3 = [10, 20, 30, 40, 50],
                              col4 = ["A", "B", "C", "D", "E"],
                              col5 = [100.0, 200.0, 300.0, 400.0, 500.0],
                              split = ["train", "train", "train", "valid", "valid"])
        td = TableDataset(testtable)
        @testset for fn in (Tables.istable,
                            Tables.rowaccess, Tables.rows,
                            Tables.columnaccess, Tables.columns,
                            Tables.schema, Tables.materializer)
            @test fn(td) == fn(testtable)
        end
    end
end

[.\test\datasets\graphs.jl]

@testset "CiteSeer" begin
    data = CiteSeer()
    @test data isa AbstractDataset
    @test length(data) == 1
    g = data[1]
    @test g === data[:]
    @test g isa MLDatasets.Graph

    @test g.num_nodes == 3327
    @test g.num_edges == 9104
    @test size(g.node_data.features) == (3703, g.num_nodes)
    @test size(g.node_data.targets) == (g.num_nodes,)
    @test sum(g.node_data.train_mask) == 120
    @test sum(g.node_data.val_mask) == 500
    @test sum(g.node_data.test_mask) == 1015
    @test g.edge_index isa Tuple{Vector{Int}, Vector{Int}}
    s, t = g.edge_index
    for a in (s, t)
        @test a isa Vector{Int}
        @test length(a) == g.num_edges
        @test minimum(a) == 1
        @test maximum(a) == g.num_nodes
    end
end

@testset "Cora" begin
    data = Cora()
    @test data isa AbstractDataset
    @test length(data) == 1
    g = data[1]
    @test g === data[:]
    @test g isa MLDatasets.Graph

    @test g.num_nodes == 2708
    @test g.num_edges == 10556
    @test size(g.node_data.features) == (1433, g.num_nodes)
    @test size(g.node_data.targets) == (g.num_nodes,)
    @test sum(g.node_data.train_mask) == 140
    @test sum(g.node_data.val_mask) == 500
    @test sum(g.node_data.test_mask) == 1000
    @test g.edge_index isa Tuple{Vector{Int}, Vector{Int}}
    s, t = g.edge_index
    for a in (s, t)
        @test a isa Vector{Int}
        @test length(a) == g.num_edges
        @test minimum(a) == 1
        @test maximum(a) == g.num_nodes
    end
end

@testset "KarateClub" begin
    data = KarateClub()
    @test data isa AbstractDataset
    @test length(data) == 1
    g = data[1]
    @test g === data[:]
    @test g isa MLDatasets.Graph

    @test g.num_nodes == 34
    @test g.num_edges == 156
    @test size(g.node_data.labels_clubs) == (g.num_nodes,)
    @test sort(unique(g.node_data.labels_clubs)) == [0, 1]
    @test size(g.node_data.labels_comm) == (g.num_nodes,)
    @test sort(unique(g.node_data.labels_comm)) == [0, 1, 2, 3]

    @test g.edge_index isa Tuple{Vector{Int}, Vector{Int}}
    s, t = g.edge_index
    for a in (s, t)
        @test a isa Vector{Int}
        @test length(a) == g.num_edges
        @test minimum(a) == 1
        @test maximum(a) == g.num_nodes
    end
end

@testset "PolBlogs" begin
    data = PolBlogs()
    @test data isa AbstractDataset
    @test length(data) == 1
    g = data[1]
    @test g === data[:]
    @test g isa MLDatasets.Graph

    @test g.num_nodes == 1490
    @test g.num_edges == 19025
    @test size(g.node_data.labels) == (g.num_nodes,)
    @test sort(unique(g.node_data.labels)) == [0, 1]
    @test g.node_data.names isa Vector{String}
    @test size(g.node_data.names) == (g.num_nodes,)

    @test g.edge_index isa Tuple{Vector{Int}, Vector{Int}}
    s, t = g.edge_index
    for a in (s, t)
        @test a isa Vector{Int}
        @test length(a) == g.num_edges
        @test minimum(a) >= 1
        @test maximum(a) <= g.num_nodes
    end
end

@testset "PubMed" begin
    data = PubMed()
    @test data isa AbstractDataset
    @test length(data) == 1
    g = data[1]
    @test g === data[:]
    @test g isa MLDatasets.Graph

    @test g.num_nodes == 19717
    @test g.num_edges == 88648
    @test size(g.node_data.features) == (500, g.num_nodes)
    @test size(g.node_data.targets) == (g.num_nodes,)
    @test sum(g.node_data.train_mask) == 60
    @test sum(g.node_data.val_mask) == 500
    @test sum(g.node_data.test_mask) == 1000
    @test g.edge_index isa Tuple{Vector{Int}, Vector{Int}}
    s, t = g.edge_index
    for a in (s, t)
        @test a isa Vector{Int}
        @test length(a) == g.num_edges
        @test minimum(a) == 1
        @test maximum(a) == g.num_nodes
    end
end

# maybe, maybe, maybe??
Sys.iswindows() || @testset "OGBn-mag" begin
    data = OGBDataset("ogbn-mag")
    # @test data isa AbstractDataset
    @test length(data) == 1

    g = data[1]
    @test g == data[:]
    @test g isa MLDatasets.HeteroGraph

    num_nodes = Dict("paper" => 736389,
                     "author" => 1134649,
                     "institution" => 8740,
                     "field_of_study" => 59965)
    num_edges = Dict(("author", "affiliated_with", "institution") => 1043998,
                     ("author", "writes", "paper") => 7145660,
                     ("paper", "cites", "paper") => 5416271,
                     ("paper", "has_topic", "field_of_study") => 7505078)

    for type in keys(num_nodes)
        @test type ∈ g.node_types
        @test g.num_nodes[type] == num_nodes[type]
        node_data = get(g.node_data, type, nothing)
        isnothing(node_data) || for (key, val) in node_data
            @test key ∈ [:year, :features, :label, :train_mask, :test_mask, :val_mask]
            if key == :features
                @test size(val)[1] == 128
            end
            @test size(val)[end] == num_nodes[type]
        end
    end

    for type in keys(num_edges)
        @test type ∈ g.edge_types
        @test g.num_edges[type] == num_edges[type]
        @test length(g.edge_indices[type][1]) == num_edges[type]
        @test length(g.edge_indices[type][2]) == num_edges[type]
        edge_data = g.edge_data[type]
        for (key, val) in edge_data
            @test key == :reltype
            @test ndims(val) == 1
            @test size(val)[end] == num_edges[type]
        end
    end
end

@testset "OGBl-ddi" begin
    data = OGBDataset("ogbl-ddi")
    # @test data isa AbstractDataset
    @test length(data) == 1

    g = data[1]
    @test g == data[:]
    @test g isa MLDatasets.Graph

    @test g.num_nodes == 4267
    @test g.num_edges == 2135822
    @test g.edge_index isa Tuple{Vector{Int}, Vector{Int}}
    s, t = g.edge_index
    for a in (s, t)
        @test a isa Vector{Int}
        @test length(a) == g.num_edges
        @test minimum(a) == 1
        @test maximum(a) == g.num_nodes
    end
    for key in keys(g.edge_data)
        @assert key ∈ [:train_dict, :val_dict, :test_dict]
        s, t = g.edge_data[key]["edge"]
        for a in (s, t)
            @test a isa Vector{Int}
            @test minimum(a) >= 1
            @test maximum(a) <= g.num_nodes
        end
        if haskey(g.edge_data[key], "edge_neg")
            s, t = g.edge_data[key]["edge_neg"]
            for a in (s, t)
                @test a isa Vector{Int}
                @test minimum(a) >= 1
                @test maximum(a) <= g.num_nodes
            end
        end
    end
end

@testset "ml-latest-small" begin
    data = MovieLens("latest-small")
    @test length(data) == 1

    g = data[1]
    @test g == data[:]
    @test g isa MLDatasets.HeteroGraph

    num_nodes = Dict("tag" => 3683,
                     "movie" => 9742,
                     "user" => 610)
    num_edges = Dict(("user", "rating", "movie") => 100836,
                     ("user", "tag", "movie") => 3683)

    for type in keys(num_nodes)
        @test type ∈ g.node_types
        @test g.num_nodes[type] == num_nodes[type]
        node_data = get(g.node_data, type, nothing)
        isnothing(node_data) || for (key, val) in node_data
            @test size(val)[end] == num_nodes[type]
        end
    end

    for type in keys(num_edges)
        @test type ∈ g.edge_types
        @test g.num_edges[type] == num_edges[type]
        @test length(g.edge_indices[type][1]) == num_edges[type]
        @test length(g.edge_indices[type][2]) == num_edges[type]
        edge_data = g.edge_data[type]
        for (key, val) in edge_data
            @test key in [:timestamp, :tag_name, :rating]
            @test ndims(val) == 1
            @test size(val)[end] == num_edges[type]
        end
    end
end

@testset "TUDataset - Cuneiform" begin
    data = TUDataset("Cuneiform")

    @test data.num_nodes == 5680
    @test data.num_edges == 23922
    @test data.num_graphs == 267

    @test data.num_nodes == sum(g -> g.num_nodes, data.graphs)
    @test data.num_edges == sum(g -> g.num_edges, data.graphs)
    @test data.num_edges == sum(g -> length(g.edge_index[1]), data.graphs)
    @test data.num_edges == sum(g -> length(g.edge_index[2]), data.graphs)
    @test data.num_graphs == length(data) == length(data.graphs)

    i = rand(1:length(data))
    di = data[i]
    @test di isa NamedTuple
    g, targets = di.graphs, di.targets
    @test targets isa Int
    @test g isa Graph
    @test all(1 .<= g.edge_index[1] .<= g.num_nodes)
    @test all(1 .<= g.edge_index[2] .<= g.num_nodes)

    # graph data
    @test size(data.graph_data.targets) == (data.num_graphs,)

    # node data
    @test size(g.node_data.features) == (3, g.num_nodes)
    @test size(g.node_data.targets) == (2, g.num_nodes)

    # edge data
    @test size(g.edge_data.features) == (2, g.num_edges)
    @test size(g.edge_data.targets) == (g.num_edges,)
end

[.\test\datasets\graphs_no_ci.jl]

@testset "ml-100k" begin
    data = MovieLens("100k")
    @test length(data) == 1

    g = data[1]
    @test g == data[:]
    @test g isa MLDatasets.HeteroGraph

    num_nodes = Dict("movie" => 1682,
                     "user" => 943)
    num_edges = Dict(("user", "rating", "movie") => 100000)

    for type in keys(num_nodes)
        @test type ∈ g.node_types
        @test g.num_nodes[type] == num_nodes[type]
        node_data = get(g.node_data, type, nothing)
        @test !isnothing(node_data)
        for (key, val) in node_data
            @test key ∈ [:release_date, :genres, :age, :occupation, :zipcode, :gender]
            @test size(val)[end] == num_nodes[type]
        end
    end

    for type in keys(num_edges)
        @test type ∈ g.edge_types
        @test g.num_edges[type] == num_edges[type]
        @test length(g.edge_indices[type][1]) == num_edges[type]
        @test length(g.edge_indices[type][2]) == num_edges[type]
        edge_data = g.edge_data[type]
        for (key, val) in edge_data
            @test key in [:timestamp, :rating]
            @test ndims(val) == 1
            @test size(val)[end] == num_edges[type]
        end
    end
end

@testset "ml-1m" begin
    data = MovieLens("1m")
    @test length(data) == 1

    g = data[1]
    @test g == data[:]
    @test g isa MLDatasets.HeteroGraph

    num_nodes = Dict("movie" => 3883,
                     "user" => 6040)
    num_edges = Dict(("user", "rating", "movie") => 1000209)

    for type in keys(num_nodes)
        @test type ∈ g.node_types
        @test g.num_nodes[type] == num_nodes[type]
        node_data = get(g.node_data, type, nothing)
        @test !isnothing(node_data)
        for (key, val) in node_data
            @test key ∈ [:genres, :age, :occupation, :zipcode, :gender]
            @test size(val)[end] == num_nodes[type]
        end
    end

    for type in keys(num_edges)
        @test type ∈ g.edge_types
        @test g.num_edges[type] == num_edges[type]
        @test length(g.edge_indices[type][1]) == num_edges[type]
        @test length(g.edge_indices[type][2]) == num_edges[type]
        edge_data = g.edge_data[type]
        for (key, val) in edge_data
            @test key in [:timestamp, :rating]
            @test ndims(val) == 1
            @test size(val)[end] == num_edges[type]
        end
    end
end

@testset "ml-10m" begin
    data = MovieLens("10m")
    @test length(data) == 1

    g = data[1]
    @test g == data[:]
    @test g isa MLDatasets.HeteroGraph

    num_nodes = Dict("tag" => 95580,
                     "movie" => 10681,
                     "user" => 69878)
    num_edges = Dict(("user", "tag", "movie") => 95580,
                     ("user", "rating", "movie") => 10000054)

    for type in keys(num_nodes)
        @test type ∈ g.node_types
        @test g.num_nodes[type] == num_nodes[type]
        node_data = get(g.node_data, type, nothing)
        isnothing(node_data) || for (key, val) in node_data
            @test size(val)[end] == num_nodes[type]
        end
    end

    for type in keys(num_edges)
        @test type ∈ g.edge_types
        @test g.num_edges[type] == num_edges[type]
        @test length(g.edge_indices[type][1]) == num_edges[type]
        @test length(g.edge_indices[type][2]) == num_edges[type]
        edge_data = g.edge_data[type]
        for (key, val) in edge_data
            @test key in [:timestamp, :tag_name, :rating]
            @test ndims(val) == 1
            @test size(val)[end] == num_edges[type]
        end
    end
end

@testset "ml-20m" begin
    data = MovieLens("20m")
    @test length(data) == 1

    g = data[1]
    @test g == data[:]
    @test g isa MLDatasets.HeteroGraph

    num_nodes = Dict("tag" => 465564,
                     "movie" => 27278,
                     "user" => 138493)
    num_edges = Dict(("movie", "score", "tag") => 11709768,
                     ("user", "tag", "movie") => 465564,
                     ("user", "rating", "movie") => 20000263)

    for type in keys(num_nodes)
        @test type ∈ g.node_types
        @test g.num_nodes[type] == num_nodes[type]
        @test isempty(g.node_data)
    end

    for type in keys(num_edges)
        @test type ∈ g.edge_types
        @test g.num_edges[type] == num_edges[type]
        @test length(g.edge_indices[type][1]) == num_edges[type]
        @test length(g.edge_indices[type][2]) == num_edges[type]
        edge_data = g.edge_data[type]
        for (key, val) in edge_data
            @test key in [:timestamp, :tag_name, :rating, :score]
            @test ndims(val) == 1
            @test size(val)[end] == num_edges[type]
        end
    end
end

@testset "ml-25m" begin
    data = MovieLens("25m")
    @test length(data) == 1

    g = data[1]
    @test g == data[:]
    @test g isa MLDatasets.HeteroGraph

    num_nodes = Dict("tag" => 1093360,
                     "movie" => 62423,
                     "user" => 162541)
    num_edges = Dict(("movie", "score", "tag") => 15584448,
                     ("user", "tag", "movie") => 1093360,
                     ("user", "rating", "movie") => 25000095)

    for type in keys(num_nodes)
        @test type ∈ g.node_types
        @test g.num_nodes[type] == num_nodes[type]
        @test isempty(g.node_data)
    end

    for type in keys(num_edges)
        @test type ∈ g.edge_types
        @test g.num_edges[type] == num_edges[type]
        @test length(g.edge_indices[type][1]) == num_edges[type]
        @test length(g.edge_indices[type][2]) == num_edges[type]
        edge_data = g.edge_data[type]
        for (key, val) in edge_data
            @test key in [:timestamp, :tag_name, :rating, :score]
            @test ndims(val) == 1
            @test size(val)[end] == num_edges[type]
        end
    end
end

@testset "OGBDataset - ogbn-arxiv" begin
    d = OGBDataset("ogbn-arxiv")
    g = d[:]
    @test g.num_nodes == 169343
    @test g.num_edges == 1166243

    @test sum(count.([g.node_data.train_mask, g.node_data.test_mask, g.node_data.val_mask])) ==
          g.num_nodes
end

@testset "OGBDataset - ogbg-molhiv" begin
    d = OGBDataset("ogbg-molhiv")

    @test sum(count.([
                         d.graph_data.train_mask,
                         d.graph_data.test_mask,
                         d.graph_data.val_mask,
                     ])) == length(d)
    g = d[1].graphs
    @test size(g.edge_data.features, 2) == g.num_edges
end

@testset "Reddit_full" begin
    data = Reddit(full = true)
    @test length(data) == 1
    g = data[1]
    @test g.num_nodes == 232965
    @test g.num_edges == 114615892
    @test size(g.node_data.features) == (602, g.num_nodes)
    @test size(g.node_data.labels) == (g.num_nodes,)
    @test count(g.node_data.train_mask) == 153431
    @test count(g.node_data.val_mask) == 23831
    @test count(g.node_data.test_mask) == 55703
    s, t = g.edge_index
    @test length(s) == length(t) == g.num_edges
    @test minimum(s) == minimum(t) == 1
    @test maximum(s) == maximum(t) == g.num_nodes
end

@testset "Reddit_subset" begin
    data = Reddit(full = false)
    @test length(data) == 1
    g = data[1]
    @test g.num_nodes == 231443
    @test g.num_edges == 23213838
    @test size(g.node_data.features) == (602, g.num_nodes)
    @test size(g.node_data.labels) == (g.num_nodes,)
    @test count(g.node_data.train_mask) == 152410
    @test count(g.node_data.val_mask) == 23699
    @test count(g.node_data.test_mask) == 55334
    s, t = g.edge_index
    @test length(s) == length(t) == g.num_edges
    @test minimum(s) == minimum(t) == 1
    @test maximum(s) == maximum(t) == g.num_nodes
end

@testset "TUDataset - PROTEINS" begin
    data = TUDataset("PROTEINS")

    @test data.num_nodes == 43471
    @test data.num_edges == 162088
    @test data.num_graphs == 1113

    @test data.num_nodes == sum(g -> g.num_nodes, data.graphs)
    @test data.num_edges == sum(g -> g.num_edges, data.graphs)
    @test data.num_edges == sum(g -> length(g.edge_index[1]), data.graphs)
    @test data.num_edges == sum(g -> length(g.edge_index[2]), data.graphs)
    @test data.num_graphs == length(data) == length(data.graphs)

    i = rand(1:length(data))
    di = data[i]
    @test di isa NamedTuple
    g, targets = di.graphs, di.targets
    @test targets isa Int
    @test g isa Graph
    @test all(1 .<= g.edge_index[1] .<= g.num_nodes)
    @test all(1 .<= g.edge_index[2] .<= g.num_nodes)

    # graph data
    @test size(data.graph_data.targets) == (data.num_graphs,)

    # node data
    @test size(g.node_data.features) == (1, g.num_nodes)
    @test size(g.node_data.targets) == (g.num_nodes,)

    # edge data
    @test g.edge_data === nothing
end

@testset "TUDataset - QM9" begin
    data = TUDataset("QM9")

    @test data.num_nodes == 2333625
    @test data.num_edges == 4823498
    @test data.num_graphs == 129433

    @test data.num_nodes == sum(g -> g.num_nodes, data.graphs)
    @test data.num_edges == sum(g -> g.num_edges, data.graphs)
    @test data.num_edges == sum(g -> length(g.edge_index[1]), data.graphs)
    @test data.num_edges == sum(g -> length(g.edge_index[2]), data.graphs)
    @test data.num_graphs == length(data) == length(data.graphs)

    i = rand(1:length(data))
    g, features = data[i]
    @test g isa Graph
    @test all(1 .<= g.edge_index[1] .<= g.num_nodes)
    @test all(1 .<= g.edge_index[2] .<= g.num_nodes)

    # graph data
    @test size(data.graph_data.features) == (19, data.num_graphs)

    # node data
    @test size(g.node_data.features) == (16, g.num_nodes)

    # edge data
    @test size(g.edge_data.features) == (4, g.num_edges)
end

@testset "TUDataset - Fingerprint" begin
    @test_warn "" TUDataset("Fingerprint")
    data = TUDataset("Fingerprint")

    @test data.num_nodes == 15167
    @test data.num_edges == 24756
    @test data.num_graphs == 2149

    @test data.num_nodes == sum(g -> g.num_nodes, data.graphs)
    @test data.num_edges == sum(g -> g.num_edges, data.graphs)
    @test data.num_edges == sum(g -> length(g.edge_index[1]), data.graphs)
    @test data.num_edges == sum(g -> length(g.edge_index[2]), data.graphs)
    @test data.num_graphs == length(data) == length(data.graphs)

    i = rand(1:length(data))
    @test_throws DimensionMismatch data[i]
    g = data.graphs[i]
    @test g isa Graph
    @test all(1 .<= g.edge_index[1] .<= g.num_nodes)
    @test all(1 .<= g.edge_index[2] .<= g.num_nodes)

    # graph data
    @test size(data.graph_data.targets) == (2800,)

    # node data
    @test size(g.node_data.features) == (2, g.num_nodes)

    # edge data
    @test size(g.edge_data.features) == (2, g.num_edges)
end

@testset "OrganicMaterialsDB" begin
    data = OrganicMaterialsDB(split = :train)
    @test length(data) == 10000
    @test data[1].graphs isa MLDatasets.Graph
    @test data[1].bandgaps isa Number
    @test data[:].graphs isa Vector{MLDatasets.Graph}
    @test data[:].bandgaps isa Vector{<:Number}

    data = OrganicMaterialsDB(split = :test)
    @test length(data) == 2500
end

@testset "METR-LA" begin
    data = METRLA()
    @test data isa AbstractDataset
    @test length(data) == 1
    g = data[1]
    @test g === data[:]
    @test g isa MLDatasets.Graph

    @test g.num_nodes == 207
    @test g.num_edges == 1722
    @test all(g.node_data.features[1][:,:,1][2:end,1] == g.node_data.targets[1][:,:,1][1:end-1])
end

@testset "PEMS-BAY" begin
    data = PEMSBAY()
    @test data isa AbstractDataset
    @test length(data) == 1
    g = data[1]
    @test g === data[:]
    @test g isa MLDatasets.Graph

    @test g.num_nodes == 325
    @test g.num_edges == 2694
    @test all(g.node_data.features[1][:,:,1][2:end,1] == g.node_data.targets[1][:,:,1][1:end-1])
end

@testset "TemporalBrains" begin
    data = TemporalBrains()
    @test data isa AbstractDataset
    @test length(data) == 1000
    g = data[1]
    @test g isa MLDatasets.TemporalSnapshotsGraph

    @test g.num_nodes == [102 for _ in 1:27]
    @test g.num_snapshots == 27
    @test g.snapshots[1] isa MLDatasets.Graph
    @test length(g.snapshots[1].node_data) == 102
end

@testset "WindMillEnergy" begin
    data = WindMillEnergy(size = "small")
    @test data isa AbstractDataset
    @test length(data) == 1
    g = data[1]
    @test g === data[:]
    @test g isa MLDatasets.Graph

    @test g.num_nodes == 11
    @test g.num_edges == 121
    @test g.node_data.features[1][:,:,2:end] == g.node_data.features[2][:,:,1:end-1]
end

@testset "ChickenPox" begin
    data = ChickenPox()
    @test data isa AbstractDataset
    @test data.metadata isa Dict
    @test length(data) == 1
    g = data[1]
    @test g === data[:]
    @test g isa MLDatasets.Graph

    @test g.num_nodes == 20
    @test g.num_edges == 102
    @test g.node_data.features[1][:,:,2:end] == g.node_data.features[2][:,:,1:end-1]

    @test data.metadata[:BUDAPEST] == 5
    @test data.metadata[:BACS] == 1
    @test data.metadata[:ZALA] == 20
    @test data.metadata[:HEVES] == 10
    @test data.metadata isa Dict{Symbol, Any}
end
[.\test\datasets\meshes.jl]
# requires manual downloading
@testset "MPI-FAUST" begin
    train_data = FAUST()
    test_data = FAUST(:test)
    @assert length(train_data.scans) == 100
    @assert length(train_data.scans) == length(train_data.registrations)
    @assert length(train_data.scans) == length(train_data.labels)
    @assert length(test_data.scans) == 200
end

[.\test\datasets\misc.jl]
@testset "BostonHousing" begin
    n_obs = 506
    n_features = 13
    n_targets = 1
    feature_names = [
        "CRIM",
        "ZN",
        "INDUS",
        "CHAS",
        "NOX",
        "RM",
        "AGE",
        "DIS",
        "RAD",
        "TAX",
        "PTRATIO",
        "B",
        "LSTAT",
    ]
    target_names = ["MEDV"]

    d = BostonHousing()
    test_inmemory_supervised_table_dataset(d;
                                           n_obs, n_features, n_targets,
                                           feature_names, target_names)

    d = BostonHousing(as_df = false)
    test_inmemory_supervised_table_dataset(d;
                                           n_obs, n_features, n_targets,
                                           feature_names, target_names,
                                           Tx = Float64, Ty = Float64)
end

@testset "Iris" begin
    n_obs = 150
    n_features = 4
    n_targets = 1

    feature_names = ["sepallength", "sepalwidth", "petallength", "petalwidth"]
    target_names = ["class"]

    d = Iris()
    test_inmemory_supervised_table_dataset(d;
                                           n_obs, n_features, n_targets,
                                           feature_names, target_names)

    d = Iris(as_df = false)
    test_inmemory_supervised_table_dataset(d;
                                           n_obs, n_features, n_targets,
                                           feature_names, target_names,
                                           Tx = Float64, Ty = AbstractString)
end

@testset "Mutagenesis" begin
    dtrain = Mutagenesis(split = :train)
    dtest = Mutagenesis(split = :test)
    dval = Mutagenesis(split = :val)
    dall = Mutagenesis(split = :all)

    train_x, train_y = dtrain[:]
    test_x, test_y = dtest[:]
    val_x, val_y = dval[:]
    all_x, all_y = dall[:]

    @test length(train_x) == length(train_y) == 100
    @test length(test_x) == length(test_y) == 44
    @test length(val_x) == length(val_y) == 44
    @test length(all_x) == length(all_y) == 188

    # test that label is not contained in features
    @test !any(haskey.(train_x, :mutagenic))
    @test !any(haskey.(test_x, :mutagenic))
    @test !any(haskey.(val_x, :mutagenic))
    # test data is materialized
    @test train_x isa Vector{<:Dict}
    @test test_x isa Vector{<:Dict}
    @test val_x isa Vector{<:Dict}

    x, y = dtrain[1:2]
    @test x isa Vector{Dict{Symbol, Any}}
    @test length(x) == 2
    @test y isa Vector{Int}
    @test length(y) == 2
end

@testset "Titanic" begin
    n_obs = 891
    n_features = 11
    n_targets = 1
    feature_names = [
        "PassengerId",
        "Pclass",
        "Name",
        "Sex",
        "Age",
        "SibSp",
        "Parch",
        "Ticket",
        "Fare",
        "Cabin",
        "Embarked",
    ]
    target_names = ["Survived"]

    d = Titanic()
    test_inmemory_supervised_table_dataset(d;
                                           n_obs, n_features, n_targets,
                                           feature_names, target_names)

    d = Titanic(as_df = false)
    test_inmemory_supervised_table_dataset(d;
                                           n_obs, n_features, n_targets,
                                           feature_names, target_names,
                                           Tx = Any, Ty = Int)

    @test isequal(d[1].features,
                  [
                      1,
                      3,
                      "Braund, Mr. Owen Harris",
                      "male",
                      22,
                      1,
                      0,
                      "A/5 21171",
                      7.25,
                      missing,
                      "S",
                  ])
end

@testset "Wine" begin
    n_obs = 178
    n_features = 13
    n_targets = 1
    feature_names = [
        "Alcohol",
        "Malic.acid",
        "Ash",
        "Acl",
        "Mg",
        "Phenols",
        "Flavanoids",
        "Nonflavanoid.phenols",
        "Proanth",
        "Color.int",
        "Hue",
        "OD",
        "Proline",
    ]
    target_names = ["Wine"]

    d = Wine()
    test_inmemory_supervised_table_dataset(d;
                                           n_obs, n_features, n_targets,
                                           feature_names, target_names)

    d = Wine(as_df = false)
    test_inmemory_supervised_table_dataset(d;
                                           n_obs, n_features, n_targets,
                                           feature_names, target_names,
                                           Tx = Any, Ty = Int)

    @test isequal(d[1].features,
                  [
                      14.23,
                      1.71,
                      2.43,
                      15.6,
                      127,
                      2.8,
                      3.06,
                      0.28,
                      2.29,
                      5.64,
                      1.04,
                      3.92,
                      1065,
                  ])
end

[.\test\datasets\text.jl]

@testset "PTBLM" begin
    n_targets = 1
    n_features = ()
    Tx = Vector{String}
    Ty = Vector{String}

    for (n_obs, split) in [(42068, :train), (3761, :test)]
        d = PTBLM(split)

        test_supervised_array_dataset(d;
                                      n_obs, n_targets, n_features,
                                      Tx, Ty)
    end
end

@testset "UD_English" begin
    n_features = ()
    Tx = Vector{Vector{String}}
    for (n_obs, split) in [(12544, :train), (2077, :test), (2001, :dev)]
        d = UD_English(split)

        test_unsupervised_array_dataset(d;
                                        n_obs, n_features, Tx)
    end
end

[.\test\datasets\text_no_ci.jl]
@testset "SMSSpamCollection" begin
    n_obs = 5574
    n_targets = 1
    n_features = ()
    Tx = String
    Ty = String

    d = SMSSpamCollection()

    test_supervised_array_dataset(d;
                                  n_obs, n_targets, n_features,
                                  Tx, Ty)
end

[.\test\datasets\vision\cifar10.jl]
n_features = (32, 32, 3)
n_targets = 1

@testset "trainset" begin
    d = CIFAR10()

    @test d.split == :train
    @test extrema(d.features) == (0, 1)
    @test convert2image(d, 1) isa AbstractMatrix{<:RGB}
    @test convert2image(d, 1:2) isa AbstractArray{<:RGB, 3}

    test_supervised_array_dataset(d;
                                  n_features, n_targets, n_obs = 50000,
                                  Tx = Float32, Ty = Int)

    d = CIFAR10(:train)
    @test d.split == :train
    d = CIFAR10(Int, :train)
    @test d.split == :train
    @test d.features isa AbstractArray{Int}
end

@testset "testset" begin
    d = CIFAR10(split = :test, Tx = UInt8)

    @test d.split == :test
    @test extrema(d.features) == (0, 255)
    @test convert2image(d, 1) isa AbstractMatrix{<:RGB}
    @test convert2image(d, 1:2) isa AbstractArray{<:RGB, 3}

    test_supervised_array_dataset(d;
                                  n_features, n_targets, n_obs = 10000,
                                  Tx = UInt8, Ty = Int)
end

[.\test\datasets\vision\cifar100.jl]
n_features = (32, 32, 3)
n_targets = (coarse = 1, fine = 1)

@testset "trainset" begin
    d = CIFAR100()

    @test d.split == :train
    @test extrema(d.features) == (0, 1)
    @test convert2image(d, 1) isa AbstractMatrix{<:RGB}
    @test convert2image(d, 1:2) isa AbstractArray{<:RGB, 3}

    test_supervised_array_dataset(d;
                                  n_features, n_targets, n_obs = 50000,
                                  Tx = Float32, Ty = Int)

    d = CIFAR100(:train)
    @test d.split == :train
    d = CIFAR100(Int, :train)
    @test d.split == :train
    @test d.features isa AbstractArray{Int}
end

@testset "testset" begin
    d = CIFAR100(split = :test, Tx = UInt8)

    @test d.split == :test
    @test extrema(d.features) == (0, 255)
    @test convert2image(d, 1) isa AbstractMatrix{<:RGB}
    @test convert2image(d, 1:2) isa AbstractArray{<:RGB, 3}

    test_supervised_array_dataset(d;
                                  n_features, n_targets, n_obs = 10000,
                                  Tx = UInt8, Ty = Int)
end

[.\test\datasets\vision\emnist.jl]

n_features = (28, 28)
n_targets = 1

for (name, n_obs) in [(:balanced, (112800, 18800)),
    (:byclass, (697932, 116323)),
    (:bymerge, (697932, 116323)),
    (:digits, (240000, 40000)),
    (:letters, (124800, 20800)),
    (:mnist, (60000, 10000))]
    d = EMNIST(name)

    @test d.name == name
    @test d.split == :train
    @test extrema(d.features) == (0, 1)
    @test convert2image(d, 1) isa AbstractMatrix{<:Gray}
    @test convert2image(d, 1:2) isa AbstractArray{<:Gray, 3}

    test_supervised_array_dataset(d;
                                  n_features, n_targets, n_obs = n_obs[1],
                                  Tx = Float32, Ty = Int,
                                  conv2img = true)

    d = EMNIST(name, split = :test, Tx = UInt8)

    @test d.name == name
    @test d.split == :test
    @test extrema(d.features) == (0, 255)
    @test convert2image(d, 1) isa AbstractMatrix{<:Gray}

    test_supervised_array_dataset(d;
                                  n_features, n_targets, n_obs = n_obs[2],
                                  Tx = UInt8, Ty = Int,
                                  conv2img = true)
end

d = EMNIST(:balanced, :train)
@test d.split == :train
d = EMNIST(:balanced, Int)
@test d.split == :train
@test d.features isa Array{Int}

[.\test\datasets\vision\fashion_mnist.jl]
n_features = (28, 28)
n_targets = 1

@testset "trainset" begin
    d = FashionMNIST()

    @test d.split == :train
    @test extrema(d.features) == (0, 1)
    @test convert2image(d, 1) isa AbstractMatrix{<:Gray}
    @test convert2image(d, 1:2) isa AbstractArray{<:Gray, 3}

    test_supervised_array_dataset(d;
                                  n_features, n_targets, n_obs = 60000,
                                  Tx = Float32, Ty = Int,
                                  conv2img = true)

    d = FashionMNIST(:train)
    @test d.split == :train
    d = FashionMNIST(Int)
    @test d.split == :train
    @test d.features isa Array{Int}
end

@testset "testset" begin
    d = FashionMNIST(split = :test, Tx = UInt8)

    @test d.split == :test
    @test extrema(d.features) == (0, 255)
    @test convert2image(d, 1) isa AbstractMatrix{<:Gray}

    test_supervised_array_dataset(d;
                                  n_features, n_targets, n_obs = 10000,
                                  Tx = UInt8, Ty = Int,
                                  conv2img = true)
end

[.\test\datasets\vision\mnist.jl]
n_features = (28, 28)
n_targets = 1

@testset "trainset" begin
    d = MNIST()

    @test d.split == :train
    @test extrema(d.features) == (0, 1)
    @test convert2image(d, 1) isa AbstractMatrix{<:Gray}
    @test convert2image(d, 1:2) isa AbstractArray{<:Gray, 3}

    test_supervised_array_dataset(d;
                                  n_features, n_targets, n_obs = 60000,
                                  Tx = Float32, Ty = Int,
                                  conv2img = true)

    d = MNIST(:train)
    @test d.split == :train
    d = MNIST(Int)
    @test d.split == :train
    @test d.features isa Array{Int}
end

@testset "testset" begin
    d = MNIST(split = :test, Tx = UInt8)

    @test d.split == :test
    @test extrema(d.features) == (0, 255)
    @test convert2image(d, 1) isa AbstractMatrix{<:Gray}

    test_supervised_array_dataset(d;
                                  n_features, n_targets, n_obs = 10000,
                                  Tx = UInt8, Ty = Int,
                                  conv2img = true)

    d = MNIST(:test)
    @test d.split == :test
    d = MNIST(Int, :test)
    @test d.split == :test
    @test d.features isa Array{Int}
end

[.\test\datasets\vision\omniglot.jl]
n_features = (105, 105)
n_targets = 1

@testset "trainset" begin
    d = Omniglot()

    @test d.split == :train
    @test extrema(d.features) == (0, 1)
    @test convert2image(d, 1) isa AbstractMatrix{<:Gray}
    @test convert2image(d, 1:2) isa AbstractArray{<:Gray, 3}

    test_supervised_array_dataset(d;
                                  n_features, n_targets, n_obs = 19280,
                                  Tx = Float32, Ty = String,
                                  conv2img = true)

    d = Omniglot(:train)
    @test d.split == :train
    d = Omniglot(Int)
    @test d.split == :train
    @test d.features isa Array{Int}
end

@testset "testset" begin
    d = Omniglot(split = :test, Tx = UInt8)

    @test d.split == :test
    @test extrema(d.features) == (0, 1)
    @test convert2image(d, 1) isa AbstractMatrix{<:Gray}

    test_supervised_array_dataset(d;
                                  n_features, n_targets, n_obs = 13180,
                                  Tx = UInt8, Ty = String,
                                  conv2img = true)

    d = Omniglot(:test)
    @test d.split == :test
    d = Omniglot(Int, :test)
    @test d.split == :test
    @test d.features isa Array{Int}
end

@testset "small1set" begin
    d = Omniglot(split = :small1, Tx = Float32)

    @test d.split == :small1
    @test extrema(d.features) == (0, 1)
    @test convert2image(d, 1) isa AbstractMatrix{<:Gray}

    test_supervised_array_dataset(d;
                                  n_features, n_targets, n_obs = 2720,
                                  Tx = Float32, Ty = String,
                                  conv2img = true)

    d = Omniglot(:small1)
    @test d.split == :small1
    d = Omniglot(Int, :small1)
    @test d.split == :small1
    @test d.features isa Array{Int}
end

@testset "small2set" begin
    d = Omniglot(split = :small2, Tx = UInt8)

    @test d.split == :small2
    @test extrema(d.features) == (0, 1)
    @test convert2image(d, 1) isa AbstractMatrix{<:Gray}

    test_supervised_array_dataset(d;
                                  n_features, n_targets, n_obs = 3120,
                                  Tx = UInt8, Ty = String,
                                  conv2img = true)

    d = Omniglot(:small2)
    @test d.split == :small2
    d = Omniglot(Int, :small2)
    @test d.split == :small2
    @test d.features isa Array{Int}
end

[.\test\datasets\vision\svhn2.jl]
n_features = (32, 32, 3)
n_targets = 1

@testset "trainset" begin
    d = SVHN2()

    @test d.split == :train
    @test extrema(d.features) == (0, 1)
    @test convert2image(d, 1) isa AbstractMatrix{<:RGB}
    @test convert2image(d, 1:2) isa AbstractArray{<:RGB, 3}

    test_supervised_array_dataset(d;
                                  n_features, n_targets, n_obs = 73257,
                                  Tx = Float32, Ty = Int)
end

@testset "testset" begin
    d = SVHN2(split = :test, Tx = UInt8)

    @test d.split == :test
    @test extrema(d.features) == (0, 255)
    @test convert2image(d, 1) isa AbstractMatrix{<:RGB}
    @test convert2image(d, 1:2) isa AbstractArray{<:RGB, 3}

    test_supervised_array_dataset(d;
                                  n_features, n_targets, n_obs = 26032,
                                  Tx = UInt8, Ty = Int)
end

# @testset "extraset" begin 
#     d = SVHN2(split=:test)

#     @test d.split == :extra
#     @test extrema(d.features) == (0, 1)
#     @test convert2image(d, 1) isa AbstractMatrix{<:RGB}
#     @test convert2image(d, 1:2) isa AbstractArray{<:RGB,3}

#     test_supervised_array_dataset(d;
#         n_features, n_targets, n_obs=531131,
#         Tx=Float32, Ty=Int)
# end

